<!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8">
  <title>Resumen y perspectivas del proyecto KBLaM</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Revisión de la teoría y los experimentos de KBLaM, lecciones de un despliegue en servidores nacionales, un plan de entrenamiento concreto y una propuesta de localización al chino con mejoras futuras.">
  <meta name="author" content="Fan Wan">
  <link rel="canonical" href="https://stefanwan-durham.github.io/wanfan.github.io/blog/kblam-project-summary.es.html">
  <link rel="alternate" hreflang="zh" href="https://stefanwan-durham.github.io/wanfan.github.io/blog/kblam-project-summary.html">
  <link rel="alternate" hreflang="en" href="https://stefanwan-durham.github.io/wanfan.github.io/blog/kblam-project-summary.en.html">
  <link rel="alternate" hreflang="es" href="https://stefanwan-durham.github.io/wanfan.github.io/blog/kblam-project-summary.es.html">
  <meta property="og:type" content="article">
  <meta property="og:title" content="Resumen y perspectivas del proyecto KBLaM">
  <meta property="og:description" content="Revisión de la teoría y los experimentos de KBLaM, lecciones de un despliegue en servidores nacionales, un plan de entrenamiento concreto y una propuesta de localización al chino con mejoras futuras.">
  <meta property="og:url" content="https://stefanwan-durham.github.io/wanfan.github.io/blog/kblam-project-summary.es.html">
  <meta property="og:image" content="https://stefanwan-durham.github.io/wanfan.github.io/assets/blog/kblam-project-summary-es.svg">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:secure_url" content="https://stefanwan-durham.github.io/wanfan.github.io/assets/blog/kblam-project-summary-es.svg">
  <meta property="og:image:type" content="image/png">
  <link rel="image_src" href="https://stefanwan-durham.github.io/wanfan.github.io/assets/blog/kblam-project-summary-es.svg">
  <meta itemprop="image" content="https://stefanwan-durham.github.io/wanfan.github.io/assets/blog/kblam-project-summary-es.svg">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Resumen y perspectivas del proyecto KBLaM">
  <meta name="twitter:description" content="Revisión de la teoría y los experimentos de KBLaM, lecciones de un despliegue en servidores nacionales, un plan de entrenamiento concreto y una propuesta de localización al chino con mejoras futuras.">
  <meta name="twitter:image" content="https://stefanwan-durham.github.io/wanfan.github.io/assets/blog/kblam-project-summary-es.svg">
  <meta name="theme-color" content="#0f172a">
  <link rel="icon" href="../assets/logo.svg" type="image/svg+xml">
  <link rel="stylesheet" href="../style.css">
  <!-- Code highlight (Highlight.js) -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" crossorigin="anonymous" referrerpolicy="no-referrer">
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
  <!-- Math (KaTeX) -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
  <script>try{var L='es';localStorage.setItem('lang',L);document.documentElement.setAttribute('lang',L);}catch(e){}</script>
  <script defer src="../lang.js"></script>
  <script defer src="../script.js"></script>
  <script defer src="../assets/vendor/qrcode.min.js"></script>
</head>
<body>
  <a class="skip-link" href="#main">Skip to main content</a>
  <header>
    <nav class="navbar container">
      <a href="../index.html" class="brand" aria-label="Home">
        <img src="../assets/logo.svg" alt="Fan Wan logo" class="brand-logo" width="28" height="28" />
        <span class="logo"><span class="i18n l-zh">首页</span><span class="i18n l-en">Home</span><span class="i18n l-es">Inicio</span></span>
      </a>
      <ul class="nav-links">
        <li><a href="../index.html"><span class="icon" aria-hidden="true"><svg viewBox="0 0 24 24"><path d="M3 12l9-9 9 9"/><path d="M9 21V9h6v12"/></svg></span> <span class="i18n l-zh">首页</span><span class="i18n l-en">Home</span><span class="i18n l-es">Inicio</span></a></li>
        <li><a href="../about.html"><span class="i18n l-zh">关于我</span><span class="i18n l-en">About</span><span class="i18n l-es">Acerca de</span></a></li>
        <li><a href="../publications.html"><span class="i18n l-zh">学术出版物</span><span class="i18n l-en">Research</span><span class="i18n l-es">Investigación</span></a></li>
        <li><a href="../blog.html"><span class="i18n l-zh">博客</span><span class="i18n l-en">Blog</span><span class="i18n l-es">Blog</span></a></li>
        <li><a href="../contact.html"><span class="i18n l-zh">联系</span><span class="i18n l-en">Contact</span><span class="i18n l-es">Contacto</span></a></li>
      </ul>
      <div class="nav-actions">
        <div class="lang-switcher">
          <button id="lang-button" class="btn outline icon-btn" aria-haspopup="listbox" aria-expanded="false">
            <svg class="icon icon-globe" viewBox="0 0 24 24" aria-hidden="true"><g fill="none" stroke="currentColor" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="9"/><path d="M3 12h18M12 3a15 15 0 0 1 0 18M12 3a15 15 0 0 0 0 18"/></g></svg>
            <span class="label"></span>
          </button>
          <ul id="lang-menu" class="lang-menu" role="listbox" aria-label="Language" hidden>
            <li role="option" data-lang="en">English</li>
            <li role="option" data-lang="zh">中文</li>
            <li role="option" data-lang="es">Español</li>
          </ul>
        </div>
        <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme" title="Toggle theme">
          <svg class="icon icon-bulb" viewBox="0 0 24 24" aria-hidden="true"><g fill="none" stroke="currentColor" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round"><path d="M9 18h6"/><path d="M10 22h4"/><path d="M8.5 15.5c-.9-1-1.5-2.3-1.5-3.8a5 5 0 1 1 10 0c0 1.5-.6 2.8-1.5 3.8-.6.7-1.1 1.4-1.3 2.2H9.8c-.2-.8-.7-1.5-1.3-2.2z"/><path d="M12 2v2"/><path d="M4 10h2"/><path d="M18 10h2"/><path d="M5.5 5.5l1.4 1.4"/><path d="M18.5 5.5l-1.4 1.4"/></g></svg>
          <svg class="icon icon-moon" viewBox="0 0 24 24" aria-hidden="true"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z" fill="none" stroke="currentColor" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round"/></svg>
          <svg class="icon icon-system" viewBox="0 0 24 24" aria-hidden="true"><g fill="none" stroke="currentColor" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="12" rx="2" ry="2"/><path d="M8 20h8M12 16v4"/></g></svg>
        </button>
        <div class="hamburger" id="hamburger"><span></span><span></span><span></span></div>
      </div>
    </nav>
  </header>
  <main id="main" class="blog-post">
    <section class="page-hero section">
      <div class="container">
        <div class="i18n-block" data-lang="es">
          <h1 class="post-title">Resumen y perspectivas del proyecto KBLaM</h1>
          <p class="muted post-meta">Publicado el 2025-09-03 · Lectura 5 min</p>
        </div>
      </div>
    </section>
    <section class="section">
      <div class="container prose">
        <div class="post-hero-art" data-lang="es"><img src="https://stefanwan-durham.github.io/wanfan.github.io/assets/blog/kblam-project-summary-es.svg" alt="Cover"/></div>
        <nav class="toc card" aria-label="Contents" style="padding:16px;margin:12px 0;"><strong>Índice</strong><ol></ol></nav>
        <article class="i18n-block" data-lang="es">

<h1 id="resumen-y-perspectivas-del-proyecto-kblam">Resumen y perspectivas del proyecto KBLaM</h1>

<p>En los últimos meses hemos desplegado, depurado y estudiado a fondo <b>KBLaM</b> (Knowledge Base augmented Language Model). Propuesto por Microsoft Research en 2025, KBLaM inyecta conocimiento estructurado directamente en un LLM preentrenado mediante un codificador de oraciones y adaptadores lineales. Convierte la base de conocimiento en vectores continuos clave‑valor (tokens de conocimiento) y los fusiona con el LLM mediante una atención <b>rectangular</b> modificada, lo que permite responder sin recuperación externa. Este artículo revisa los principios y experimentos de KBLaM, resume la experiencia de despliegue en servidores nacionales, presenta el plan de entrenamiento, discute la localización al chino y propone líneas de mejora.</p>

<h2 id="i-revisin-de-la-teora-y-los-experimentos-de-kblam">I. Revisión de la teoría y los experimentos de KBLaM</h2>

<h3 id="11-diseo-del-modelo">1.1 Diseño del modelo</h3>

<p>La idea central es mapear las tripletas de la base de conocimiento $\langle \text{name},\,\text{property},\,\text{value} \rangle$ a vectores del tamaño de la caché clave‑valor del LLM, llamados <b>tokens de conocimiento</b>. El proceso es:</p>

<ol>
<li><b>Codificación del conocimiento.</b> Un codificador $f(\cdot)$ mapea “\&lt;name\&gt; y su \&lt;property\&gt;” y “\&lt;value\&gt;” a $k_m = f(\text{property}_m\,\text{of}\,\text{name}_m)$ y $v_m = f(\text{value}_m)$. Adaptadores lineales los proyectan a los espacios de clave/valor de cada capa: $\tilde{k}_m = \tilde{W}_K k_m$, $\tilde{v}_m = \tilde{W}_V v_m$. Cada token de conocimiento contiene vectores para $L$ capas.</li>
<li><b>Atención rectangular.</b> En inferencia, el modelo alimenta $N$ tokens del prompt y $M$ tokens de conocimiento. Para evitar $O((N+M)^2)$, los tokens de conocimiento no se atienden entre sí; los tokens del prompt pueden atender a anteriores y a todos los de conocimiento, formando una matriz $(M+N)\!\times\!N$. La salida $\tilde{y}_n$ suma: (a) valores del conocimiento ponderados por la similitud entre consulta y claves del conocimiento, y (b) autoatención entre tokens del prompt. El costo crece linealmente con $M$, ventajoso cuando $M\gg N$.</li>
<li><b>Ajuste instruccional con KB.</b> Dado el desfase semántico entre codificador y LLM, el paper entrena solo los adaptadores lineales y una cabeza de consulta, maximizando $\log p_\theta(A\mid Q,KB)$ con una KB sintética (≈45k nombres, 135k tripletas) y sin tocar los pesos del LLM. Con AdamW durante 20k pasos en una A100, el modelo aprende a recuperar y a negarse cuando no hay evidencia.</li>
</ol>

<p>Para entender el flujo, la Figura 1 muestra las fases offline/online: offline se construye y codifica la KB; online la atención rectangular fusiona prompt y conocimiento antes de generar. El siguiente seudocódigo ilustra una capa de atención:</p>

<p><img src="../content/blog/kblam-project-summary/kblam_work_flow.png" alt="Flujo de KBLaM"></p>

<p>Figura 1. Offline: construir/codificar la KB para generar tokens de conocimiento. Online: atención rectangular + LLM para responder.</p>

<h4 id="13-seudocdigo-de-atencin-rectangular">1.3 Seudocódigo de atención rectangular</h4>

<pre><code class="language-python">
def rectangular_attention(Q, K_kb, V_kb, K_text, V_text):
    """
    Q: matriz de consulta de los tokens del prompt, forma (N, d)
    K_kb, V_kb: claves/valores del conocimiento, forma (M, d)
    K_text, V_text: claves/valores de autoatención del prompt
    Devuelve: salida fusionada conocimiento + prompt
    """
    # Atención sobre tokens de conocimiento
    attn_kb = softmax(Q @ K_kb.T / sqrt(d))
    output_kb = attn_kb @ V_kb
    # Autoatención entre tokens del prompt
    attn_text = softmax(Q @ K_text.T / sqrt(d))
    output_text = attn_text @ V_text
    # Suma de ambas partes
    return output_kb + output_text
</code></pre>

<h3 id="12-resultados-experimentales">1.2 Resultados experimentales</h3>

<ul>
<li><b>Precisión e interpretabilidad en recuperación.</b> Las puntuaciones de atención actúan como señales implícitas de recuperación: las palabras de la pregunta se enfocan en los tokens correctos. En conjuntos sintéticos y Enron, KBLaM mantiene top‑1/top‑5 precisos con KBs grandes.</li>
<li><b>Calidad en QA.</b> En QA de respuesta corta, multi‑entidad y abierta, la calidad (BERTScore o GPT‑4) iguala a concatenar todas las tripletas en contexto, con mucha menos memoria. Con &gt;10k tripletas, el aprendizaje en contexto es inviable por memoria $O((KN)^2)$, mientras KBLaM sigue estable.</li>
<li><b>Comportamiento de negativa.</b> KBLaM detecta cuando la KB carece de evidencia y se niega educadamente; la tasa de falsas negativas crece más lento que en el enfoque en contexto.</li>
<li><b>Limitaciones.</b> Vectores de longitud fija pierden números/nombres exactos; las KB sintéticas no siempre reflejan la distribución real; queda trabajo en multi‑salto, compresión controlable y construcción a partir de datos reales.</li>
</ul>

<h2 id="ii-despliegue-en-servidores-nacionales">II. Despliegue en servidores nacionales</h2>

<p>Nuestro entorno usa Kylin v10 y ocho NPUs Ascend 910B sin acceso a Internet. Esto difiere del entorno GPU + GPT/ada‑002 del paper, por lo que adaptamos:</p>

<ol>
<li><b>Generación de KB e instrucciones.</b> Sin OpenAI, usamos LLMs locales offline. En Windows + RTX 2080 Ti probamos <b>Qwen3‑8B</b>, <b>Meta‑Llama‑3‑8B‑Instruct</b> y <b>Meta‑Llama‑3.1‑8B‑Instruct</b>; elegimos <b>Meta‑Llama‑3.1‑8B‑Instruct</b> por equilibrio calidad‑eficiencia y generamos ≈45k nombres con atributos, controlando aleatoriedad y deduplicación.</li>
<li><b>Sustitución del codificador de oraciones.</b> En lugar de ada‑002 (1536‑d), usamos <b>all‑MiniLM‑L6‑v2</b> (384‑d), re‑inicializando adaptadores y normalización. Evaluamos además:</li>
</ol>
<ul>
<li><b>BGE‑base‑en‑v1.5</b>, <b>E5‑base‑v2</b>, <b>nomic‑embed‑text‑v1</b>, <b>all‑mpnet‑base‑v2</b>, <b>gtr‑base</b>.</li>
</ul>

<p>   Elegiremos el mejor para ≈45k tripletas. Estudios señalan que, re‑entrenando adaptadores, cambiar el encoder no degrada la recuperación.</p>

<p>Tabla 1. Comparativa de modelos de incrustación open‑source</p>

<div class="table-wrap"><table>
<thead><tr><th>Modelo</th><th>Arquitectura</th><th>Dim</th><th>Pros</th><th>Contras</th></tr></thead>
<tbody><tr><td>all‑MiniLM‑L6‑v2</td><td>MiniLM (6L)</td><td>384</td><td>Ligero, rápido</td><td>Peor en oraciones largas</td></tr><tr><td>BGE‑base‑en‑v1.5</td><td>BERT</td><td>768</td><td>Recuperación fuerte; prefijos</td><td>Modelo más grande</td></tr><tr><td>E5‑base‑v2</td><td>RoBERTa</td><td>768</td><td>Equilibrado; sin prefijos</td><td>Truncamiento en textos largos</td></tr><tr><td>nomic‑embed‑text‑v1</td><td>Tipo GPT</td><td>≈1024</td><td>Entradas largas; multilingüe</td><td>Grande; más lento</td></tr><tr><td>all‑mpnet‑base‑v2</td><td>MPNet</td><td>768</td><td>Recuperación de alta calidad</td><td>Más demanda de recursos</td></tr></tbody>
</table></div>

<ol>
<li><b>Adaptación del LLM y memoria.</b> Verificamos <b>Llama‑3‑8B‑Instruct</b> en PC personal; en servidor (32 GB por 910B) planeamos paralelismo en 8 tarjetas con ZeRO‑2 y FP16, convirtiendo pesos HF a MindSpore‑CKPT. Para la atención rectangular re‑implementaremos <code>masked_add</code> y <code>softmax</code> en MindSpore y ajustaremos lotes.</li>
<li><b>Compatibilidad del ecosistema.</b> Kylin v10 carece de algunas dependencias y choca con CANN. Compilamos MindSpore 2.2 y PyTorch 2.0 adaptado a Ascend, enlazamos libcann y ajustamos <code>LD_LIBRARY_PATH</code>, usando un índice interno de paquetes.</li>
</ol>

<p>Con lo anterior, reproducimos los principales experimentos en PC y estamos generando ≈45k nombres y 135k tripletas sintéticas, entrenando adaptadores y validando. Después migraremos el flujo al servidor 910B, probaremos atención rectangular y memoria, y afinaremos la construcción de KB y el fine‑tuning.</p>

<h2 id="iii-plan-de-entrenamiento">III. Plan de entrenamiento</h2>

<ol>
<li><b>Entrenamiento en etapas</b></li>
</ol>
<ul>
<li><b>Etapa 1 (línea base):</b> Adaptadores sobre ≈45k tripletas. Lote 32 por tarjeta; ~30k pasos; métricas de recuperación, BERTScore y tasa de negativa.</li>
<li><b>Etapa 2 (ampliar KB):</b> 50k, 100k+; paralelismo de datos en 8 tarjetas; escalabilidad de la atención rectangular; memoria y latencia.</li>
<li><b>Etapa 3 (relaciones y multi‑salto):</b> Grafo de conocimiento con relaciones; tareas de uno/múltiples saltos y conflictos; explicaciones tipo cadena de pensamiento.</li>
</ul>

<ol>
<li><b>Comparativa de encoders:</b> Soportar BGE/E5/nomic/mpnet/gtr, entrenar y evaluar en la misma KB y comparar calidad de recuperación/razonamiento.</li>
</ol>

<ol>
<li><b>Memoria y velocidad:</b> Aumentar el tamaño de la KB por lotes (1k→10k→50k→100k), registrar curvas de memoria/latencia en NPUs Ascend y estabilidad de respuestas.</li>
</ol>

<h2 id="iv-localizacin-al-chino">IV. Localización al chino</h2>

<ol>
<li><b>Construcción de KB en chino:</b> Enciclopedias/documentos empresariales; extracción de información y enlace de entidades para generar tripletas; listas de entidades/atributos por dominio; incluir atributos conflictivos para pruebas.</li>
<li><b>Codificadores chinos:</b> <b>bge‑large‑zh</b>, <b>sent‑bert‑zh</b>; inferencia con Ascend‑PyTorch; fine‑tuning cuando sea necesario.</li>
<li><b>LLM y adaptadores chinos:</b> Modelos chinos open‑source (ChatGLM3, Yi‑34B); re‑entrenar adaptadores por diferencias de vocab/posicional; plantillas de instrucción en chino natural (p. ej., “请说明…的用途”, “无法在知识库中找到相关信息”).</li>
<li><b>Razonamiento multi‑salto y explicaciones:</b> Incluir relaciones entre entidades; preguntas que requieran componer varios tokens y producir cadenas explicativas.</li>
<li><b>Evaluación y seguridad:</b> Además de precisión, robustez ante ruido/conflictos; desensibilizar conocimiento sensible para evitar filtraciones.</li>
</ol>

<h2 id="v-mejoras-y-lneas-futuras">V. Mejoras y líneas futuras</h2>

<ol>
<li><b>Recuperación jerárquica y mezclas:</b> Usar tokens de conocimiento como índice (coarse con FAISS/Annoy y reranking con atención rectangular) o una red de compuertas (estilo MoE) para activar solo bloques relevantes.</li>
<li><b>Codificación que preserve estructura:</b> Más allá del pooling fijo, explorar codificadores secuencia‑grafo de longitud variable que conserven orden, números y relaciones, favoreciendo el razonamiento multi‑salto.</li>
<li><b>Compresión y selección adaptativas:</b> Presupuestos de dimensión por token según frecuencia/confianza/relevancia; saltar inyecciones irrelevantes; muestreo tipo atención deformable para reducir cómputo.</li>
<li><b>Ajuste instruccional rico y cadena de pensamiento:</b> De QA de un salto a multi‑salto, conflictos, contrafactuales y explicaciones, con entrenamiento por etapas y plantillas CoT.</li>
<li><b>Herramientas externas y verificación:</b> Módulo de verificación factual (SPARQL/grafo); auto‑consistencia sobre tokens seleccionados; combinar con recuperación de documentos/web cuando convenga.</li>
<li><b>Adaptación por dominio y modelos abiertos/cerrados:</b> KBs e instrucciones por dominio; LLMs enfocados (FinGPT/MedGPT) o APIs manteniendo la KB local.</li>
<li><b>Expansión al chino y multilingüe:</b> Encoders multilingües (multi‑qa‑mpnet‑base‑dot‑v1, gtr‑xxl) para un espacio unificado; etiquetas de idioma en tokens y adaptadores condicionales por idioma.</li>
</ol>

<h2 id="conclusin">Conclusión</h2>

<p>KBLaM ofrece una vía end‑to‑end para entretejer conocimiento externo en LLMs mediante tokens de conocimiento y atención rectangular. Pese a retos de entorno offline, hardware heterogéneo y localización al chino, al sustituir codificadores/LLMs, re‑escribir kernels y diseñar datos sintéticos locales, validamos la viabilidad de KBLaM y planificamos la siguiente fase. Con avances en recuperación híbrida, codificación estructural, compresión adaptativa y razonamiento en cadena, KBLaM puede ser un puente eficiente entre bases de conocimiento y LLMs para la gestión del conocimiento y el QA.</p>

<h2 id="referencias">Referencias</h2>

<p>[1] Taketomo Isazawa, Xi Wang, Liana Mikaelyan, Mathew Salvaris, James Hensman. “KBLaM: Knowledge Base Augmented Language Model.” Proceedings of 2025.</p>

<p>[2] Revisión de LLMs aumentados con bases de conocimiento. 2023.</p>

<p>[3] Naman Bansal. “Best Open‑Source Embedding Models Benchmarked and Ranked.” Supermemory Blog, 2025.</p>

<p>[4] Microsoft Research. “Introducing KBLaM: Bringing plug‑and‑play external knowledge to LLMs.” Microsoft Research Blog, 2025.</p>

        </article>
        <div class="share-toolbar card" style="margin-top:24px;padding:12px 16px;display:flex;gap:8px;align-items:center;flex-wrap:wrap">
          <strong class="share-title" data-i18n="share_label">Compartir</strong>
          <div class="spacer" style="flex:0 0 8px"></div>
          <button class="btn outline share-btn" data-share="wechat">
            <svg class="icon" viewBox="0 0 24 24" aria-hidden="true" focusable="false"><path d="M7.5 3C4.46 3 2 5.08 2 7.65c0 1.52.84 2.88 2.15 3.8l-.53 1.93 2.06-1.24c.56.14 1.15.21 1.77.21 3.04 0 5.5-2.08 5.5-4.65S10.54 3 7.5 3zm-1.4 3.6a.9.9 0 110 1.8.9.9 0 010-1.8zm3.8 0a.9.9 0 110 1.8.9.9 0 010-1.8zM16.5 10c-2.86 0-5.17 1.86-5.17 4.15 0 1.27.7 2.4 1.78 3.17l-.44 1.6 1.72-1.03c.47.12.97.18 1.48.18 2.86 0 5.17-1.86 5.17-4.15S19.36 10 16.5 10zm-1.2 2.7a.9.9 0 110 1.8.9.9 0 010-1.8zm3.6 0a.9.9 0 110 1.8.9.9 0 010-1.8z" fill="currentColor" stroke="none"></path></svg>
            <span data-i18n="share_wechat">WeChat</span>
          </button>
          <a class="btn outline share-btn" data-share="whatsapp" target="_blank" rel="noopener"><svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M20 12a8 8 0 1 1-14.32 4.906L4 21l4.2-1.11A8 8 0 1 1 20 12z" fill="none" stroke="currentColor" stroke-width="1.8" stroke-linecap="round" stroke-linejoin="round"/><path d="M8.5 9.5c.5 2 2.5 3.5 4 4l1.2-.8c.3-.2.7-.1.9.2l.7 1.1c.2.3.1.7-.2.9-1 .7-2.1 1.1-3.3 1.1-2.9 0-5.3-2.4-5.3-5.3 0-1.2.4-2.3 1.1-3.3.2-.3.6-.4.9-.2l1.1.7c.3.2.4.6.2.9l-.8 1.2z"/></svg>
            <span data-i18n="share_whatsapp">WhatsApp</span></a>
          <button class="btn outline share-btn" data-share="copy"><svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><rect x="9" y="9" width="10" height="10" rx="2"/><rect x="5" y="5" width="10" height="10" rx="2"/></svg>
            <span data-i18n="share_copy">Copiar enlace</span></button>
          <a class="btn outline share-btn" data-share="download" download><svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 3v10"/><path d="M8 9l4 4 4-4"/><path d="M5 21h14"/></svg>
            <span data-i18n="share_download">Descargar portada</span></a>
          <button class="btn outline share-btn" data-share="native"><svg class="icon" viewBox="0 0 24 24" aria-hidden="true"><path d="M4 12v7a1 1 0 0 0 1 1h14a1 1 0 0 0 1-1v-7"/><path d="M12 16V3"/><path d="M8 7l4-4 4 4"/></svg>
            <span data-i18n="share_share">Compartir…</span></button>
        </div>
        <div id="share-modal" class="modal" hidden>
          <div class="modal-content card" role="dialog" aria-modal="true" aria-labelledby="share-title">
            <div style="display:flex;align-items:center;justify-content:space-between;gap:12px">
              <h3 id="share-title" data-i18n="share_wechat">WeChat</h3>
              <button class="btn outline" data-close><span data-i18n="share_close">Cerrar</span></button>
            </div>
            <p class="muted" style="margin:8px 0" data-i18n="share_wechat_qr_tip">Escanea en WeChat para compartir</p>
            <div id="qr" style="display:grid;place-items:center;padding:12px"></div>
          </div>
        </div>
        <hr style="margin: 24px 0">
        <nav class="post-nav" aria-label="Post navigation">
          <a class="btn outline" href="../blog.html">← Volver al blog</a>
          <span class="muted" style="margin:0 .5rem">·</span>
          <a class="btn outline" href="#" aria-disabled="true" onclick="return false;">Anterior</a>
          <a class="btn outline" href="#" aria-disabled="true" onclick="return false;">Siguiente</a>
        </nav>
      </div>
    </section>
  </main>
  <footer>
    <div class="container"><p>© <span id="year"></span> Fan Wan</p></div>
  </footer>
  <script>
    (function(){
      if (window.hljs) { try { window.hljs.highlightAll(); } catch(e){} }
      function render(){ try { if (window.renderMathInElement) window.renderMathInElement(document.body, { delimiters:[{left:'$$', right:'$$', display:true},{left:'$', right:'$', display:false}] }); } catch(e){} }
      if (document.readyState === 'loading') document.addEventListener('DOMContentLoaded', render); else render();
    })();
  </script>
</body>
</html>