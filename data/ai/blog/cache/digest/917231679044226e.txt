百度AI研究团队推出的ERNIE-4.5-21B-A3B-Thinking模型，旨在填补现有大型语言模型在推理效率和长文本处理能力方面的不足。该模型采用混合专家（MoE）架构，具备21亿参数，但每个token仅激活30亿参数，从而实现了计算效率与推理能力的平衡。其关键创新在于高效的参数使用和工具集成能力，显著提升了深度推理的表现。实验结果表明，该模型在推理任务上优于主流基线，展示了其在实际应用中的潜力。此研究为未来语言模型的设计提供了新的思路，尤其在需要高效推理的场景中具有广泛的适用性。