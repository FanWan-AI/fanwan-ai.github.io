大型语言模型（LLMs）的快速发展暴露了其在动态信息丰富环境中的脆弱性，尤其是在多步推理任务中的认知负荷限制尚未得到充分理解。为此，研究者提出了一种形式化的计算认知负荷理论，强调了无关信息的干扰和任务切换对模型性能的影响，并设计了“交错认知评估”（ICE）基准，以系统性地操控这些负荷因素。关键创新在于通过ICE基准揭示了不同模型在认知负荷下的表现差异，尤其是小型开源架构在高负荷任务中表现出明显的脆弱性，而某些模型则在控制条件下展现出一定的韧性。这些结果初步证明了认知负荷是推理失败的重要因素，支持了在不确定性下的猜测理论。研究强调了动态认知压力测试的重要性，为评估先进AI系统的韧性和安全性提供了新的视角，具有广泛的应用潜力。