本研究旨在评估大型语言模型（LLMs）在临床实验室测试解释中的因果推理能力，填补了现有文献在此领域的空白。研究采用99个与Pearl因果层级相对应的临床场景，分析了常见实验室测试及其相关因果因素，并对两种LLM（GPT-o1和Llama-3.2-8b-instruct）进行了评估，结果由四位医学专家进行审查。关键创新点在于系统性地将因果推理框架应用于临床场景，以及通过专家评估提供了更为客观的性能比较。结果显示，GPT-o1在整体表现和各个因果推理维度上均优于Llama-3.2-8b-instruct，尤其在干预问题上表现最佳。该研究为未来在高风险临床应用中使用LLMs提供了参考，但仍需进一步优化其推理能力，以确保在实际应用中的可靠性。