随着生成性人工智能模型的广泛应用，合成数据的生成速度显著加快，导致模型在重复训练中出现性能退化现象，即模型崩溃。现有研究虽然探讨了模型崩溃的原因和检测方法，但有效的缓解策略仍然不足。本文提出了一种新的损失函数——截断交叉熵（TCE），通过降低高置信度预测的权重，显著延缓了模型崩溃的发生。研究表明，该方法不仅在理论和实证上有效，而且能够在不同模型和模态中推广应用，延长模型的有效性区间超过2.3倍。这一发现强调了损失函数设计在应对合成数据时代模型质量保持中的重要性，具有广泛的应用潜力。