随着大型语言模型（LLMs）在基准测试和奖励建模中被广泛应用，确保其可靠性和效率变得至关重要。本文通过对开放源代码的Qwen 3模型进行系统比较，探讨了“思考型”与“非思考型”LLM在作为评判者时的表现，评估了其准确性和计算效率，并考察了多种增强策略。研究发现，尽管非思考型模型经过增强后有所改善，但其准确性和鲁棒性仍显著低于思考型模型，后者在准确性上平均提高约10%，且计算开销较小。该研究的关键创新在于明确展示了显性推理在评判任务中的优势，尤其是在多语言环境下的适用性。此研究为LLM在评判领域的应用提供了系统性证据，强调了显性推理在提升模型表现方面的重要性。