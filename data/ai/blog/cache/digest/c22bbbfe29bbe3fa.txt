本研究关注在使用大型语言模型（LLMs）作为评判者时，如何有效监控其评估的准确性，尤其是在缺乏真实标准的情况下。研究提出了一种基于逻辑一致性的评估方法，通过分析多个LLM评判者之间的意见分歧，构建线性规划模型来识别评判者的潜在失误。关键创新在于提出了“无知识警报”机制，能够在不产生假阳性的情况下，准确检测出至少一位评判者未能满足用户设定的评分能力要求。实验结果表明，该方法在检测评判者一致性方面优于现有主流基线，具有较高的实用性。该研究为评估系统的可靠性提供了新的思路，具有广泛的应用潜力，尤其是在需要多模型评估的复杂决策场景中。