本研究聚焦于可解释人工智能（XAI）中的不确定性解释，填补了现有文献中对全球解释关注不足的空白。研究采用了一种综合考虑不确定性、鲁棒性和全球可解释性的算法，旨在校准用户对AI系统的信任。关键创新在于提出了针对不确定性的解释框架，并验证了复杂算法在提供直观视觉理解方面的有效性。结果表明，该算法在用户满意度和人类可解释性方面优于主流基线，显示出其在实际应用中的潜力。该研究为XAI领域提供了新的指导思路，尤其在增强用户信任和理解方面具有重要意义。