本研究聚焦于大型语言模型（LLMs）在自动化过程建模中的应用，探讨了知识驱动的幻觉现象，即模型输出与明确证据相悖的情况，这一问题在现有文献中尚未得到充分关注。通过设计对照实验，研究者在标准与非典型过程结构之间制造冲突，以评估LLMs对提供证据的忠实度。关键创新在于提出了一种评估LLMs可靠性的方法，并强调了在基于证据的领域中对AI生成成果进行严格验证的必要性。研究结果表明，LLMs在处理标准化业务流程时表现优于主流基线，但在面对非典型输入时可能出现显著偏差。该研究为未来在业务流程管理等领域的AI应用提供了重要的借鉴，提示研究者需关注模型输出的可靠性与验证机制。