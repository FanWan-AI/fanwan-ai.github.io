随着大型语言模型（LLMs）在渗透测试中的应用日益增多，其在不同攻击阶段的有效性和可靠性仍存在不确定性。本研究通过对多种LLM代理的全面评估，分析了单一代理与模块化设计在真实渗透测试场景中的表现，并识别出五种核心功能能力的影响。关键创新在于通过有针对性的增强措施显著提升了模块化代理的性能，尤其是在复杂的多步骤和实时任务中。研究结果表明，尽管某些架构在某些功能上表现良好，但通过增强措施可以显著改善其整体表现。这一方法为渗透测试领域提供了新的思路，可能推动LLM在安全领域的更广泛应用。