{
  "date": "2025-09-19",
  "updated_at": "2025-09-18T22:06:45.745Z",
  "items": [
    {
      "id": "twbs/bootstrap",
      "source": "github",
      "name": "bootstrap",
      "url": "https://github.com/twbs/bootstrap",
      "license": "MIT",
      "lang": "MDX",
      "tags": [
        "bootstrap",
        "css",
        "css-framework",
        "html",
        "javascript",
        "sass",
        "scss"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 173434,
        "forks": 79155,
        "issues": 577
      },
      "score": 189364.49197584877,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Bootstrap 是一个开源前端框架，用于快速构建响应式、移动优先的网页项目。它提供了一套完整的 HTML、CSS 和 JavaScript 组件，包括网格系统、表单、按钮和导航等，帮助开发者高效实现一致的用户界面。其核心亮点在于强大的响应式设计能力，支持跨设备适配，同时具备高度可定制性和丰富的预定义样式。适用于各类 Web 应用开发，尤其适合需要快速原型设计或统一视觉规范的项目。",
      "updated_at": "2025-09-18T15:42:19Z",
      "summary_en": "Bootstrap is a widely-used front-end framework for building responsive, mobile-first websites. It provides a comprehensive set of CSS and JavaScript components, such as grids, forms, and navigation bars, to streamline development. Its key strengths include ease of use, extensive documentation, and a large community for support. It is ideal for developers seeking to create consistent, cross-device compatible web projects efficiently.",
      "summary_zh": "Bootstrap 是一个开源前端框架，用于快速构建响应式、移动优先的网页项目。它提供了一套完整的 HTML、CSS 和 JavaScript 组件，包括网格系统、表单、按钮和导航等，帮助开发者高效实现一致的用户界面。其核心亮点在于强大的响应式设计能力，支持跨设备适配，同时具备高度可定制性和丰富的预定义样式。适用于各类 Web 应用开发，尤其适合需要快速原型设计或统一视觉规范的项目。",
      "summary_es": "Bootstrap es un framework front-end para crear sitios web responsivos y mobile-first. Incluye componentes predefinidos, sistema de grid y utilidades CSS/JS. Su principal fortaleza es la rapidez de desarrollo y consistencia cross-browser. Ideal para prototipado y proyectos que priorizan compatibilidad."
    },
    {
      "id": "avelino/awesome-go",
      "source": "github",
      "name": "awesome-go",
      "url": "https://github.com/avelino/awesome-go",
      "license": "MIT",
      "lang": "Go",
      "tags": [
        "awesome",
        "awesome-list",
        "go",
        "golang",
        "golang-library",
        "hacktoberfest"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 152639,
        "forks": 12574,
        "issues": 157
      },
      "score": 155253.73078757714,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "这是一个精选的 Go 语言资源集合，收录了大量高质量的框架、库和软件项目。它覆盖了 Go 生态系统的多个领域，包括网络开发、数据库、工具链和实用程序等。该项目由社区共同维护，内容持续更新，适合 Go 开发者快速查找和评估可用资源。无论是初学者寻找入门工具，还是资深开发者探索高级库，都能从中获得实用参考。",
      "updated_at": "2025-09-18T18:51:53Z",
      "summary_en": "A curated collection of high-quality Go frameworks, libraries, and software, organized by category. Ideal for developers seeking reliable tools for building applications in Go, from web development to system utilities. Its strength lies in community-driven curation and regular updates, ensuring relevance and quality. Essential for Go programmers at any level to discover and evaluate resources efficiently.",
      "summary_zh": "这是一个精选的 Go 语言资源集合，收录了大量高质量的框架、库和软件项目。它覆盖了 Go 生态系统的多个领域，包括网络开发、数据库、工具链和实用程序等。该项目由社区共同维护，内容持续更新，适合 Go 开发者快速查找和评估可用资源。无论是初学者寻找入门工具，还是资深开发者探索高级库，都能从中获得实用参考。",
      "summary_es": "Lista curada de frameworks, bibliotecas y software destacados en Go. Facilita el descubrimiento de herramientas para desarrollo backend, sistemas distribuidos y microservicios. Ideal para desarrolladores que buscan recursos confiables y comunitariamente validados."
    },
    {
      "id": "trimstray/the-book-of-secret-knowledge",
      "source": "github",
      "name": "the-book-of-secret-knowledge",
      "url": "https://github.com/trimstray/the-book-of-secret-knowledge",
      "license": "MIT",
      "lang": "N/A",
      "tags": [
        "awesome",
        "awesome-list",
        "bsd",
        "cheatsheets",
        "devops",
        "guidelines",
        "hacking",
        "hacks",
        "howtos",
        "linux",
        "lists",
        "manuals",
        "one-liners",
        "pentesters",
        "resources",
        "search-engines",
        "security",
        "security-researchers",
        "sysops"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 186629,
        "forks": 11545,
        "issues": 99
      },
      "score": 189037.99795578702,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "《the-book-of-secret-knowledge》是一个综合性的技术知识集合库，收录了大量实用资源，包括速查表、操作指南、命令行工具和博客等。该项目特别适合开发者和运维人员使用，内容涵盖Linux系统管理、DevOps实践以及安全攻防技巧。其亮点在于整理了大量高质量的一手资料和实用技巧，帮助用户快速解决实际问题。无论是日常开发还是系统维护，都能从中找到有价值的参考信息。",
      "updated_at": "2025-09-18T19:20:54Z",
      "summary_en": "The Book of Secret Knowledge is a comprehensive, community-driven repository of practical IT and security resources. It includes cheatsheets, guides, tools, and one-liners for system administration, DevOps, and cybersecurity. Its strength lies in its extensive, curated content and ease of access for quick reference. This makes it highly applicable for professionals and enthusiasts seeking efficient solutions and best practices.",
      "summary_zh": "《the-book-of-secret-knowledge》是一个综合性的技术知识集合库，收录了大量实用资源，包括速查表、操作指南、命令行工具和博客等。该项目特别适合开发者和运维人员使用，内容涵盖Linux系统管理、DevOps实践以及安全攻防技巧。其亮点在于整理了大量高质量的一手资料和实用技巧，帮助用户快速解决实际问题。无论是日常开发还是系统维护，都能从中找到有价值的参考信息。",
      "summary_es": "El proyecto es una colección extensa de recursos técnicos, incluyendo listas, manuales, hojas de referencia y herramientas CLI/web. Destaca por su enfoque en DevOps, hacking y administración de sistemas Linux. Es ideal para profesionales que buscan referencias rápidas, mejores prácticas y soluciones prácticas. Su estructura clara y contenido diverso lo convierten en una guía valiosa para desarrolladores y administradores de sistemas."
    },
    {
      "id": "ohmyzsh/ohmyzsh",
      "source": "github",
      "name": "ohmyzsh",
      "url": "https://github.com/ohmyzsh/ohmyzsh",
      "license": "MIT",
      "lang": "Shell",
      "tags": [
        "cli",
        "cli-app",
        "hacktoberfest",
        "oh-my-zsh",
        "oh-my-zsh-plugin",
        "oh-my-zsh-theme",
        "ohmyzsh",
        "plugin-framework",
        "plugins",
        "productivity",
        "shell",
        "terminal",
        "theme",
        "themes",
        "zsh",
        "zsh-configuration"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 181487,
        "forks": 26217,
        "issues": 504
      },
      "score": 186830.2595298611,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Ohmyzsh 是一个基于 Zsh 的社区驱动型 Shell 配置管理框架，拥有超过 2400 名贡献者。它提供了 300 多个可选插件，支持多种开发环境和工具（如 Git、Docker、Python 等），以及 140 多种主题，帮助用户自定义终端外观。内置的自动更新工具可轻松获取社区最新功能。适用于希望提升终端效率和美观度的开发者和系统管理员。",
      "updated_at": "2025-09-18T18:21:06Z",
      "summary_en": "Oh My Zsh is a community-driven framework for managing Zsh configurations, offering extensive customization and productivity enhancements. It includes over 300 plugins for tools like Git, Docker, and Python, and 140+ themes to personalize the terminal experience. Its auto-update feature ensures users stay current with community contributions. Ideal for developers and power users seeking an efficient, extensible command-line environment.",
      "summary_zh": "Ohmyzsh 是一个基于 Zsh 的社区驱动型 Shell 配置管理框架，拥有超过 2400 名贡献者。它提供了 300 多个可选插件，支持多种开发环境和工具（如 Git、Docker、Python 等），以及 140 多种主题，帮助用户自定义终端外观。内置的自动更新工具可轻松获取社区最新功能。适用于希望提升终端效率和美观度的开发者和系统管理员。",
      "summary_es": "Ohmyzsh es un framework comunitario para gestionar configuraciones de Zsh. Ofrece más de 300 plugins y 140 temas, mejorando productividad en terminal. Incluye herramientas para Git, Docker y actualizaciones automáticas. Ideal para desarrolladores que buscan personalizar y optimizar su línea de comandos."
    },
    {
      "id": "github/gitignore",
      "source": "github",
      "name": "gitignore",
      "url": "https://github.com/github/gitignore",
      "license": "CC0-1.0",
      "lang": "N/A",
      "tags": [
        "git",
        "gitignore"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 169442,
        "forks": 83000,
        "issues": 319
      },
      "score": 186141.72542492283,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "这是一个由 GitHub 官方维护的 .gitignore 模板集合，适用于各种编程语言、框架和开发环境。它可以帮助开发者在 Git 版本控制中自动忽略不必要的文件，如编译产物、日志文件或依赖目录，从而保持仓库的整洁。该项目采用 CC0-1.0 协议，完全开放使用，并支持社区贡献。无论是个人项目还是团队协作，都可以通过直接引用或自定义这些模板来提升开发效率。",
      "updated_at": "2025-09-18T17:23:10Z",
      "summary_en": "A comprehensive collection of .gitignore templates for various programming languages, frameworks, and tools. It helps developers exclude unnecessary files from version control, reducing repository clutter and improving performance. Widely used across open-source and private projects to maintain clean Git histories. Essential for any developer working with Git to streamline workflows and avoid committing sensitive or irrelevant files.",
      "summary_zh": "这是一个由 GitHub 官方维护的 .gitignore 模板集合，适用于各种编程语言、框架和开发环境。它可以帮助开发者在 Git 版本控制中自动忽略不必要的文件，如编译产物、日志文件或依赖目录，从而保持仓库的整洁。该项目采用 CC0-1.0 协议，完全开放使用，并支持社区贡献。无论是个人项目还是团队协作，都可以通过直接引用或自定义这些模板来提升开发效率。",
      "summary_es": "Colección de plantillas .gitignore para múltiples lenguajes y entornos. Simplifica la exclusión de archivos innecesarios en repositorios Git, mejorando la limpieza y eficiencia. Ideal para desarrolladores que buscan configuraciones predefinidas y evitar commits accidentales de archivos sensibles o temporales."
    },
    {
      "id": "flutter/flutter",
      "source": "github",
      "name": "flutter",
      "url": "https://github.com/flutter/flutter",
      "license": "BSD-3-Clause",
      "lang": "Dart",
      "tags": [
        "android",
        "app-framework",
        "cross-platform",
        "dart",
        "dart-platform",
        "desktop",
        "flutter",
        "flutter-package",
        "fuchsia",
        "ios",
        "linux-desktop",
        "macos",
        "material-design",
        "mobile",
        "mobile-development",
        "skia",
        "web",
        "web-framework",
        "windows"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 172656,
        "forks": 29224,
        "issues": 12248
      },
      "score": 178600.78684467592,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Flutter 是由 Google 开发的开源 UI 框架，使用 Dart 语言编写，旨在帮助开发者高效构建高质量、跨平台的移动、Web 和桌面应用。其核心亮点在于高性能的渲染引擎和丰富的组件库，支持一套代码多端部署，显著提升开发效率。Flutter 适用于需要快速迭代、追求一致用户体验的应用场景，尤其受到移动应用开发者的青睐。通过热重载功能，开发者可以实时查看界面修改效果，极大简化了调试和界面设计流程。",
      "updated_at": "2025-09-18T19:16:06Z",
      "summary_en": "Flutter is an open-source UI framework for building natively compiled applications for mobile, web, and desktop from a single codebase. It uses the Dart programming language and provides a rich set of pre-designed widgets for creating visually appealing and performant apps. Its hot reload feature accelerates development, enabling real-time updates without restarting the app. Ideal for cross-platform projects, it is widely adopted for creating consistent user experiences across iOS, Android, and other platforms.",
      "summary_zh": "Flutter 是由 Google 开发的开源 UI 框架，使用 Dart 语言编写，旨在帮助开发者高效构建高质量、跨平台的移动、Web 和桌面应用。其核心亮点在于高性能的渲染引擎和丰富的组件库，支持一套代码多端部署，显著提升开发效率。Flutter 适用于需要快速迭代、追求一致用户体验的应用场景，尤其受到移动应用开发者的青睐。通过热重载功能，开发者可以实时查看界面修改效果，极大简化了调试和界面设计流程。",
      "summary_es": "Flutter es un framework de código abierto para crear aplicaciones nativas compiladas, multi-plataforma desde una única base de código. Utiliza el lenguaje Dart y ofrece alto rendimiento con renderizado propio. Es ideal para desarrollo móvil (iOS/Android), web y escritorio, destacando por su hot reload y widgets personalizables."
    },
    {
      "id": "AUTOMATIC1111/stable-diffusion-webui",
      "source": "github",
      "name": "stable-diffusion-webui",
      "url": "https://github.com/AUTOMATIC1111/stable-diffusion-webui",
      "license": "AGPL-3.0",
      "lang": "Python",
      "tags": [
        "ai",
        "ai-art",
        "deep-learning",
        "diffusion",
        "gradio",
        "image-generation",
        "image2image",
        "img2img",
        "pytorch",
        "stable-diffusion",
        "text2image",
        "torch",
        "txt2img",
        "unstable",
        "upscaling",
        "web"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 156514,
        "forks": 29043,
        "issues": 2425
      },
      "score": 162422.56327214508,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Stable Diffusion web UI 是一个基于 Gradio 构建的 Web 界面，用于运行和自定义 Stable Diffusion 图像生成模型。它支持文本生成图像、图像到图像转换、图像修复等多种功能，并提供了丰富的插件和自定义选项。该项目适用于 AI 艺术创作、设计辅助和图像处理实验等场景，用户无需编写代码即可使用高级功能。其开源特性允许开发者扩展和优化界面，适合对 AI 生成图像感兴趣的技术爱好者和创作者。",
      "updated_at": "2025-09-18T19:05:55Z",
      "summary_en": "Stable Diffusion web UI is a popular open-source interface for generating and manipulating images using AI. It supports text-to-image, image-to-image, and inpainting workflows, making it versatile for creative and practical applications. Built on Gradio and PyTorch, it is user-friendly and highly extensible, appealing to both beginners and developers. Its AGPL-3.0 license ensures open access while encouraging community contributions.",
      "summary_zh": "Stable Diffusion web UI 是一个基于 Gradio 构建的 Web 界面，用于运行和自定义 Stable Diffusion 图像生成模型。它支持文本生成图像、图像到图像转换、图像修复等多种功能，并提供了丰富的插件和自定义选项。该项目适用于 AI 艺术创作、设计辅助和图像处理实验等场景，用户无需编写代码即可使用高级功能。其开源特性允许开发者扩展和优化界面，适合对 AI 生成图像感兴趣的技术爱好者和创作者。",
      "summary_es": "Interfaz web para Stable Diffusion que permite generar y transformar imágenes mediante IA. Destaca por su interfaz accesible, múltiples funciones avanzadas y amplia personalización. Ideal para artistas digitales, investigadores y entusiastas que buscan explorar modelos de difusión sin requerir conocimientos técnicos profundos."
    },
    {
      "id": "Snailclimb/JavaGuide",
      "source": "github",
      "name": "JavaGuide",
      "url": "https://github.com/Snailclimb/JavaGuide",
      "license": "Apache-2.0",
      "lang": "Java",
      "tags": [
        "algorithms",
        "interview",
        "java",
        "jvm",
        "mysql",
        "redis",
        "spring",
        "system",
        "system-design",
        "zookeeper"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 151793,
        "forks": 45987,
        "issues": 73
      },
      "score": 161090.1819835648,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "JavaGuide 是一个面向 Java 开发者的开源学习与面试指南项目，内容涵盖 Java 核心知识、JVM、MySQL、Redis、Spring 等关键技术栈。该项目以系统化的方式整理了大量面试常见问题和知识点解析，适合准备 Java 相关岗位的求职者以及希望巩固技术基础的开发者。其亮点在于内容全面、结构清晰，并持续更新维护，帮助用户高效掌握 Java 生态的核心概念。无论是面试准备还是日常学习，JavaGuide 都是一个实用的参考资源。",
      "updated_at": "2025-09-18T17:47:36Z",
      "summary_en": "JavaGuide is a comprehensive open-source guide for Java developers, covering core knowledge areas like JVM, algorithms, databases, and system design. It serves as a valuable resource for interview preparation and skill enhancement, with practical examples and clear explanations. Its broad applicability makes it suitable for both beginners and experienced programmers looking to deepen their understanding of Java and related technologies. The project is well-maintained and widely adopted in the developer community.",
      "summary_zh": "JavaGuide 是一个面向 Java 开发者的开源学习与面试指南项目，内容涵盖 Java 核心知识、JVM、MySQL、Redis、Spring 等关键技术栈。该项目以系统化的方式整理了大量面试常见问题和知识点解析，适合准备 Java 相关岗位的求职者以及希望巩固技术基础的开发者。其亮点在于内容全面、结构清晰，并持续更新维护，帮助用户高效掌握 Java 生态的核心概念。无论是面试准备还是日常学习，JavaGuide 都是一个实用的参考资源。",
      "summary_es": "JavaGuide es una guía integral para desarrolladores Java, que cubre conocimientos esenciales como algoritmos, JVM, bases de datos y frameworks. Destaca por su enfoque práctico para preparación de entrevistas y aprendizaje autodidacta. Incluye ejemplos claros y temas de diseño de sistemas. Ideal para estudiantes y profesionales que buscan dominar el ecosistema Java."
    },
    {
      "id": "huggingface/transformers",
      "source": "github",
      "name": "transformers",
      "url": "https://github.com/huggingface/transformers",
      "license": "Apache-2.0",
      "lang": "Python",
      "tags": [
        "audio",
        "deep-learning",
        "deepseek",
        "gemma",
        "glm",
        "hacktoberfest",
        "llm",
        "machine-learning",
        "model-hub",
        "natural-language-processing",
        "nlp",
        "pretrained-models",
        "python",
        "pytorch",
        "pytorch-transformers",
        "qwen",
        "speech-recognition",
        "transformer",
        "vlm"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 149950,
        "forks": 30449,
        "issues": 1980
      },
      "score": 156139.58302523146,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "🤗 Transformers 是一个基于 Apache-2.0 协议的开源框架，专注于提供最先进的机器学习模型，涵盖文本、视觉、音频和多模态任务。它支持推理和训练，集成了众多前沿模型如 DeepSeek、Gemma 和 GLM，并提供了统一的模型定义接口。该框架适用于自然语言处理、音频分析及多模态研究，广泛应用于学术和工业场景。其模型中心（Model Hub）允许用户轻松共享和部署预训练模型，极大提升了开发效率。",
      "updated_at": "2025-09-18T17:48:03Z",
      "summary_en": "🤗 Transformers is a widely-used open-source library for state-of-the-art machine learning models across text, vision, audio, and multimodal tasks. It supports both inference and training, offering pre-trained models and tools for fine-tuning. Its strengths include extensive model availability, ease of use, and community-driven improvements. Ideal for researchers and developers in NLP, computer vision, and audio processing.",
      "summary_zh": "🤗 Transformers 是一个基于 Apache-2.0 协议的开源框架，专注于提供最先进的机器学习模型，涵盖文本、视觉、音频和多模态任务。它支持推理和训练，集成了众多前沿模型如 DeepSeek、Gemma 和 GLM，并提供了统一的模型定义接口。该框架适用于自然语言处理、音频分析及多模态研究，广泛应用于学术和工业场景。其模型中心（Model Hub）允许用户轻松共享和部署预训练模型，极大提升了开发效率。",
      "summary_es": "Transformers es un framework de código abierto para modelos de aprendizaje profundo en texto, visión, audio y multimodal. Ofrece una amplia gama de modelos preentrenados y herramientas para inferencia y entrenamiento. Sus puntos fuertes incluyen flexibilidad, escalabilidad y soporte para múltiples modalidades. Es ampliamente utilizado en procesamiento de lenguaje natural, generación de contenido y análisis multimodal."
    },
    {
      "id": "omni-research/Tarsier2-Recap-7b",
      "source": "hf",
      "name": "Tarsier2-Recap-7b",
      "url": "https://huggingface.co/omni-research/Tarsier2-Recap-7b",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "safetensors",
        "video LLM",
        "arxiv:2501.07888",
        "license:apache-2.0",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 8475547,
        "hf_likes": 17
      },
      "score": 16959.594,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Tarsier2-Recap-7b 是一个基于 Transformer 架构的视频语言模型，专注于视频内容理解与摘要生成。该模型能够解析视频帧序列，并自动生成简洁准确的文字总结，适用于视频内容检索、智能剪辑辅助等场景。其核心亮点在于结合时序建模与语义理解，支持对长视频的高效处理，同时保持较低的推理延迟。该模型适用于媒体制作、在线教育、安防监控等领域，帮助用户快速提取视频关键信息。",
      "updated_at": "2025-09-18T19:21:47.074Z",
      "summary_en": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for summarizing and analyzing video content. It excels at generating concise recaps and extracting key information from videos, making it suitable for applications in media analysis, content indexing, and accessibility. Its strengths include efficient processing and robust performance on diverse video datasets. The model is licensed under Apache 2.0, ensuring broad usability for both research and commercial purposes.",
      "summary_zh": "Tarsier2-Recap-7b 是一个基于 Transformer 架构的视频语言模型，专注于视频内容理解与摘要生成。该模型能够解析视频帧序列，并自动生成简洁准确的文字总结，适用于视频内容检索、智能剪辑辅助等场景。其核心亮点在于结合时序建模与语义理解，支持对长视频的高效处理，同时保持较低的推理延迟。该模型适用于媒体制作、在线教育、安防监控等领域，帮助用户快速提取视频关键信息。",
      "summary_es": "Tarsier2-Recap-7b es un modelo de lenguaje de gran tamaño especializado en el procesamiento y resumen de contenido de video. Basado en la arquitectura Transformer, destaca por su capacidad para generar descripciones precisas y contextualizadas de secuencias visuales. Sus principales aplicaciones incluyen la automatización de subtítulos, análisis de metraje y extracción de información clave en entornos multimedia. Optimizado para eficiencia computacional, es adecuado para implementaciones que requieren equilibrio entre rendimiento y recursos."
    },
    {
      "id": "meta-llama/Llama-3.1-8B-Instruct",
      "source": "hf",
      "name": "Llama-3.1-8B-Instruct",
      "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "facebook",
        "meta",
        "pytorch",
        "llama-3",
        "conversational",
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th",
        "arxiv:2204.05149",
        "base_model:meta-llama/Llama-3.1-8B",
        "base_model:finetune:meta-llama/Llama-3.1-8B",
        "license:llama3.1",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 6885240,
        "hf_likes": 4631
      },
      "score": 16085.98,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Llama-3.1-8B-Instruct 是 Meta 推出的开源指令微调语言模型，基于 Llama 3 架构优化，专门用于对话和指令跟随任务。该模型具有 80 亿参数，在多项基准测试中表现出色，尤其擅长生成自然、连贯的文本响应。它适用于聊天机器人、内容创作、代码生成等多种场景，并支持 Transformers 和 PyTorch 框架集成。模型以英文为主，适合开发者构建高质量的 AI 对话应用。",
      "updated_at": "2025-09-18T19:21:47.074Z",
      "summary_en": "Llama-3.1-8B-Instruct is an 8-billion-parameter language model optimized for instruction-following tasks. It excels in conversational AI, text generation, and structured reasoning, making it suitable for chatbots, content creation, and educational tools. Built on the robust Llama-3 architecture, it balances performance and efficiency for deployment in resource-constrained environments. Its open availability encourages broad use in research and practical applications.",
      "summary_zh": "Llama-3.1-8B-Instruct 是 Meta 推出的开源指令微调语言模型，基于 Llama 3 架构优化，专门用于对话和指令跟随任务。该模型具有 80 亿参数，在多项基准测试中表现出色，尤其擅长生成自然、连贯的文本响应。它适用于聊天机器人、内容创作、代码生成等多种场景，并支持 Transformers 和 PyTorch 框架集成。模型以英文为主，适合开发者构建高质量的 AI 对话应用。",
      "summary_es": "Llama-3.1-8B-Instruct es un modelo de lenguaje de 8 mil millones de parámetros optimizado para instrucciones y conversaciones. Basado en Transformers, destaca por su eficiencia en generación de texto y su capacidad para diálogos naturales. Es ideal para aplicaciones de chatbots, asistentes virtuales y automatización de respuestas en inglés. Su arquitectura permite un buen equilibrio entre rendimiento y consumo computacional."
    },
    {
      "id": "distilbert/distilbert-base-uncased",
      "source": "hf",
      "name": "distilbert-base-uncased",
      "url": "https://huggingface.co/distilbert/distilbert-base-uncased",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "safetensors",
        "distilbert",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1910.01108",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 12201728,
        "hf_likes": 755
      },
      "score": 24780.956000000002,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "DistilBERT-base-uncased 是一个轻量化的 BERT 变体，通过知识蒸馏技术大幅压缩模型规模，同时保持接近原版的性能。它专为英文文本的掩码语言建模任务设计，适用于文本分类、情感分析、实体识别等多种自然语言处理场景。该模型支持多种主流框架，包括 PyTorch、TensorFlow 和 JAX，便于集成到不同技术栈中。其高效推理和较低资源需求使其成为部署在计算受限环境中的理想选择。",
      "updated_at": "2025-09-18T19:21:47.074Z",
      "summary_en": "DistilBERT is a distilled version of BERT, designed for efficient natural language understanding. It retains 97% of BERT's performance while being 40% smaller and 60% faster. Ideal for tasks like text classification, sentiment analysis, and masked language modeling, it is well-suited for resource-constrained environments. Supports multiple frameworks including PyTorch, TensorFlow, and JAX.",
      "summary_zh": "DistilBERT-base-uncased 是一个轻量化的 BERT 变体，通过知识蒸馏技术大幅压缩模型规模，同时保持接近原版的性能。它专为英文文本的掩码语言建模任务设计，适用于文本分类、情感分析、实体识别等多种自然语言处理场景。该模型支持多种主流框架，包括 PyTorch、TensorFlow 和 JAX，便于集成到不同技术栈中。其高效推理和较低资源需求使其成为部署在计算受限环境中的理想选择。",
      "summary_es": "DistilBERT es un modelo de lenguaje basado en BERT, optimizado para reducir su tamaño y tiempo de inferencia manteniendo alta precisión. Es ideal para tareas de procesamiento de lenguaje natural como clasificación de texto, análisis de sentimientos y relleno de máscaras. Su eficiencia lo hace adecuado para entornos con recursos limitados o aplicaciones que requieren baja latencia."
    },
    {
      "id": "FacebookAI/roberta-large",
      "source": "hf",
      "name": "roberta-large",
      "url": "https://huggingface.co/FacebookAI/roberta-large",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "onnx",
        "safetensors",
        "roberta",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1907.11692",
        "arxiv:1806.02847",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 12171005,
        "hf_likes": 245
      },
      "score": 24464.510000000002,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "RoBERTa-large是基于BERT架构优化的预训练语言模型，由Facebook AI开发。它在BERT基础上通过移除下一句预测任务、采用更大批次训练和更长的序列进行改进，提升了语言理解能力。该模型适用于文本分类、实体识别、语义相似度计算等多种自然语言处理任务，尤其擅长处理英文文本的掩码语言建模。支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。",
      "updated_at": "2025-09-18T19:21:47.074Z",
      "summary_en": "RoBERTa-large is a robust transformer-based language model optimized for masked language modeling. It excels in tasks like text classification, named entity recognition, and question answering, offering strong performance across various benchmarks. With support for multiple frameworks (PyTorch, TensorFlow, JAX, ONNX), it is highly adaptable for research and production use. Its pre-trained nature makes it ideal for fine-tuning on domain-specific datasets with minimal data.",
      "summary_zh": "RoBERTa-large是基于BERT架构优化的预训练语言模型，由Facebook AI开发。它在BERT基础上通过移除下一句预测任务、采用更大批次训练和更长的序列进行改进，提升了语言理解能力。该模型适用于文本分类、实体识别、语义相似度计算等多种自然语言处理任务，尤其擅长处理英文文本的掩码语言建模。支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。",
      "summary_es": "RoBERTa-large es un modelo de lenguaje basado en Transformers optimizado para tareas de comprensión de texto. Destaca por su entrenamiento robusto sin tareas de predicción de oraciones, mejorando el rendimiento en clasificación, inferencia y relleno de máscaras. Es ampliamente utilizado en procesamiento de lenguaje natural (PLN) para análisis de sentimientos, respuesta a preguntas y resumen. Su arquitectura grande permite un alto rendimiento en aplicaciones que requieren precisión y contexto lingüístico profundo."
    },
    {
      "id": "amazon/chronos-t5-small",
      "source": "hf",
      "name": "chronos-t5-small",
      "url": "https://huggingface.co/amazon/chronos-t5-small",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "t5",
        "text2text-generation",
        "time series",
        "forecasting",
        "pretrained models",
        "foundation models",
        "time series foundation models",
        "time-series",
        "time-series-forecasting",
        "arxiv:2403.07815",
        "arxiv:1910.10683",
        "license:apache-2.0",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 11891514,
        "hf_likes": 131
      },
      "score": 23848.528000000002,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Chronos-T5-small是亚马逊基于T5架构开发的时间序列预测基础模型。该模型通过将时间序列数据转换为文本标记序列，利用预训练的文本到文本转换能力进行多步预测。其核心优势在于无需领域特定特征工程，可直接处理不同频率和规模的时间序列数据。模型适用于零售销量预测、能源负荷预测、经济指标分析等通用场景，为时间序列分析提供了零样本和少样本的预测解决方案。",
      "updated_at": "2025-09-18T19:21:47.074Z",
      "summary_en": "Chronos-T5-small is a compact pretrained foundation model for time series forecasting, based on the T5 architecture. It excels at converting time series data into text-like sequences for accurate predictions across various domains like finance, energy, and IoT. Its strengths include strong zero-shot performance, scalability, and ease of fine-tuning with minimal data. It is widely applicable for both univariate and multivariate forecasting tasks, offering a flexible and efficient solution for real-world time series analysis.",
      "summary_zh": "Chronos-T5-small是亚马逊基于T5架构开发的时间序列预测基础模型。该模型通过将时间序列数据转换为文本标记序列，利用预训练的文本到文本转换能力进行多步预测。其核心优势在于无需领域特定特征工程，可直接处理不同频率和规模的时间序列数据。模型适用于零售销量预测、能源负荷预测、经济指标分析等通用场景，为时间序列分析提供了零样本和少样本的预测解决方案。",
      "summary_es": "Chronos-T5-small es un modelo de series temporales basado en T5, preentrenado para predicción. Transforma datos numéricos en secuencias de tokens para generar pronósticos precisos. Es ideal para aplicaciones como demanda energética, finanzas o IoT. Su arquitectura eficiente permite integración sencilla en pipelines existentes."
    },
    {
      "id": "google/electra-base-discriminator",
      "source": "hf",
      "name": "electra-base-discriminator",
      "url": "https://huggingface.co/google/electra-base-discriminator",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "electra",
        "pretraining",
        "en",
        "arxiv:1406.2661",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 11457730,
        "hf_likes": 64
      },
      "score": 22947.46,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "ELECTRA-base-discriminator是由Google开发的预训练语言模型，采用判别式预训练方法替代传统的生成式预训练。该模型通过区分输入文本中的原始token与替换token进行训练，显著提升了训练效率和下游任务性能。适用于文本分类、命名实体识别、情感分析等多种自然语言处理任务。其架构轻量高效，在多项基准测试中表现优异，尤其适合需要高精度和快速推理的场景。支持多种主流深度学习框架，包括PyTorch、TensorFlow和JAX。",
      "updated_at": "2025-09-18T19:21:47.074Z",
      "summary_en": "ELECTRA-base-discriminator is a pre-trained transformer model designed for discriminative tasks, such as identifying whether input tokens are original or replaced. It excels in natural language understanding, offering efficient fine-tuning for downstream applications like text classification, named entity recognition, and question answering. Built on the ELECTRA architecture, it provides strong performance with reduced computational costs compared to generative pre-training. Suitable for researchers and developers working with English text in PyTorch, TensorFlow, or JAX environments.",
      "summary_zh": "ELECTRA-base-discriminator是由Google开发的预训练语言模型，采用判别式预训练方法替代传统的生成式预训练。该模型通过区分输入文本中的原始token与替换token进行训练，显著提升了训练效率和下游任务性能。适用于文本分类、命名实体识别、情感分析等多种自然语言处理任务。其架构轻量高效，在多项基准测试中表现优异，尤其适合需要高精度和快速推理的场景。支持多种主流深度学习框架，包括PyTorch、TensorFlow和JAX。",
      "summary_es": "ELECTRA-base-discriminator es un modelo de lenguaje preentrenado que utiliza un método de discriminación para el aprendizaje. Destaca por su eficiencia en tareas de comprensión del lenguaje y detección de tokens reemplazados. Es ideal para clasificación de texto, análisis sintáctico y fine-tuning en dominios específicos. Soporta múltiples frameworks como PyTorch, TensorFlow y JAX."
    },
    {
      "id": "prajjwal1/bert-tiny",
      "source": "hf",
      "name": "bert-tiny",
      "url": "https://huggingface.co/prajjwal1/bert-tiny",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "BERT",
        "MNLI",
        "NLI",
        "transformer",
        "pre-training",
        "en",
        "arxiv:1908.08962",
        "arxiv:2110.01518",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 10154678,
        "hf_likes": 127
      },
      "score": 20372.856,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "bert-tiny是一个轻量级BERT模型，专为资源受限环境设计。它基于BERT架构，但显著减少了参数量，在保持合理性能的同时大幅提升推理速度。该模型适用于文本分类、自然语言推理（NLI）等下游任务，尤其适合移动设备或边缘计算场景。支持PyTorch框架，预训练权重可直接用于微调，是高效NLP应用的实用选择。",
      "updated_at": "2025-09-18T19:21:47.074Z",
      "summary_en": "BERT-Tiny is a compact, efficient variant of the BERT model, designed for resource-constrained environments. It is pre-trained on English text and fine-tuned for natural language inference tasks like MNLI. Its small size makes it suitable for on-device applications, edge computing, and rapid prototyping. While less accurate than larger models, it offers a strong balance of performance and efficiency for lightweight NLP needs.",
      "summary_zh": "bert-tiny是一个轻量级BERT模型，专为资源受限环境设计。它基于BERT架构，但显著减少了参数量，在保持合理性能的同时大幅提升推理速度。该模型适用于文本分类、自然语言推理（NLI）等下游任务，尤其适合移动设备或边缘计算场景。支持PyTorch框架，预训练权重可直接用于微调，是高效NLP应用的实用选择。",
      "summary_es": "BERT-tiny es un modelo de lenguaje ligero basado en la arquitectura BERT, optimizado para tareas de inferencia en lenguaje natural (NLI) como MNLI. Su principal fortaleza es la eficiencia computacional, manteniendo un rendimiento sólido en clasificación de texto y comprensión contextual. Ideal para entornos con recursos limitados o aplicaciones que requieren baja latencia."
    },
    {
      "id": "trpakov/vit-face-expression",
      "source": "hf",
      "name": "vit-face-expression",
      "url": "https://huggingface.co/trpakov/vit-face-expression",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "vit",
        "image-classification",
        "doi:10.57967/hf/2289",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 8073901,
        "hf_likes": 78
      },
      "score": 16186.802,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "vit-face-expression是基于Vision Transformer（ViT）架构的面部表情分类模型，由trpakov开发并托管于Hugging Face平台。该模型使用Apache 2.0开源协议，支持ONNX和PyTorch格式，适用于图像分类任务，能够识别多种人类面部表情。其核心优势在于结合了Transformer的全局建模能力与高效的特征提取，适用于情感分析、人机交互和心理学研究等场景。模型兼容AutoTrain和端部署，适合需要高精度表情识别的技术团队集成使用。",
      "updated_at": "2025-09-18T19:21:47.074Z",
      "summary_en": "A Vision Transformer (ViT) model fine-tuned for facial expression recognition. It classifies images into common emotion categories, useful for applications in human-computer interaction, psychology research, and sentiment analysis. Built on PyTorch and ONNX, it supports efficient inference and deployment. Compatible with Hugging Face Transformers and AutoTrain, it is suitable for both research and production use.",
      "summary_zh": "vit-face-expression是基于Vision Transformer（ViT）架构的面部表情分类模型，由trpakov开发并托管于Hugging Face平台。该模型使用Apache 2.0开源协议，支持ONNX和PyTorch格式，适用于图像分类任务，能够识别多种人类面部表情。其核心优势在于结合了Transformer的全局建模能力与高效的特征提取，适用于情感分析、人机交互和心理学研究等场景。模型兼容AutoTrain和端部署，适合需要高精度表情识别的技术团队集成使用。",
      "summary_es": "Modelo de clasificación de expresiones faciales basado en Vision Transformer (ViT). Detecta emociones como alegría, tristeza o enfado en imágenes. Destaca por su precisión y velocidad usando arquitecturas transformer. Útil para análisis de comportamiento humano e interacción persona-computadora."
    },
    {
      "id": "BAAI/bge-base-en-v1.5",
      "source": "hf",
      "name": "bge-base-en-v1.5",
      "url": "https://huggingface.co/BAAI/bge-base-en-v1.5",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "mteb",
        "en",
        "arxiv:2401.03462",
        "arxiv:2312.15503",
        "arxiv:2311.13534",
        "arxiv:2310.07554",
        "arxiv:2309.07597",
        "license:mit",
        "model-index",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 7434960,
        "hf_likes": 347
      },
      "score": 15043.42,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "bge-base-en-v1.5 是一个基于 BERT 架构的英文文本嵌入模型，由北京智源人工智能研究院（BAAI）开发。该模型主要用于生成高质量的句子或段落级向量表示，适用于语义相似度计算、信息检索和文本聚类等任务。其亮点在于通过大规模对比学习优化，在多个基准测试（如 MTEB）中表现出色，能够有效捕捉语义细节。适用于需要高效且精准的英文文本表示场景，如搜索引擎、推荐系统和文档分析工具。",
      "updated_at": "2025-09-18T19:21:47.074Z",
      "summary_en": "BGE-base-en-v1.5 is a BERT-based English sentence embedding model optimized for semantic similarity and retrieval tasks. It excels in generating dense vector representations for text, making it suitable for search, clustering, and recommendation systems. With strong performance on benchmarks like MTEB, it is widely used in both research and production environments. The model supports multiple deployment formats, including PyTorch, ONNX, and SafeTensors.",
      "summary_zh": "bge-base-en-v1.5 是一个基于 BERT 架构的英文文本嵌入模型，由北京智源人工智能研究院（BAAI）开发。该模型主要用于生成高质量的句子或段落级向量表示，适用于语义相似度计算、信息检索和文本聚类等任务。其亮点在于通过大规模对比学习优化，在多个基准测试（如 MTEB）中表现出色，能够有效捕捉语义细节。适用于需要高效且精准的英文文本表示场景，如搜索引擎、推荐系统和文档分析工具。",
      "summary_es": "Modelo de incrustación de texto BGE base en inglés. Genera representaciones vectoriales densas para búsqueda semántica y similitud textual. Destaca por su eficiencia en recuperación de información y clustering. Ideal para aplicaciones de búsqueda contextual y análisis de similitud en documentos."
    }
  ]
}