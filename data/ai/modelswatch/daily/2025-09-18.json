{
  "date": "2025-09-18",
  "updated_at": "2025-09-17T18:42:44.950Z",
  "items": [
    {
      "id": "twbs/bootstrap",
      "source": "github",
      "name": "bootstrap",
      "url": "https://github.com/twbs/bootstrap",
      "license": "MIT",
      "lang": "MDX",
      "tags": [
        "bootstrap",
        "css",
        "css-framework",
        "html",
        "javascript",
        "sass",
        "scss"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 173434,
        "forks": 79156,
        "issues": 575
      },
      "score": 189365.05255952934,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Bootstrap 是一个开源的前端框架，用于快速构建响应式、移动优先的网站和 Web 应用。它提供了一套丰富的 CSS 和 JavaScript 组件，包括网格系统、按钮、表单和导航等，帮助开发者高效实现一致的用户界面。其核心亮点在于强大的响应式设计能力，能够自动适配不同设备屏幕尺寸。适用于各类 Web 项目开发，尤其适合需要快速原型设计或团队协作的场景。",
      "updated_at": "2025-09-17T16:49:33Z",
      "summary_en": "Bootstrap is a leading open-source front-end framework for building responsive, mobile-first websites and web applications. It provides a comprehensive set of CSS and JavaScript components, such as grids, forms, and navigation bars, enabling rapid and consistent UI development. Its extensive documentation and large community support make it accessible for developers of all skill levels. Widely used for prototyping and production projects, it streamlines cross-device compatibility and design consistency.",
      "summary_zh": "Bootstrap 是一个开源的前端框架，用于快速构建响应式、移动优先的网站和 Web 应用。它提供了一套丰富的 CSS 和 JavaScript 组件，包括网格系统、按钮、表单和导航等，帮助开发者高效实现一致的用户界面。其核心亮点在于强大的响应式设计能力，能够自动适配不同设备屏幕尺寸。适用于各类 Web 项目开发，尤其适合需要快速原型设计或团队协作的场景。",
      "summary_es": "Bootstrap es un framework front-end para crear sitios web responsivos y mobile-first. Incluye componentes predefinidos y un sistema de grid flexible. Es ideal para prototipado rápido y desarrollo consistente. Ampliamente usado en aplicaciones empresariales y proyectos que priorizan la compatibilidad multiplataforma."
    },
    {
      "id": "avelino/awesome-go",
      "source": "github",
      "name": "awesome-go",
      "url": "https://github.com/avelino/awesome-go",
      "license": "MIT",
      "lang": "Go",
      "tags": [
        "awesome",
        "awesome-list",
        "go",
        "golang",
        "golang-library",
        "hacktoberfest"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 152540,
        "forks": 12568,
        "issues": 157
      },
      "score": 155153.57667218364,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "这是一个精选的Go语言资源集合，收录了大量高质量的框架、库和软件项目。内容涵盖网络编程、数据库、工具链等多个领域，适合Go开发者快速查找和使用成熟的开源组件。该项目由社区共同维护，遵循MIT开源协议，具有极高的活跃度和可靠性。无论是初学者还是资深工程师，都可以从中找到提升开发效率的实用工具。",
      "updated_at": "2025-09-17T17:43:10Z",
      "summary_en": "A curated collection of high-quality Go frameworks, libraries, and software, maintained by the community. Ideal for developers seeking reliable tools for building scalable applications, microservices, and system utilities. Covers a wide range of domains, from web development to networking and DevOps. An essential resource for Go programmers to discover and evaluate production-ready solutions.",
      "summary_zh": "这是一个精选的Go语言资源集合，收录了大量高质量的框架、库和软件项目。内容涵盖网络编程、数据库、工具链等多个领域，适合Go开发者快速查找和使用成熟的开源组件。该项目由社区共同维护，遵循MIT开源协议，具有极高的活跃度和可靠性。无论是初学者还是资深工程师，都可以从中找到提升开发效率的实用工具。",
      "summary_es": "Lista curada de marcos, bibliotecas y software destacados en Go. Facilita el descubrimiento de herramientas de calidad para desarrollo backend, sistemas distribuidos y CLI. Ideal para desarrolladores que buscan referencias confiables y soluciones probadas en el ecosistema Go."
    },
    {
      "id": "trimstray/the-book-of-secret-knowledge",
      "source": "github",
      "name": "the-book-of-secret-knowledge",
      "url": "https://github.com/trimstray/the-book-of-secret-knowledge",
      "license": "MIT",
      "lang": "N/A",
      "tags": [
        "awesome",
        "awesome-list",
        "bsd",
        "cheatsheets",
        "devops",
        "guidelines",
        "hacking",
        "hacks",
        "howtos",
        "linux",
        "lists",
        "manuals",
        "one-liners",
        "pentesters",
        "resources",
        "search-engines",
        "security",
        "security-researchers",
        "sysops"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 186475,
        "forks": 11535,
        "issues": 100
      },
      "score": 188881.96745150464,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "《秘密知识之书》是一个开源的知识集合项目，收录了大量实用资源，包括清单、手册、速查表、博客、技巧、命令行和网页工具等。内容涵盖系统管理、DevOps、安全、Linux等多个技术领域，适合开发者和运维人员快速查找和参考。项目结构清晰，资源分类详细，便于高效检索和使用。无论是日常开发、问题排查还是技能提升，都能从中找到有价值的参考资料。",
      "updated_at": "2025-09-17T17:39:11Z",
      "summary_en": "The Book of Secret Knowledge is a comprehensive, curated collection of resources for developers, sysadmins, and security professionals. It includes cheatsheets, guides, tools, and practical tips covering Linux, DevOps, and cybersecurity. Its strength lies in its extensive, well-organized content, making it a valuable reference for both learning and daily use. Ideal for quick lookups, skill enhancement, and troubleshooting in technical environments.",
      "summary_zh": "《秘密知识之书》是一个开源的知识集合项目，收录了大量实用资源，包括清单、手册、速查表、博客、技巧、命令行和网页工具等。内容涵盖系统管理、DevOps、安全、Linux等多个技术领域，适合开发者和运维人员快速查找和参考。项目结构清晰，资源分类详细，便于高效检索和使用。无论是日常开发、问题排查还是技能提升，都能从中找到有价值的参考资料。",
      "summary_es": "Este proyecto es una extensa colección de recursos técnicos, incluyendo listas, manuales, hojas de referencia y herramientas CLI/web. Destaca por su enfoque en DevOps, hacking y administración de sistemas Linux. Es ideal para profesionales que buscan referencias rápidas, mejores prácticas y soluciones técnicas. Su estructura organizada y contenido práctico lo convierten en una valiosa fuente de conocimiento para desarrolladores y administradores."
    },
    {
      "id": "ohmyzsh/ohmyzsh",
      "source": "github",
      "name": "ohmyzsh",
      "url": "https://github.com/ohmyzsh/ohmyzsh",
      "license": "MIT",
      "lang": "Shell",
      "tags": [
        "cli",
        "cli-app",
        "hacktoberfest",
        "oh-my-zsh",
        "oh-my-zsh-plugin",
        "oh-my-zsh-theme",
        "ohmyzsh",
        "plugin-framework",
        "plugins",
        "productivity",
        "shell",
        "terminal",
        "theme",
        "themes",
        "zsh",
        "zsh-configuration"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 181464,
        "forks": 26215,
        "issues": 502
      },
      "score": 186806.79700397377,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Ohmyzsh 是一个社区驱动的 Zsh 配置管理框架，拥有超过 2400 名贡献者。它提供了 300 多个可选插件，支持多种开发环境和工具，如 Git、Docker、Python 等，并包含 140 多种主题，可高度自定义终端外观。内置的自动更新工具方便用户及时获取社区最新功能。适用于希望提升终端使用效率和美观度的开发者和高级用户。",
      "updated_at": "2025-09-17T16:25:33Z",
      "summary_en": "Oh My Zsh is a community-driven framework for managing ZSH shell configurations. It offers over 300 plugins for tools like Git, Docker, and Python, enhancing productivity and workflow automation. With 140+ themes, it allows extensive customization of the terminal interface. Its auto-update feature ensures users stay current with community contributions, making it ideal for developers and power users.",
      "summary_zh": "Ohmyzsh 是一个社区驱动的 Zsh 配置管理框架，拥有超过 2400 名贡献者。它提供了 300 多个可选插件，支持多种开发环境和工具，如 Git、Docker、Python 等，并包含 140 多种主题，可高度自定义终端外观。内置的自动更新工具方便用户及时获取社区最新功能。适用于希望提升终端使用效率和美观度的开发者和高级用户。",
      "summary_es": "Ohmyzsh es un marco de configuración para la terminal zsh, impulsado por la comunidad. Ofrece más de 300 plugins y 140 temas para mejorar la productividad en línea de comandos. Su principal fortaleza es la personalización y automatización de tareas comunes. Es ideal para desarrolladores y administradores de sistemas."
    },
    {
      "id": "github/gitignore",
      "source": "github",
      "name": "gitignore",
      "url": "https://github.com/github/gitignore",
      "license": "CC0-1.0",
      "lang": "N/A",
      "tags": [
        "git",
        "gitignore"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 169421,
        "forks": 83002,
        "issues": 319
      },
      "score": 186121.2881690972,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "这是一个由 GitHub 官方维护的 `.gitignore` 模板集合，适用于各种编程语言、框架和开发环境。它可以帮助开发者快速生成适合项目的 `.gitignore` 文件，避免将不必要的文件（如编译产物、日志、依赖目录等）提交到 Git 仓库中。该项目采用 CC0-1.0 协议，允许自由使用和分发。无论是个人项目还是团队协作，都可以通过这个模板库提升代码管理的效率和整洁性。",
      "updated_at": "2025-09-17T17:04:56Z",
      "summary_en": "Project: gitignore  \nA comprehensive collection of .gitignore templates for various programming languages, frameworks, and tools. It helps developers exclude unnecessary files from version control, reducing repository clutter and improving performance. Widely applicable for any Git-based project, it is maintained by GitHub and community contributions. Simplifies setup and ensures best practices for ignoring files across different environments.",
      "summary_zh": "这是一个由 GitHub 官方维护的 `.gitignore` 模板集合，适用于各种编程语言、框架和开发环境。它可以帮助开发者快速生成适合项目的 `.gitignore` 文件，避免将不必要的文件（如编译产物、日志、依赖目录等）提交到 Git 仓库中。该项目采用 CC0-1.0 协议，允许自由使用和分发。无论是个人项目还是团队协作，都可以通过这个模板库提升代码管理的效率和整洁性。",
      "summary_es": "Colección de plantillas .gitignore para proyectos de software. Simplifica la exclusión de archivos innecesarios en repositorios Git. Incluye configuraciones para lenguajes, entornos y herramientas específicas. Ideal para mantener repositorios limpios y optimizados."
    },
    {
      "id": "flutter/flutter",
      "source": "github",
      "name": "flutter",
      "url": "https://github.com/flutter/flutter",
      "license": "BSD-3-Clause",
      "lang": "Dart",
      "tags": [
        "android",
        "app-framework",
        "cross-platform",
        "dart",
        "dart-platform",
        "desktop",
        "flutter",
        "flutter-package",
        "fuchsia",
        "ios",
        "linux-desktop",
        "macos",
        "material-design",
        "mobile",
        "mobile-development",
        "skia",
        "web",
        "web-framework",
        "windows"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 172478,
        "forks": 29213,
        "issues": 12251
      },
      "score": 178420.59939594907,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Flutter 是由 Google 开发的开源 UI 框架，使用 Dart 语言构建高性能、跨平台的应用程序。它支持移动端（iOS 和 Android）、桌面端（Windows、macOS、Linux）以及 Web 端开发，通过单一代码库实现多平台部署。Flutter 的核心亮点包括其丰富的组件库、高度自定义的 UI 设计能力，以及热重载功能，显著提升开发效率。适用于需要快速迭代、追求一致用户体验的跨平台应用开发场景。",
      "updated_at": "2025-09-17T17:52:59Z",
      "summary_en": "Flutter is an open-source UI framework for building natively compiled applications for mobile, web, and desktop from a single codebase. It uses the Dart programming language and provides a rich set of pre-designed widgets, enabling fast development with a consistent look and feel across platforms. Its hot reload feature allows for real-time updates during development, improving productivity. Flutter is ideal for developers aiming to create high-performance, visually appealing apps efficiently.",
      "summary_zh": "Flutter 是由 Google 开发的开源 UI 框架，使用 Dart 语言构建高性能、跨平台的应用程序。它支持移动端（iOS 和 Android）、桌面端（Windows、macOS、Linux）以及 Web 端开发，通过单一代码库实现多平台部署。Flutter 的核心亮点包括其丰富的组件库、高度自定义的 UI 设计能力，以及热重载功能，显著提升开发效率。适用于需要快速迭代、追求一致用户体验的跨平台应用开发场景。",
      "summary_es": "Flutter es un framework de código abierto para crear aplicaciones nativas compiladas para móvil, web y escritorio desde una única base de código. Utiliza el lenguaje Dart y ofrece alto rendimiento con renderizado propio. Ideal para desarrollo cross-platform con interfaces fluidas y personalizables. Ampliamente adoptado en aplicaciones de producción."
    },
    {
      "id": "AUTOMATIC1111/stable-diffusion-webui",
      "source": "github",
      "name": "stable-diffusion-webui",
      "url": "https://github.com/AUTOMATIC1111/stable-diffusion-webui",
      "license": "AGPL-3.0",
      "lang": "Python",
      "tags": [
        "ai",
        "ai-art",
        "deep-learning",
        "diffusion",
        "gradio",
        "image-generation",
        "image2image",
        "img2img",
        "pytorch",
        "stable-diffusion",
        "text2image",
        "torch",
        "txt2img",
        "unstable",
        "upscaling",
        "web"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 156485,
        "forks": 29043,
        "issues": 2426
      },
      "score": 162393.59025243056,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Stable Diffusion web UI 是一个基于 Gradio 构建的 Web 界面，用于运行 Stable Diffusion 图像生成模型。它支持文本到图像、图像到图像等多种生成模式，并提供了丰富的自定义选项，如模型选择、参数调整和插件扩展。该项目适用于 AI 艺术创作、设计辅助和实验研究等场景，用户无需编写代码即可使用强大的图像生成功能。其开源特性允许开发者自由修改和扩展功能，推动了 AI 生成内容的普及和创新。",
      "updated_at": "2025-09-17T17:49:02Z",
      "summary_en": "Stable Diffusion web UI is a popular open-source interface for running and customizing Stable Diffusion models. It enables users to generate, edit, and manipulate images via text prompts and image inputs. The tool supports various workflows, including text-to-image, image-to-image, and inpainting, making it versatile for creative and experimental use. Built with Gradio and PyTorch, it is widely adopted by artists, researchers, and hobbyists for accessible AI-driven image generation.",
      "summary_zh": "Stable Diffusion web UI 是一个基于 Gradio 构建的 Web 界面，用于运行 Stable Diffusion 图像生成模型。它支持文本到图像、图像到图像等多种生成模式，并提供了丰富的自定义选项，如模型选择、参数调整和插件扩展。该项目适用于 AI 艺术创作、设计辅助和实验研究等场景，用户无需编写代码即可使用强大的图像生成功能。其开源特性允许开发者自由修改和扩展功能，推动了 AI 生成内容的普及和创新。",
      "summary_es": "Interfaz web para Stable Diffusion que permite generar y transformar imágenes mediante IA. Ofrece amplias opciones de personalización, soporta múltiples modelos y extensiones. Ideal para artistas digitales, investigadores y entusiastas de la inteligencia artificial que buscan una herramienta versátil y accesible para creación visual."
    },
    {
      "id": "Snailclimb/JavaGuide",
      "source": "github",
      "name": "JavaGuide",
      "url": "https://github.com/Snailclimb/JavaGuide",
      "license": "Apache-2.0",
      "lang": "Java",
      "tags": [
        "algorithms",
        "interview",
        "java",
        "jvm",
        "mysql",
        "redis",
        "spring",
        "system",
        "system-design",
        "zookeeper"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 151773,
        "forks": 45985,
        "issues": 73
      },
      "score": 161069.93299934414,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "《JavaGuide》是一份面向 Java 开发者的综合性学习与面试指南，内容涵盖 Java 核心知识、JVM、常用框架（如 Spring）、数据库（MySQL、Redis）、分布式系统设计及常用中间件（如 ZooKeeper）。该项目结构清晰、内容实用，不仅适合系统学习 Java 技术栈，也为求职面试提供了高质量的资料支持。无论是初学者入门，还是开发者进阶和面试准备，都能从中获得帮助。",
      "updated_at": "2025-09-17T17:24:18Z",
      "summary_en": "JavaGuide is a comprehensive open-source resource for Java developers, covering core knowledge areas like algorithms, JVM, databases, and frameworks. It serves as a study and interview preparation guide, offering practical insights and examples. Its strengths include broad topic coverage, clear explanations, and relevance to real-world development and hiring processes. Ideal for learners and professionals aiming to deepen their Java expertise or prepare for technical interviews.",
      "summary_zh": "《JavaGuide》是一份面向 Java 开发者的综合性学习与面试指南，内容涵盖 Java 核心知识、JVM、常用框架（如 Spring）、数据库（MySQL、Redis）、分布式系统设计及常用中间件（如 ZooKeeper）。该项目结构清晰、内容实用，不仅适合系统学习 Java 技术栈，也为求职面试提供了高质量的资料支持。无论是初学者入门，还是开发者进阶和面试准备，都能从中获得帮助。",
      "summary_es": "JavaGuide es una guía integral para desarrolladores Java, enfocada en aprendizaje y preparación para entrevistas. Cubre conocimientos esenciales como algoritmos, JVM, bases de datos y frameworks como Spring. Su enfoque práctico y contenido actualizado lo hacen ideal para estudiantes y profesionales que buscan reforzar habilidades técnicas o prepararse para procesos de selección."
    },
    {
      "id": "huggingface/transformers",
      "source": "github",
      "name": "transformers",
      "url": "https://github.com/huggingface/transformers",
      "license": "Apache-2.0",
      "lang": "Python",
      "tags": [
        "audio",
        "deep-learning",
        "deepseek",
        "gemma",
        "glm",
        "hacktoberfest",
        "llm",
        "machine-learning",
        "model-hub",
        "natural-language-processing",
        "nlp",
        "pretrained-models",
        "python",
        "pytorch",
        "pytorch-transformers",
        "qwen",
        "speech-recognition",
        "transformer",
        "vlm"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 149912,
        "forks": 30436,
        "issues": 1990
      },
      "score": 156099.1655996528,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "🤗 Transformers 是一个开源机器学习框架，专注于提供最先进的文本、视觉、音频和多模态模型的实现。它支持推理和训练，适用于自然语言处理、计算机视觉和音频处理等多种任务。该框架集成了大量预训练模型，如 BERT、GPT 和 T5，并支持快速部署和微调。适用于研究人员、工程师和开发者，用于构建和实验先进的 AI 应用。",
      "updated_at": "2025-09-17T17:38:23Z",
      "summary_en": "🤗 Transformers is a versatile open-source library for state-of-the-art machine learning models across text, vision, audio, and multimodal tasks. It supports both training and inference, offering a unified framework for models like BERT, GPT, and Whisper. Its strengths include extensive pre-trained models, ease of fine-tuning, and broad applicability in NLP, computer vision, and speech processing. Ideal for researchers and developers building scalable AI applications.",
      "summary_zh": "🤗 Transformers 是一个开源机器学习框架，专注于提供最先进的文本、视觉、音频和多模态模型的实现。它支持推理和训练，适用于自然语言处理、计算机视觉和音频处理等多种任务。该框架集成了大量预训练模型，如 BERT、GPT 和 T5，并支持快速部署和微调。适用于研究人员、工程师和开发者，用于构建和实验先进的 AI 应用。",
      "summary_es": "Transformers es una biblioteca de código abierto para modelos de aprendizaje profundo en texto, visión, audio y multimodal. Ofrece implementaciones de última generación para inferencia y entrenamiento, con soporte para múltiples arquitecturas. Sus puntos fuertes incluyen integración con el Hub de modelos y facilidad de uso. Es ampliamente utilizado en procesamiento de lenguaje natural y tareas multimodales."
    },
    {
      "id": "omni-research/Tarsier2-Recap-7b",
      "source": "hf",
      "name": "Tarsier2-Recap-7b",
      "url": "https://huggingface.co/omni-research/Tarsier2-Recap-7b",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "safetensors",
        "video LLM",
        "arxiv:2501.07888",
        "license:apache-2.0",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 8263301,
        "hf_likes": 17
      },
      "score": 16535.102,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Tarsier2-Recap-7b 是一个基于 Transformer 架构的视频语言模型，专注于视频内容理解与摘要生成。该模型能够处理视频输入并生成结构化的文本摘要，适用于视频内容分析、自动字幕生成和多媒体信息检索等场景。其亮点在于结合了视觉与语言模态，支持对视频时序信息的建模，提升了长视频内容的理解能力。该模型适用于视频平台、智能监控和多媒体数据分析等领域，为自动化视频处理提供了高效工具。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_en": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for summarizing and analyzing video content. It excels at generating concise recaps and extracting key information from videos, making it suitable for applications in media analysis, content indexing, and accessibility. Its Apache 2.0 license and strong performance metrics indicate broad usability for research and practical deployment. The model is particularly effective for tasks requiring efficient video understanding without extensive computational resources.",
      "summary_zh": "Tarsier2-Recap-7b 是一个基于 Transformer 架构的视频语言模型，专注于视频内容理解与摘要生成。该模型能够处理视频输入并生成结构化的文本摘要，适用于视频内容分析、自动字幕生成和多媒体信息检索等场景。其亮点在于结合了视觉与语言模态，支持对视频时序信息的建模，提升了长视频内容的理解能力。该模型适用于视频平台、智能监控和多媒体数据分析等领域，为自动化视频处理提供了高效工具。",
      "summary_es": "Tarsier2-Recap-7b es un modelo de lenguaje multimodal especializado en el análisis y resumen de contenido de video. Basado en la arquitectura Llama-3.2-7B, destaca por su capacidad para procesar marcos de video y generar descripciones textuales precisas. Sus principales aplicaciones incluyen la creación automática de resúmenes de vídeos, extracción de información clave y generación de metadatos descriptivos. El modelo utiliza el formato de tensores seguros (safetensors) y está disponible bajo licencia Apache 2.0."
    },
    {
      "id": "meta-llama/Llama-3.1-8B-Instruct",
      "source": "hf",
      "name": "Llama-3.1-8B-Instruct",
      "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "facebook",
        "meta",
        "pytorch",
        "llama-3",
        "conversational",
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th",
        "arxiv:2204.05149",
        "base_model:meta-llama/Llama-3.1-8B",
        "base_model:finetune:meta-llama/Llama-3.1-8B",
        "license:llama3.1",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 7230352,
        "hf_likes": 4626
      },
      "score": 16773.703999999998,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Llama-3.1-8B-Instruct 是 Meta 推出的开源对话优化语言模型，基于 Llama-3 架构，参数量为 80 亿。该模型专为指令遵循和对话交互设计，适用于文本生成、问答、代码辅助等多种任务。其亮点在于高效的推理性能与较强的上下文理解能力，尤其适合资源受限环境下的部署。模型支持 Transformers 和 PyTorch 框架，主要面向英语场景，可用于聊天机器人、内容创作或自动化任务处理等应用。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_en": "Llama-3.1-8B-Instruct is an 8-billion-parameter language model optimized for instruction-following tasks. It excels in conversational applications, text generation, and structured reasoning. Built on the robust Llama-3 architecture, it offers strong performance in English while maintaining efficiency for deployment. Ideal for chatbots, content creation, and research prototyping.",
      "summary_zh": "Llama-3.1-8B-Instruct 是 Meta 推出的开源对话优化语言模型，基于 Llama-3 架构，参数量为 80 亿。该模型专为指令遵循和对话交互设计，适用于文本生成、问答、代码辅助等多种任务。其亮点在于高效的推理性能与较强的上下文理解能力，尤其适合资源受限环境下的部署。模型支持 Transformers 和 PyTorch 框架，主要面向英语场景，可用于聊天机器人、内容创作或自动化任务处理等应用。",
      "summary_es": "Llama-3.1-8B-Instruct es un modelo de lenguaje de 8 mil millones de parámetros optimizado para tareas de instrucción y diálogo. Basado en la arquitectura Transformer, destaca por su eficiencia computacional y capacidad para generar respuestas coherentes en inglés. Es ideal para chatbots, asistentes virtuales y aplicaciones de procesamiento de lenguaje natural. Su licencia permite uso comercial e investigación."
    },
    {
      "id": "amazon/chronos-t5-small",
      "source": "hf",
      "name": "chronos-t5-small",
      "url": "https://huggingface.co/amazon/chronos-t5-small",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "t5",
        "text2text-generation",
        "time series",
        "forecasting",
        "pretrained models",
        "foundation models",
        "time series foundation models",
        "time-series",
        "time-series-forecasting",
        "arxiv:2403.07815",
        "arxiv:1910.10683",
        "license:apache-2.0",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 13199198,
        "hf_likes": 131
      },
      "score": 26463.896,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Chronos-T5-small是亚马逊基于T5架构开发的时间序列预测基础模型。该模型采用文本到文本的生成范式，将数值时间序列转换为标记序列进行训练和推理。其核心优势在于通过大规模预训练捕捉通用时间模式，无需领域特异性训练即可完成多种预测任务。模型适用于商业指标预测、传感器数据分析等场景，为时间序列分析提供了零样本和少样本的解决方案。开源版本为small规模，平衡了性能与计算效率。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "Chronos-T5-small is a compact, pretrained time series forecasting model based on T5 architecture. It excels at converting time series data into text-like sequences for accurate predictions across various domains like finance, energy, and IoT. Its strengths include efficient handling of univariate series, zero-shot generalization, and integration with Hugging Face Transformers. Ideal for researchers and practitioners needing lightweight, scalable forecasting without extensive retraining.",
      "summary_zh": "Chronos-T5-small是亚马逊基于T5架构开发的时间序列预测基础模型。该模型采用文本到文本的生成范式，将数值时间序列转换为标记序列进行训练和推理。其核心优势在于通过大规模预训练捕捉通用时间模式，无需领域特异性训练即可完成多种预测任务。模型适用于商业指标预测、传感器数据分析等场景，为时间序列分析提供了零样本和少样本的解决方案。开源版本为small规模，平衡了性能与计算效率。",
      "summary_es": "Chronos-T5-small es un modelo de series temporales basado en T5, preentrenado para predecir secuencias numéricas. Destaca por su capacidad de generalización en múltiples dominios sin ajuste específico. Usa tokenización numérica y genera pronósticos en formato de texto. Ideal para aplicaciones de forecasting en finanzas, energía o demanda."
    },
    {
      "id": "distilbert/distilbert-base-uncased",
      "source": "hf",
      "name": "distilbert-base-uncased",
      "url": "https://huggingface.co/distilbert/distilbert-base-uncased",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "safetensors",
        "distilbert",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1910.01108",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 12227309,
        "hf_likes": 755
      },
      "score": 24832.118000000002,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "DistilBERT-base-uncased 是一个轻量化的 BERT 模型，通过知识蒸馏技术将 BERT 参数量压缩 40%，同时保留 97% 的语言理解能力。它适用于掩码语言建模任务，支持多种框架，包括 PyTorch、TensorFlow 和 JAX。该模型在英文文本处理中表现高效，特别适合资源受限或需要快速推理的场景，如搜索、分类和实体识别。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "DistilBERT is a distilled version of BERT, designed for efficient natural language understanding tasks. It retains 97% of BERT's performance while being 40% smaller and 60% faster, making it ideal for resource-constrained environments. Common use cases include text classification, named entity recognition, and masked language modeling. It supports multiple frameworks and is widely used in production for its balance of speed and accuracy.",
      "summary_zh": "DistilBERT-base-uncased 是一个轻量化的 BERT 模型，通过知识蒸馏技术将 BERT 参数量压缩 40%，同时保留 97% 的语言理解能力。它适用于掩码语言建模任务，支持多种框架，包括 PyTorch、TensorFlow 和 JAX。该模型在英文文本处理中表现高效，特别适合资源受限或需要快速推理的场景，如搜索、分类和实体识别。",
      "summary_es": "DistilBERT es un modelo de lenguaje basado en BERT, optimizado para eficiencia y velocidad. Conserva el 97% del rendimiento de BERT original con un 40% menos de parámetros. Ideal para tareas de procesamiento de lenguaje natural como clasificación de texto, análisis de sentimiento y relleno de máscaras. Ampliamente utilizado en entornos con recursos limitados."
    },
    {
      "id": "FacebookAI/roberta-large",
      "source": "hf",
      "name": "roberta-large",
      "url": "https://huggingface.co/FacebookAI/roberta-large",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "onnx",
        "safetensors",
        "roberta",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1907.11692",
        "arxiv:1806.02847",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 12085823,
        "hf_likes": 245
      },
      "score": 24294.146,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "RoBERTa-large是基于Transformer架构的大规模预训练语言模型，由Facebook AI开发。它在BERT的基础上通过优化训练策略（如移除下一句预测任务、使用更大批次和更长的序列）提升了性能。该模型适用于多种自然语言处理任务，包括文本分类、命名实体识别和掩码语言建模。主要面向研究人员和开发者，用于构建高效的NLP应用或作为下游任务的基准模型。支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "RoBERTa-large is a robust transformer-based language model optimized for masked language modeling. It excels in a wide range of natural language understanding tasks, including text classification, named entity recognition, and sentiment analysis. With strong performance across benchmarks, it is well-suited for research and production applications requiring high accuracy. Its compatibility with multiple frameworks (PyTorch, TensorFlow, JAX, ONNX) ensures flexibility in deployment.",
      "summary_zh": "RoBERTa-large是基于Transformer架构的大规模预训练语言模型，由Facebook AI开发。它在BERT的基础上通过优化训练策略（如移除下一句预测任务、使用更大批次和更长的序列）提升了性能。该模型适用于多种自然语言处理任务，包括文本分类、命名实体识别和掩码语言建模。主要面向研究人员和开发者，用于构建高效的NLP应用或作为下游任务的基准模型。支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。",
      "summary_es": "RoBERTa-large es un modelo de lenguaje basado en transformers optimizado para tareas de comprensión y generación de texto. Destaca por su entrenamiento robusto sin tareas de siguiente frase, mejorando rendimiento en clasificación, entidad nombrada y relleno de máscaras. Usos comunes incluyen análisis de sentimientos, preguntas-respuestas y procesamiento de lenguaje natural en inglés."
    },
    {
      "id": "google/electra-base-discriminator",
      "source": "hf",
      "name": "electra-base-discriminator",
      "url": "https://huggingface.co/google/electra-base-discriminator",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "electra",
        "pretraining",
        "en",
        "arxiv:1406.2661",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 11547887,
        "hf_likes": 64
      },
      "score": 23127.774,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "ELECTRA-base-discriminator是由Google开发的一种基于ELECTRA架构的预训练语言模型判别器。该模型通过替换token检测任务进行预训练，相比传统MLM方法具有更高的训练效率。模型支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。适用于文本分类、情感分析等下游NLP任务，特别适合需要高效文本表征的场景。模型在英文语料上训练，采用Apache 2.0开源协议。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "ELECTRA-base-discriminator is a pretrained transformer model designed for efficient discriminative tasks. It excels in natural language understanding, including text classification, named entity recognition, and sentiment analysis. Its key strength lies in its pretraining efficiency, outperforming many models of similar size. It is widely applicable in research and production environments for English-language NLP tasks.",
      "summary_zh": "ELECTRA-base-discriminator是由Google开发的一种基于ELECTRA架构的预训练语言模型判别器。该模型通过替换token检测任务进行预训练，相比传统MLM方法具有更高的训练效率。模型支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。适用于文本分类、情感分析等下游NLP任务，特别适合需要高效文本表征的场景。模型在英文语料上训练，采用Apache 2.0开源协议。",
      "summary_es": "ELECTRA-base-discriminator es un modelo de lenguaje preentrenado que utiliza un enfoque de discriminación para el aprendizaje de representaciones. Destaca por su eficiencia computacional y rendimiento en tareas de comprensión del lenguaje natural. Es ideal para clasificación de texto, análisis de sentimientos y extracción de información. Soporta múltiples frameworks como PyTorch, TensorFlow y JAX."
    },
    {
      "id": "prajjwal1/bert-tiny",
      "source": "hf",
      "name": "bert-tiny",
      "url": "https://huggingface.co/prajjwal1/bert-tiny",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "BERT",
        "MNLI",
        "NLI",
        "transformer",
        "pre-training",
        "en",
        "arxiv:1908.08962",
        "arxiv:2110.01518",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 10176180,
        "hf_likes": 127
      },
      "score": 20415.86,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "bert-tiny是一个轻量级BERT模型，专为资源受限环境设计。它基于BERT架构，但显著减少了参数量，在保持合理性能的同时大幅提升推理速度。该模型适用于文本分类、自然语言推理等任务，特别适合移动设备或边缘计算场景。支持PyTorch框架，预训练权重可直接用于下游任务的微调。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "BERT-Tiny is a compact, efficient variant of the BERT model, designed for resource-constrained environments. It is pre-trained on English text and fine-tuned for natural language inference tasks like MNLI. Ideal for applications requiring fast inference and low memory usage, such as mobile or edge devices. Its small size makes it suitable for prototyping and lightweight NLP pipelines.",
      "summary_zh": "bert-tiny是一个轻量级BERT模型，专为资源受限环境设计。它基于BERT架构，但显著减少了参数量，在保持合理性能的同时大幅提升推理速度。该模型适用于文本分类、自然语言推理等任务，特别适合移动设备或边缘计算场景。支持PyTorch框架，预训练权重可直接用于下游任务的微调。",
      "summary_es": "BERT-Tiny es una versión ultrapequeña del modelo BERT, optimizada para eficiencia computacional. Ideal para dispositivos con recursos limitados, mantiene capacidades sólidas en tareas de comprensión del lenguaje natural como inferencia (NLI/MNLI). Su diseño compacto permite despliegues rápidos en entornos restringidos, manteniendo un buen rendimiento en clasificación de texto y otras aplicaciones de NLP ligero."
    },
    {
      "id": "trpakov/vit-face-expression",
      "source": "hf",
      "name": "vit-face-expression",
      "url": "https://huggingface.co/trpakov/vit-face-expression",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "vit",
        "image-classification",
        "doi:10.57967/hf/2289",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 8025363,
        "hf_likes": 78
      },
      "score": 16089.726,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "vit-face-expression是基于Vision Transformer架构的面部表情分类模型，能够识别图像中的七种基本表情（如高兴、悲伤、愤怒等）。该模型使用PyTorch和ONNX格式提供，支持快速推理和部署，适用于实时表情分析场景。其亮点在于结合了ViT的全局建模能力与轻量化设计，在保证精度的同时具备较高的计算效率。适用于人机交互、情感计算或心理学研究等需要自动化表情识别的领域。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_en": "A Vision Transformer (ViT) model fine-tuned for facial expression classification. It supports PyTorch, ONNX, and SafeTensors formats, enabling efficient deployment across platforms. Ideal for emotion recognition in images, with applications in human-computer interaction and behavioral analysis. High download count reflects its reliability and broad usability.",
      "summary_zh": "vit-face-expression是基于Vision Transformer架构的面部表情分类模型，能够识别图像中的七种基本表情（如高兴、悲伤、愤怒等）。该模型使用PyTorch和ONNX格式提供，支持快速推理和部署，适用于实时表情分析场景。其亮点在于结合了ViT的全局建模能力与轻量化设计，在保证精度的同时具备较高的计算效率。适用于人机交互、情感计算或心理学研究等需要自动化表情识别的领域。",
      "summary_es": "Modelo de clasificación de expresiones faciales basado en Vision Transformer (ViT). Detecta emociones como alegría, tristeza o enfado en imágenes. Destaca por su precisión y eficiencia usando arquitecturas transformer. Útil para análisis de comportamiento humano o interacciones persona-computadora."
    },
    {
      "id": "BAAI/bge-base-en-v1.5",
      "source": "hf",
      "name": "bge-base-en-v1.5",
      "url": "https://huggingface.co/BAAI/bge-base-en-v1.5",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "mteb",
        "en",
        "arxiv:2401.03462",
        "arxiv:2312.15503",
        "arxiv:2311.13534",
        "arxiv:2310.07554",
        "arxiv:2309.07597",
        "license:mit",
        "model-index",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 7340919,
        "hf_likes": 346
      },
      "score": 14854.838,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "bge-base-en-v1.5 是一个基于 BERT 架构的英文文本嵌入模型，由北京智源人工智能研究院（BAAI）开发。该模型主要用于生成高质量的句子或段落级向量表示，适用于语义相似度计算、信息检索和文本聚类等任务。其亮点在于通过大规模对比学习优化，在多项基准测试（如 MTEB）中表现优异，尤其擅长处理英文文本的语义理解。适用于需要高效且精准的文本表示嵌入的场景，如搜索引擎、推荐系统或自然语言处理应用。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_en": "BGE-base-en-v1.5 is a BERT-based English sentence embedding model optimized for semantic similarity and retrieval tasks. It excels in generating dense vector representations for text, making it suitable for applications like search, clustering, and recommendation systems. With strong performance on benchmarks like MTEB, it is widely used in both research and production environments. The model supports multiple deployment formats, including PyTorch, ONNX, and SafeTensors, ensuring flexibility and ease of integration.",
      "summary_zh": "bge-base-en-v1.5 是一个基于 BERT 架构的英文文本嵌入模型，由北京智源人工智能研究院（BAAI）开发。该模型主要用于生成高质量的句子或段落级向量表示，适用于语义相似度计算、信息检索和文本聚类等任务。其亮点在于通过大规模对比学习优化，在多项基准测试（如 MTEB）中表现优异，尤其擅长处理英文文本的语义理解。适用于需要高效且精准的文本表示嵌入的场景，如搜索引擎、推荐系统或自然语言处理应用。",
      "summary_es": "Modelo de incrustación de texto BGE base en inglés, versión 1.5. Basado en BERT, genera representaciones vectoriales densas para textos. Destaca en similitud semántica y recuperación de información. Usos: búsqueda, clustering y aplicaciones de NLP que requieran comparación de similitudes entre oraciones."
    }
  ]
}