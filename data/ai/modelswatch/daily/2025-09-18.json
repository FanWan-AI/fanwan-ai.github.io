{
  "date": "2025-09-18",
  "updated_at": "2025-09-17T18:01:14.505Z",
  "items": [
    {
      "id": "twbs/bootstrap",
      "source": "github",
      "name": "bootstrap",
      "url": "https://github.com/twbs/bootstrap",
      "license": "MIT",
      "lang": "MDX",
      "tags": [
        "bootstrap",
        "css",
        "css-framework",
        "html",
        "javascript",
        "sass",
        "scss"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 173434,
        "forks": 79156,
        "issues": 575
      },
      "score": 189365.05255952934,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Bootstrap 是一个开源的前端框架，用于快速构建响应式、移动优先的网站和 Web 应用。它提供了一套丰富的 CSS 和 JavaScript 组件，包括网格系统、按钮、表单和导航等，帮助开发者高效实现一致的用户界面。其核心亮点在于强大的响应式设计能力，能够自动适配不同设备屏幕尺寸。适用于各类 Web 项目开发，尤其适合需要快速原型设计或追求开发效率的团队。",
      "updated_at": "2025-09-17T16:49:33Z",
      "summary_zh": "Bootstrap 是一个开源的前端框架，用于快速构建响应式、移动优先的网站和 Web 应用。它提供了一套丰富的 CSS 和 JavaScript 组件，包括网格系统、按钮、表单和导航等，帮助开发者高效实现一致的用户界面。其核心亮点在于强大的响应式设计能力，能够自动适配不同设备屏幕尺寸。适用于各类 Web 项目开发，尤其适合需要快速原型设计或追求开发效率的团队。"
    },
    {
      "id": "avelino/awesome-go",
      "source": "github",
      "name": "awesome-go",
      "url": "https://github.com/avelino/awesome-go",
      "license": "MIT",
      "lang": "Go",
      "tags": [
        "awesome",
        "awesome-list",
        "go",
        "golang",
        "golang-library",
        "hacktoberfest"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 152540,
        "forks": 12568,
        "issues": 157
      },
      "score": 155153.57667218364,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "这是一个精选的Go语言资源集合，收录了大量高质量的框架、库和软件项目。内容涵盖网络编程、数据库、工具链等多个领域，适合Go开发者快速查找和参考实用资源。项目由社区共同维护，遵循MIT开源协议，更新活跃且分类清晰。无论是初学者还是资深工程师，都能从中找到提升开发效率的利器。",
      "updated_at": "2025-09-17T17:43:10Z",
      "summary_zh": "这是一个精选的Go语言资源集合，收录了大量高质量的框架、库和软件项目。内容涵盖网络编程、数据库、工具链等多个领域，适合Go开发者快速查找和参考实用资源。项目由社区共同维护，遵循MIT开源协议，更新活跃且分类清晰。无论是初学者还是资深工程师，都能从中找到提升开发效率的利器。"
    },
    {
      "id": "trimstray/the-book-of-secret-knowledge",
      "source": "github",
      "name": "the-book-of-secret-knowledge",
      "url": "https://github.com/trimstray/the-book-of-secret-knowledge",
      "license": "MIT",
      "lang": "N/A",
      "tags": [
        "awesome",
        "awesome-list",
        "bsd",
        "cheatsheets",
        "devops",
        "guidelines",
        "hacking",
        "hacks",
        "howtos",
        "linux",
        "lists",
        "manuals",
        "one-liners",
        "pentesters",
        "resources",
        "search-engines",
        "security",
        "security-researchers",
        "sysops"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 186475,
        "forks": 11535,
        "issues": 100
      },
      "score": 188881.96745150464,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "《the-book-of-secret-knowledge》是一个开源的知识集合项目，收录了大量实用的技术资源，包括清单、手册、速查表、博客和命令行工具等。该项目主要面向开发者和运维人员，内容涵盖Linux、DevOps、安全攻防等多个技术领域。其亮点在于整理了大量高质量的技术参考和实用技巧，帮助用户快速解决实际问题。适用于日常开发、系统管理和安全研究等场景。",
      "updated_at": "2025-09-17T17:39:11Z",
      "summary_zh": "《the-book-of-secret-knowledge》是一个开源的知识集合项目，收录了大量实用的技术资源，包括清单、手册、速查表、博客和命令行工具等。该项目主要面向开发者和运维人员，内容涵盖Linux、DevOps、安全攻防等多个技术领域。其亮点在于整理了大量高质量的技术参考和实用技巧，帮助用户快速解决实际问题。适用于日常开发、系统管理和安全研究等场景。"
    },
    {
      "id": "ohmyzsh/ohmyzsh",
      "source": "github",
      "name": "ohmyzsh",
      "url": "https://github.com/ohmyzsh/ohmyzsh",
      "license": "MIT",
      "lang": "Shell",
      "tags": [
        "cli",
        "cli-app",
        "hacktoberfest",
        "oh-my-zsh",
        "oh-my-zsh-plugin",
        "oh-my-zsh-theme",
        "ohmyzsh",
        "plugin-framework",
        "plugins",
        "productivity",
        "shell",
        "terminal",
        "theme",
        "themes",
        "zsh",
        "zsh-configuration"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 181464,
        "forks": 26215,
        "issues": 502
      },
      "score": 186806.79700397377,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Ohmyzsh 是一个基于 Zsh 的社区驱动型 Shell 配置管理框架，拥有超过 2400 名贡献者。它提供了 300 多个可选插件，支持 Git、Docker、Python 等常用开发工具，以及 140 多种主题，可显著提升终端使用体验。内置自动更新工具，方便用户及时获取社区最新功能。适用于开发者、系统管理员及任何希望优化命令行效率的用户。",
      "updated_at": "2025-09-17T16:25:33Z",
      "summary_zh": "Ohmyzsh 是一个基于 Zsh 的社区驱动型 Shell 配置管理框架，拥有超过 2400 名贡献者。它提供了 300 多个可选插件，支持 Git、Docker、Python 等常用开发工具，以及 140 多种主题，可显著提升终端使用体验。内置自动更新工具，方便用户及时获取社区最新功能。适用于开发者、系统管理员及任何希望优化命令行效率的用户。"
    },
    {
      "id": "github/gitignore",
      "source": "github",
      "name": "gitignore",
      "url": "https://github.com/github/gitignore",
      "license": "CC0-1.0",
      "lang": "N/A",
      "tags": [
        "git",
        "gitignore"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 169421,
        "forks": 83002,
        "issues": 319
      },
      "score": 186121.2881690972,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "这是一个由 GitHub 官方维护的 `.gitignore` 模板集合，适用于各种编程语言、框架和开发环境。它可以帮助开发者快速生成适合项目的 `.gitignore` 文件，避免将不必要的文件（如日志、缓存、依赖目录等）提交到 Git 仓库中。该项目采用 CC0-1.0 协议，允许自由使用和分发。无论是个人项目还是团队协作，都可以通过这个模板库提高代码管理的效率和整洁性。",
      "updated_at": "2025-09-17T17:04:56Z",
      "summary_zh": "这是一个由 GitHub 官方维护的 `.gitignore` 模板集合，适用于各种编程语言、框架和开发环境。它可以帮助开发者快速生成适合项目的 `.gitignore` 文件，避免将不必要的文件（如日志、缓存、依赖目录等）提交到 Git 仓库中。该项目采用 CC0-1.0 协议，允许自由使用和分发。无论是个人项目还是团队协作，都可以通过这个模板库提高代码管理的效率和整洁性。"
    },
    {
      "id": "flutter/flutter",
      "source": "github",
      "name": "flutter",
      "url": "https://github.com/flutter/flutter",
      "license": "BSD-3-Clause",
      "lang": "Dart",
      "tags": [
        "android",
        "app-framework",
        "cross-platform",
        "dart",
        "dart-platform",
        "desktop",
        "flutter",
        "flutter-package",
        "fuchsia",
        "ios",
        "linux-desktop",
        "macos",
        "material-design",
        "mobile",
        "mobile-development",
        "skia",
        "web",
        "web-framework",
        "windows"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 172478,
        "forks": 29213,
        "issues": 12251
      },
      "score": 178420.59939594907,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Flutter 是由 Google 开发的开源 UI 框架，使用 Dart 语言编写，支持跨平台应用开发。它通过自绘引擎实现高性能渲染，提供丰富的组件库和开发工具，能够快速构建高质量的原生体验应用。适用于移动端（iOS 和 Android）、桌面端（Windows、macOS、Linux）以及嵌入式设备。Flutter 尤其适合需要高度定制 UI 或追求开发效率的团队，是构建现代多平台应用的主流选择之一。",
      "updated_at": "2025-09-17T17:52:59Z",
      "summary_zh": "Flutter 是由 Google 开发的开源 UI 框架，使用 Dart 语言编写，支持跨平台应用开发。它通过自绘引擎实现高性能渲染，提供丰富的组件库和开发工具，能够快速构建高质量的原生体验应用。适用于移动端（iOS 和 Android）、桌面端（Windows、macOS、Linux）以及嵌入式设备。Flutter 尤其适合需要高度定制 UI 或追求开发效率的团队，是构建现代多平台应用的主流选择之一。"
    },
    {
      "id": "AUTOMATIC1111/stable-diffusion-webui",
      "source": "github",
      "name": "stable-diffusion-webui",
      "url": "https://github.com/AUTOMATIC1111/stable-diffusion-webui",
      "license": "AGPL-3.0",
      "lang": "Python",
      "tags": [
        "ai",
        "ai-art",
        "deep-learning",
        "diffusion",
        "gradio",
        "image-generation",
        "image2image",
        "img2img",
        "pytorch",
        "stable-diffusion",
        "text2image",
        "torch",
        "txt2img",
        "unstable",
        "upscaling",
        "web"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 156485,
        "forks": 29043,
        "issues": 2426
      },
      "score": 162393.59025243056,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Stable Diffusion web UI 是一个基于 Gradio 构建的 Web 界面，用于运行 Stable Diffusion 图像生成模型。它支持文本生成图像、图像修复、风格转换等多种功能，并提供了丰富的自定义选项和插件扩展能力。该项目降低了用户使用扩散模型的技术门槛，适用于艺术创作、设计辅助和 AI 实验等场景。由于其开源特性和活跃的社区支持，已成为最受欢迎的 Stable Diffusion 前端工具之一。",
      "updated_at": "2025-09-17T17:49:02Z",
      "summary_zh": "Stable Diffusion web UI 是一个基于 Gradio 构建的 Web 界面，用于运行 Stable Diffusion 图像生成模型。它支持文本生成图像、图像修复、风格转换等多种功能，并提供了丰富的自定义选项和插件扩展能力。该项目降低了用户使用扩散模型的技术门槛，适用于艺术创作、设计辅助和 AI 实验等场景。由于其开源特性和活跃的社区支持，已成为最受欢迎的 Stable Diffusion 前端工具之一。"
    },
    {
      "id": "Snailclimb/JavaGuide",
      "source": "github",
      "name": "JavaGuide",
      "url": "https://github.com/Snailclimb/JavaGuide",
      "license": "Apache-2.0",
      "lang": "Java",
      "tags": [
        "algorithms",
        "interview",
        "java",
        "jvm",
        "mysql",
        "redis",
        "spring",
        "system",
        "system-design",
        "zookeeper"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 151773,
        "forks": 45985,
        "issues": 73
      },
      "score": 161069.93299934414,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "《JavaGuide》是一份面向 Java 开发者的综合性学习与面试指南，内容涵盖 Java 核心知识、JVM、常用框架（如 Spring）、数据库（MySQL、Redis）、分布式系统设计及常用中间件（如 ZooKeeper）。该项目结构清晰、内容实用，不仅适合系统学习 Java 技术栈，也是准备技术面试的重要参考资料。凭借高质量的内容和开源社区的支持，它已成为 GitHub 上最受欢迎的 Java 资源库之一。",
      "updated_at": "2025-09-17T17:24:18Z",
      "summary_zh": "《JavaGuide》是一份面向 Java 开发者的综合性学习与面试指南，内容涵盖 Java 核心知识、JVM、常用框架（如 Spring）、数据库（MySQL、Redis）、分布式系统设计及常用中间件（如 ZooKeeper）。该项目结构清晰、内容实用，不仅适合系统学习 Java 技术栈，也是准备技术面试的重要参考资料。凭借高质量的内容和开源社区的支持，它已成为 GitHub 上最受欢迎的 Java 资源库之一。"
    },
    {
      "id": "huggingface/transformers",
      "source": "github",
      "name": "transformers",
      "url": "https://github.com/huggingface/transformers",
      "license": "Apache-2.0",
      "lang": "Python",
      "tags": [
        "audio",
        "deep-learning",
        "deepseek",
        "gemma",
        "glm",
        "hacktoberfest",
        "llm",
        "machine-learning",
        "model-hub",
        "natural-language-processing",
        "nlp",
        "pretrained-models",
        "python",
        "pytorch",
        "pytorch-transformers",
        "qwen",
        "speech-recognition",
        "transformer",
        "vlm"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "stars": 149912,
        "forks": 30436,
        "issues": 1990
      },
      "score": 156099.1655996528,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "🤗 Transformers 是一个基于 Apache-2.0 协议的开源框架，专注于提供最先进的机器学习模型，涵盖文本、视觉、音频和多模态任务。它支持从推理到训练的全流程，适用于自然语言处理、音频分析及多模态融合等多种场景。其核心亮点包括丰富的预训练模型库（如 DeepSeek、Gemma、GLM 等），以及高度模块化的设计，便于研究人员和开发者快速实验和部署。无论是学术研究还是工业应用，Transformers 都能提供高效且灵活的解决方案。",
      "updated_at": "2025-09-17T17:38:23Z",
      "summary_zh": "🤗 Transformers 是一个基于 Apache-2.0 协议的开源框架，专注于提供最先进的机器学习模型，涵盖文本、视觉、音频和多模态任务。它支持从推理到训练的全流程，适用于自然语言处理、音频分析及多模态融合等多种场景。其核心亮点包括丰富的预训练模型库（如 DeepSeek、Gemma、GLM 等），以及高度模块化的设计，便于研究人员和开发者快速实验和部署。无论是学术研究还是工业应用，Transformers 都能提供高效且灵活的解决方案。"
    },
    {
      "id": "omni-research/Tarsier2-Recap-7b",
      "source": "hf",
      "name": "Tarsier2-Recap-7b",
      "url": "https://huggingface.co/omni-research/Tarsier2-Recap-7b",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "safetensors",
        "video LLM",
        "arxiv:2501.07888",
        "license:apache-2.0",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 8263301,
        "hf_likes": 17
      },
      "score": 16535.102,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Tarsier2-Recap-7b 是一个基于 Transformer 架构的视频语言模型，专注于视频内容理解与摘要生成。该模型能够高效解析视频内容，并生成结构化的文本摘要，适用于视频内容检索、自动字幕生成和智能剪辑等场景。其亮点在于结合了多模态输入处理能力，支持对长视频进行关键信息提取，同时保持较高的生成质量与语义连贯性。该模型基于 Apache 2.0 开源协议发布，适用于视频分析、媒体生产及人机交互等领域的研究与应用。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_zh": "Tarsier2-Recap-7b 是一个基于 Transformer 架构的视频语言模型，专注于视频内容理解与摘要生成。该模型能够高效解析视频内容，并生成结构化的文本摘要，适用于视频内容检索、自动字幕生成和智能剪辑等场景。其亮点在于结合了多模态输入处理能力，支持对长视频进行关键信息提取，同时保持较高的生成质量与语义连贯性。该模型基于 Apache 2.0 开源协议发布，适用于视频分析、媒体生产及人机交互等领域的研究与应用。"
    },
    {
      "id": "meta-llama/Llama-3.1-8B-Instruct",
      "source": "hf",
      "name": "Llama-3.1-8B-Instruct",
      "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "facebook",
        "meta",
        "pytorch",
        "llama-3",
        "conversational",
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th",
        "arxiv:2204.05149",
        "base_model:meta-llama/Llama-3.1-8B",
        "base_model:finetune:meta-llama/Llama-3.1-8B",
        "license:llama3.1",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 7230352,
        "hf_likes": 4626
      },
      "score": 16773.703999999998,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Llama-3.1-8B-Instruct 是 Meta 推出的开源指令微调语言模型，基于 Transformer 架构，参数量为 80 亿。该模型专为对话和指令跟随任务设计，支持流畅的文本生成与多轮交互。其亮点在于高效的推理性能和较强的上下文理解能力，适用于聊天机器人、内容创作和任务自动化等场景。模型以 PyTorch 和 SafeTensors 格式提供，主要面向英语用户，适合研究和轻量级应用部署。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_zh": "Llama-3.1-8B-Instruct 是 Meta 推出的开源指令微调语言模型，基于 Transformer 架构，参数量为 80 亿。该模型专为对话和指令跟随任务设计，支持流畅的文本生成与多轮交互。其亮点在于高效的推理性能和较强的上下文理解能力，适用于聊天机器人、内容创作和任务自动化等场景。模型以 PyTorch 和 SafeTensors 格式提供，主要面向英语用户，适合研究和轻量级应用部署。"
    },
    {
      "id": "amazon/chronos-t5-small",
      "source": "hf",
      "name": "chronos-t5-small",
      "url": "https://huggingface.co/amazon/chronos-t5-small",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "t5",
        "text2text-generation",
        "time series",
        "forecasting",
        "pretrained models",
        "foundation models",
        "time series foundation models",
        "time-series",
        "time-series-forecasting",
        "arxiv:2403.07815",
        "arxiv:1910.10683",
        "license:apache-2.0",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 13199198,
        "hf_likes": 131
      },
      "score": 26463.896,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Chronos-T5-small是亚马逊基于T5架构开发的时间序列预测基础模型。该模型通过将数值序列转换为标记化文本进行训练，能够直接生成未来时间点的预测值。其核心优势在于统一的文本到文本生成框架，无需针对不同数据集重新设计特征工程。模型适用于商业销售预测、能源需求预估、设备监控等标准化时间序列分析场景，为中小规模预测任务提供了开箱即用的解决方案。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_zh": "Chronos-T5-small是亚马逊基于T5架构开发的时间序列预测基础模型。该模型通过将数值序列转换为标记化文本进行训练，能够直接生成未来时间点的预测值。其核心优势在于统一的文本到文本生成框架，无需针对不同数据集重新设计特征工程。模型适用于商业销售预测、能源需求预估、设备监控等标准化时间序列分析场景，为中小规模预测任务提供了开箱即用的解决方案。"
    },
    {
      "id": "distilbert/distilbert-base-uncased",
      "source": "hf",
      "name": "distilbert-base-uncased",
      "url": "https://huggingface.co/distilbert/distilbert-base-uncased",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "safetensors",
        "distilbert",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1910.01108",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 12227309,
        "hf_likes": 755
      },
      "score": 24832.118000000002,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "DistilBERT-base-uncased 是一个轻量化的 BERT 变体，通过知识蒸馏技术将 BERT 模型压缩至约 40% 的规模，同时保留了 97% 的语言理解能力。该模型适用于掩码语言建模任务，支持多种框架（PyTorch、TensorFlow、JAX、Rust），并针对英文文本优化。其核心优势在于显著降低计算和存储需求，适合资源受限环境或需要快速推理的应用场景，如文本分类、实体识别和信息检索。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_zh": "DistilBERT-base-uncased 是一个轻量化的 BERT 变体，通过知识蒸馏技术将 BERT 模型压缩至约 40% 的规模，同时保留了 97% 的语言理解能力。该模型适用于掩码语言建模任务，支持多种框架（PyTorch、TensorFlow、JAX、Rust），并针对英文文本优化。其核心优势在于显著降低计算和存储需求，适合资源受限环境或需要快速推理的应用场景，如文本分类、实体识别和信息检索。"
    },
    {
      "id": "FacebookAI/roberta-large",
      "source": "hf",
      "name": "roberta-large",
      "url": "https://huggingface.co/FacebookAI/roberta-large",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "onnx",
        "safetensors",
        "roberta",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1907.11692",
        "arxiv:1806.02847",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 12085823,
        "hf_likes": 245
      },
      "score": 24294.146,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "RoBERTa-large是基于Transformer架构的大规模预训练语言模型，由Facebook AI开发。它在BERT的基础上通过优化训练策略（如移除下一句预测任务）提升了性能，适用于多种自然语言处理任务。该模型支持掩码语言建模，可用于文本分类、命名实体识别和语义理解等场景。支持PyTorch、TensorFlow、JAX等多种框架，适合研究人员和开发者进行高效实验与应用部署。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_zh": "RoBERTa-large是基于Transformer架构的大规模预训练语言模型，由Facebook AI开发。它在BERT的基础上通过优化训练策略（如移除下一句预测任务）提升了性能，适用于多种自然语言处理任务。该模型支持掩码语言建模，可用于文本分类、命名实体识别和语义理解等场景。支持PyTorch、TensorFlow、JAX等多种框架，适合研究人员和开发者进行高效实验与应用部署。"
    },
    {
      "id": "google/electra-base-discriminator",
      "source": "hf",
      "name": "electra-base-discriminator",
      "url": "https://huggingface.co/google/electra-base-discriminator",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "electra",
        "pretraining",
        "en",
        "arxiv:1406.2661",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 11547887,
        "hf_likes": 64
      },
      "score": 23127.774,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "ELECTRA-base-discriminator是由Google开发的预训练语言模型，采用判别式预训练方法，通过区分真实输入与生成输入来提升语言理解能力。该模型基于ELECTRA架构，适用于多种自然语言处理任务，如文本分类、命名实体识别和情感分析。支持多种框架（PyTorch、TensorFlow、JAX、Rust），具备较强的跨平台兼容性。适用于需要高效文本理解与生成的场景，如搜索引擎、对话系统和内容分析工具。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_zh": "ELECTRA-base-discriminator是由Google开发的预训练语言模型，采用判别式预训练方法，通过区分真实输入与生成输入来提升语言理解能力。该模型基于ELECTRA架构，适用于多种自然语言处理任务，如文本分类、命名实体识别和情感分析。支持多种框架（PyTorch、TensorFlow、JAX、Rust），具备较强的跨平台兼容性。适用于需要高效文本理解与生成的场景，如搜索引擎、对话系统和内容分析工具。"
    },
    {
      "id": "prajjwal1/bert-tiny",
      "source": "hf",
      "name": "bert-tiny",
      "url": "https://huggingface.co/prajjwal1/bert-tiny",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "BERT",
        "MNLI",
        "NLI",
        "transformer",
        "pre-training",
        "en",
        "arxiv:1908.08962",
        "arxiv:2110.01518",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 10176180,
        "hf_likes": 127
      },
      "score": 20415.86,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "bert-tiny是一个轻量级BERT模型，专为资源受限环境设计。它基于BERT架构，但显著减少了参数规模，适用于移动设备和边缘计算场景。该模型支持自然语言推理（NLI）任务，并在MNLI数据集上进行了微调。其核心优势在于保持较高推理效率的同时，大幅降低计算和存储需求，适合部署在实时或低延迟应用中。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_zh": "bert-tiny是一个轻量级BERT模型，专为资源受限环境设计。它基于BERT架构，但显著减少了参数规模，适用于移动设备和边缘计算场景。该模型支持自然语言推理（NLI）任务，并在MNLI数据集上进行了微调。其核心优势在于保持较高推理效率的同时，大幅降低计算和存储需求，适合部署在实时或低延迟应用中。"
    },
    {
      "id": "trpakov/vit-face-expression",
      "source": "hf",
      "name": "vit-face-expression",
      "url": "https://huggingface.co/trpakov/vit-face-expression",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "vit",
        "image-classification",
        "doi:10.57967/hf/2289",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 8025363,
        "hf_likes": 78
      },
      "score": 16089.726,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "vit-face-expression是基于Vision Transformer架构的面部表情分类模型，适用于图像分类任务。该模型能够识别输入人脸图像中的七种基本表情，包括愤怒、厌恶、恐惧、快乐、悲伤、惊讶和中性。其亮点在于结合了ViT的全局建模能力和高效推理性能，支持ONNX和Safetensors格式，便于部署。适用于情感分析、人机交互及心理学研究等场景，尤其适合需要高精度表情识别的应用。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_zh": "vit-face-expression是基于Vision Transformer架构的面部表情分类模型，适用于图像分类任务。该模型能够识别输入人脸图像中的七种基本表情，包括愤怒、厌恶、恐惧、快乐、悲伤、惊讶和中性。其亮点在于结合了ViT的全局建模能力和高效推理性能，支持ONNX和Safetensors格式，便于部署。适用于情感分析、人机交互及心理学研究等场景，尤其适合需要高精度表情识别的应用。"
    },
    {
      "id": "BAAI/bge-base-en-v1.5",
      "source": "hf",
      "name": "bge-base-en-v1.5",
      "url": "https://huggingface.co/BAAI/bge-base-en-v1.5",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "mteb",
        "en",
        "arxiv:2401.03462",
        "arxiv:2312.15503",
        "arxiv:2311.13534",
        "arxiv:2310.07554",
        "arxiv:2309.07597",
        "license:mit",
        "model-index",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 7340919,
        "hf_likes": 346
      },
      "score": 14854.838,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "bge-base-en-v1.5 是一个基于 BERT 架构的英文文本嵌入模型，由北京智源人工智能研究院（BAAI）开发。该模型专门用于生成高质量的句子级向量表示，适用于文本相似度计算、语义搜索和信息检索等任务。其亮点在于通过大规模对比学习优化，在多项基准测试（如 MTEB）中表现出色，尤其擅长处理英文短文本的语义理解。适用于需要高效且精准的语义匹配场景，如搜索引擎、推荐系统和问答系统。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_zh": "bge-base-en-v1.5 是一个基于 BERT 架构的英文文本嵌入模型，由北京智源人工智能研究院（BAAI）开发。该模型专门用于生成高质量的句子级向量表示，适用于文本相似度计算、语义搜索和信息检索等任务。其亮点在于通过大规模对比学习优化，在多项基准测试（如 MTEB）中表现出色，尤其擅长处理英文短文本的语义理解。适用于需要高效且精准的语义匹配场景，如搜索引擎、推荐系统和问答系统。"
    }
  ]
}