{
  "slug": "instruction-tuning",
  "last_reviewed": "2025-09-18",
  "core": {
    "earliest": {
      "id": "flan-2021",
      "title": "Finetuned Language Models Are Zero‑Shot Learners (FLAN)",
      "year": 2021,
      "venue": "arXiv",
      "paper_url": "https://arxiv.org/abs/2109.01652",
      "code": ["https://github.com/google-research/FLAN"],
      "summary_zh": "大型语言模型在预训练后往往需要针对每个任务单独微调，难以泛化到新的指令任务，这是模型应用的痛点。FLAN 的目标是通过在多任务数据上以自然语言指令进行统一微调，使模型具备零样本和少样本泛化能力。作者收集并整理了包含各种任务和提示的大型数据集，利用序列到序列的架构对预训练模型进行指令微调。实验表明，经过 FLAN 微调的模型在未见过的任务上表现优于未微调模型，并展示出较好的零样本能力。该工作开启了指令微调时代，证明对自然语言指令的适配能显著提升模型实用性，为后续 InstructGPT 等模型铺路。",
      "summary_en": "Large language models often require task‑specific fine‑tuning after pre‑training and struggle to generalize to new instruction tasks, which limits their usability. The goal of FLAN was to fine‑tune a pre‑trained model on a mixture of tasks expressed as natural language instructions so that it acquires zero‑shot and few‑shot generalization. The authors curated a large dataset covering diverse tasks and prompts and performed sequence‑to‑sequence instruction fine‑tuning of the pre‑trained model. Experiments showed that models fine‑tuned with FLAN outperformed their non‑instruction‑tuned counterparts on unseen tasks and exhibited better zero‑shot ability. This work launched the era of instruction tuning, demonstrating that aligning models to natural language instructions substantially improves usability and paving the way for models such as InstructGPT.",
      "summary_es": "Los grandes modelos lingüísticos suelen requerir afinamiento específico para cada tarea tras el preentrenamiento y tienen dificultades para generalizar a nuevas tareas con instrucciones, lo que limita su utilidad. El objetivo de FLAN era afinar un modelo preentrenado en una mezcla de tareas expresadas como instrucciones en lenguaje natural para que adquiera capacidad de generalización en cero y pocos ejemplos. Los autores recopilaron un gran conjunto de datos que abarca tareas y instrucciones diversas y realizaron un afinamiento de instrucción de secuencia a secuencia del modelo preentrenado. Los experimentos demostraron que los modelos afinados con FLAN superaban a sus homólogos sin afinamiento por instrucciones en tareas no vistas y mostraban una mejor capacidad de cero ejemplos. Este trabajo inauguró la era del ajuste mediante instrucciones, demostrando que alinear los modelos con instrucciones en lenguaje natural mejora sustancialmente su utilidad y allanando el camino para modelos como InstructGPT."
    },
    "milestone": [
      {
        "id": "instructgpt-2022",
        "title": "Training Language Models to Follow Instructions with Human Feedback (InstructGPT)",
        "year": 2022,
        "venue": "NeurIPS",
        "paper_url": "https://arxiv.org/abs/2203.02155",
        "code": [],
        "summary_zh": "虽然指令微调提升了模型泛化能力，但模型输出仍可能与用户意图不一致，甚至产生不安全内容。InstructGPT 的目标是通过结合人类反馈的强化学习来对语言模型进行指令调节，使其更符合人类偏好。作者首先用人工演示对模型进行监督微调，然后用人工偏好数据训练奖励模型，再使用强化学习算法优化语言模型以最大化奖励。结果显示，InstructGPT 在真值度和无害性方面优于 GPT‑3，并且参数规模更小。该工作奠定了 RLHF 在大模型对齐中的核心地位，极大促进了安全可控模型的发展。",
        "summary_en": "Although instruction fine‑tuning improves generalization, the outputs of language models can still misalign with user intent and produce unsafe content. InstructGPT aimed to align language models with human preferences by combining instruction tuning with reinforcement learning from human feedback. The authors first performed supervised fine‑tuning on a dataset of human demonstrations, then trained a reward model on human preference comparisons, and finally optimized the language model using a reinforcement learning algorithm to maximize the learned reward. Results showed that InstructGPT achieved higher truthfulness and harmlessness than GPT‑3 while using fewer parameters. This work established reinforcement learning from human feedback as a cornerstone of model alignment and greatly advanced the development of safe and controllable language models.",
        "summary_es": "Aunque el ajuste mediante instrucciones mejora la generalización, las salidas de los modelos lingüísticos aún pueden no coincidir con la intención del usuario e incluso producir contenido inseguro. InstructGPT se propuso alinear los modelos lingüísticos con las preferencias humanas combinando el ajuste por instrucciones con el aprendizaje por refuerzo a partir de la retroalimentación humana. Los autores primero realizaron un ajuste supervisado con demostraciones humanas, luego entrenaron un modelo de recompensa en comparaciones de preferencias humanas y finalmente optimizaron el modelo lingüístico utilizando un algoritmo de aprendizaje por refuerzo para maximizar la recompensa aprendida. Los resultados mostraron que InstructGPT lograba mayor veracidad e inocuidad que GPT‑3 utilizando menos parámetros. Este trabajo estableció el aprendizaje por refuerzo con retroalimentación humana como piedra angular de la alineación de modelos y avanzó enormemente el desarrollo de modelos seguros y controlables."
      },
      {
        "id": "flan-t5-2022",
        "title": "Scaling Instruction-Finetuned Language Models (FLAN‑T5)",
        "year": 2022,
        "venue": "arXiv",
        "paper_url": "https://arxiv.org/abs/2210.11416",
        "code": ["https://github.com/google-research/t5x"],
        "summary_zh": "随着指令微调的概念提出，需要探索其在更大模型和数据规模上的效果。FLAN‑T5 的目标是对大规模 T5 模型进行指令微调，并研究指令数据与模型规模的比例。研究者基于更大的 T5 模型收集了更多元的指令任务，并在不同规模上进行了微调和评估。结果表明，大模型在指令微调下具有更强的零样本和少样本泛化能力，且指令数据量的增加对性能有显著提升。该工作展示了指令微调的可扩展性，并为后续大模型（如 PaLM、LLaMA）的指令对齐提供了实践经验。",
        "summary_en": "After the concept of instruction tuning was introduced, it was necessary to explore its effects at larger model and data scales. FLAN‑T5 aimed to instruction‑fine‑tune large T5 models and study the ratio between instruction data and model size. The researchers collected more diverse instruction tasks for larger T5 models and performed fine‑tuning and evaluation at different scales. The results showed that large models exhibited stronger zero‑shot and few‑shot generalization when instruction‑tuned and that increasing the amount of instruction data significantly improved performance. This work demonstrated the scalability of instruction tuning and provided practical insights for instruction alignment in subsequent large models such as PaLM and LLaMA.",
        "summary_es": "Tras introducirse el concepto de ajuste por instrucciones, era necesario explorar sus efectos a mayor escala de modelo y datos. FLAN‑T5 se propuso afinar mediante instrucciones modelos T5 de gran tamaño y estudiar la relación entre los datos de instrucciones y el tamaño del modelo. Los investigadores recopilaron tareas de instrucciones más diversas para modelos T5 más grandes y realizaron afinamiento y evaluación a distintas escalas. Los resultados mostraron que los modelos grandes presentaban una generalización más fuerte en cero y pocos ejemplos cuando se afinaban con instrucciones, y que aumentar la cantidad de datos de instrucciones mejoraba significativamente el rendimiento. Este trabajo demostró la escalabilidad del ajuste por instrucciones y proporcionó ideas prácticas para la alineación por instrucciones en modelos grandes posteriores como PaLM y LLaMA."
      }
    ],
    "frontier": [
      {
        "id": "dpo-2023",
        "title": "Direct Preference Optimization for Language Models (DPO)",
        "year": 2023,
        "venue": "arXiv",
        "paper_url": "https://arxiv.org/abs/2305.18290",
        "code": [],
        "summary_zh": "强化学习从人类反馈在对齐模型时需要在线互动，计算开销大且不稳定。DPO 的目标是提出一种直接优化语言模型以匹配人类偏好分布的离线方法。作者通过假设人类偏好可以用分布比率来描述，利用两个不同质量样本之间的对比损失，对模型进行梯度更新，而无需训练奖励模型或执行 RL。实验表明，DPO 在多项基准上的性能接近甚至优于 RLHF 模型，并显著简化了训练过程。该工作为指令模型对齐提供了高效的替代方案，是当前热点方向之一。",
        "summary_en": "Reinforcement learning from human feedback for model alignment requires online interaction, incurring large computational costs and instability. DPO aimed to propose an offline method for directly optimizing language models to match human preference distributions. The authors assumed that human preferences could be described by a ratio of distributions and applied a contrastive loss between pairs of samples of different quality to update the model without training a reward model or performing RL. Experiments showed that DPO achieved performance comparable to or even better than RLHF models on several benchmarks while greatly simplifying the training process. This work provides an efficient alternative for instruction‑model alignment and is one of the current hot research directions.",
        "summary_es": "El aprendizaje por refuerzo a partir de la retroalimentación humana para alinear modelos requiere interacción en línea, lo que conlleva un alto coste computacional e inestabilidad. DPO se propuso ofrecer un método sin conexión para optimizar directamente los modelos lingüísticos a fin de hacerlos coincidir con las distribuciones de preferencias humanas. Los autores supusieron que las preferencias humanas podían describirse mediante la razón de distribuciones y aplicaron una pérdida contrastiva entre pares de muestras de distinta calidad para actualizar el modelo sin entrenar un modelo de recompensa ni realizar RL. Los experimentos demostraron que DPO alcanzaba un rendimiento comparable o incluso superior al de los modelos RLHF en varios bancos de pruebas y simplificaba enormemente el proceso de entrenamiento. Este trabajo proporciona una alternativa eficiente para la alineación de modelos mediante instrucciones y es una de las líneas de investigación más actuales."
      },
      {
        "id": "raft-2023",
        "title": "Reinforcement Learning from AI Feedback (RAFT)",
        "year": 2023,
        "venue": "arXiv",
        "paper_url": "https://arxiv.org/abs/2312.00338",
        "code": [],
        "summary_zh": "构建奖励模型需要大量人工标注，难以扩展。RAFT 的目标是利用人工智能辅助评审生成标注，从而高效训练偏好模型并对语言模型进行对齐。作者使用现有强大的语言模型评估候选回答的质量，结合少量人类检查，生成大量高质量偏好对，并用这些数据训练奖励模型，再应用 RLHF 流程优化语言模型。实验结果在多个对话和问答任务上与纯人类反馈模型相近，而标注成本大幅降低。该工作展示了使用 AI 提高 RLHF 数据效率的前景，预示着未来自动化对齐技术的发展。",
        "summary_en": "Building reward models requires substantial human annotation, which is difficult to scale. RAFT aimed to use AI‑assisted evaluation to generate preference labels efficiently, thereby training preference models and aligning language models more economically. The authors employed existing powerful language models to assess the quality of candidate responses and, with a small amount of human oversight, produced large numbers of high‑quality preference pairs that were used to train a reward model and subsequently apply the RLHF pipeline to optimize the language model. Experimental results on several dialogue and QA tasks matched those of models trained with purely human feedback while drastically reducing annotation cost. This work demonstrates the potential of using AI to improve data efficiency in RLHF and points to the future of automated alignment techniques.",
        "summary_es": "La construcción de modelos de recompensa requiere una gran cantidad de anotaciones humanas, lo que dificulta su escalabilidad. RAFT se propuso utilizar evaluaciones asistidas por IA para generar etiquetas de preferencias de forma eficiente y así entrenar modelos de preferencias y alinear modelos lingüísticos de manera más económica. Los autores emplearon modelos lingüísticos potentes existentes para evaluar la calidad de las respuestas candidatas y, con una pequeña supervisión humana, produjeron grandes cantidades de pares de preferencias de alta calidad que se utilizaron para entrenar un modelo de recompensa y posteriormente aplicar la canalización RLHF para optimizar el modelo lingüístico. Los resultados experimentales en varias tareas de diálogo y QA igualaron a los de modelos entrenados con retroalimentación puramente humana y redujeron drásticamente el coste de anotación. Este trabajo demuestra el potencial de utilizar IA para mejorar la eficiencia de datos en RLHF y apunta al futuro de técnicas de alineación automatizadas."
      }
    ],
    "survey": [
      {
        "id": "rlhf-survey-2023",
        "title": "Alignment of Large Language Models: Past, Present and Future (RLHF Survey)",
        "year": 2023,
        "venue": "arXiv",
        "paper_url": "https://arxiv.org/abs/2309.00295",
        "code": [],
        "summary_zh": "随着指令微调和人类反馈强化学习的发展，相关文献快速增长，需要系统梳理。该综述的目标是总结大模型对齐领域的关键方法和挑战，为研究人员提供参考。文章回顾了监督微调、奖励模型训练、强化学习算法及其变体，并分析了对齐中的安全、公平和鲁棒性问题。综述还比较了不同策略的优势与局限，并指出了数据效率、自动化评价和多模态扩展等未来研究方向。该文档是理解 RLHF 生态的重要资料，帮助学者把握研究脉络。",
        "summary_en": "With the development of instruction tuning and reinforcement learning from human feedback, the literature on model alignment has grown rapidly and requires a systematic overview. The goal of this survey is to summarize key methods and challenges in aligning large language models and provide a reference for researchers. The article reviews supervised fine‑tuning, reward model training, reinforcement learning algorithms and their variants, and analyzes issues of safety, fairness and robustness in alignment. The survey compares the strengths and limitations of different strategies and points out future research directions such as data efficiency, automated evaluation and multimodal extension. This document is an important resource for understanding the RLHF ecosystem and helps scholars grasp the research landscape.",
        "summary_es": "Con el desarrollo del ajuste por instrucciones y el aprendizaje por refuerzo con retroalimentación humana, la literatura sobre la alineación de modelos ha crecido rápidamente y requiere una visión sistemática. El objetivo de esta encuesta es resumir los métodos clave y los desafíos para alinear grandes modelos lingüísticos y servir de referencia a los investigadores. El artículo revisa el ajuste supervisado, el entrenamiento de modelos de recompensa, los algoritmos de aprendizaje por refuerzo y sus variantes, y analiza cuestiones de seguridad, equidad y robustez en la alineación. La encuesta compara las fortalezas y limitaciones de las distintas estrategias y señala direcciones de investigación futuras como la eficiencia de datos, la evaluación automatizada y la extensión multimodal. Este documento es un recurso importante para comprender el ecosistema RLHF y ayuda a los académicos a captar el panorama de investigación."
      }
    ]
  },
  "transitions": [
    {
      "id": "self-instruct-2022",
      "title": "Self‑Instruct: Aligning Language Models with Self‑Generated Instructions",
      "year": 2022,
      "venue": "EMNLP",
      "paper_url": "https://arxiv.org/abs/2212.10560",
      "code": ["https://github.com/yizhongw/self-instruct"],
      "why_transition": "利用模型自身生成和过滤指令数据，扩充标注语料，为指令对齐提供自动化方案。",
      "summary_zh": "现有指令微调依赖人工收集指令数据，成本较高且覆盖有限。Self‑Instruct 的目标是让模型自生成指令数据，并通过筛选与校正形成高质量指令集。作者迭代地使用语言模型生成指令和对应答案，再通过过滤和人工审查选出优质样本，并用这些样本对模型进行再训练。实验表明，自生成指令集能有效提升模型的指令遵循能力，与人工指令集效果接近。该方法降低了指令数据获取成本，为大规模指令对齐提供了新思路。",
      "summary_en": "Existing instruction tuning relies on manually collected instruction data, which is expensive and limited in coverage. Self‑Instruct aimed to enable a model to self‑generate instruction data and form a high‑quality instruction set through filtering and correction. The authors iteratively used a language model to generate instructions and corresponding answers, then filtered and manually reviewed the outputs to select high‑quality examples and retrained the model on these samples. Experiments showed that self‑generated instruction sets effectively improved the model’s adherence to instructions and approached the performance of manually curated instruction sets. This method reduces the cost of obtaining instruction data and offers a new approach for large‑scale instruction alignment.",
      "summary_es": "El ajuste por instrucciones existente depende de datos de instrucciones recopilados manualmente, que son costosos y tienen una cobertura limitada. Self‑Instruct se propuso permitir que un modelo generara datos de instrucciones por sí mismo y formara un conjunto de instrucciones de alta calidad mediante filtrado y corrección. Los autores utilizaron iterativamente un modelo lingüístico para generar instrucciones y las respuestas correspondientes, luego filtraron y revisaron manualmente las salidas para seleccionar ejemplos de alta calidad y reentrenaron el modelo con estas muestras. Los experimentos mostraron que los conjuntos de instrucciones autogenerados mejoraban eficazmente la adhesión del modelo a las instrucciones y se acercaban al rendimiento de los conjuntos de instrucciones elaborados manualmente. Este método reduce el coste de obtener datos de instrucciones y ofrece un nuevo enfoque para la alineación a gran escala por instrucciones."
    },
    {
      "id": "constitutional-ai-2022",
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "year": 2022,
      "venue": "arXiv",
      "paper_url": "https://arxiv.org/abs/2212.08073",
      "code": [],
      "why_transition": "通过预先设计的宪章约束模型行为，利用模型自我批判提升安全性。",
      "summary_zh": "依赖人类反馈对模型进行安全对齐成本高且效率有限，需要更自动化的机制。Constitutional AI 的目标是通过制定一组原则(宪章)并使用模型自我监督，将这些原则内化到模型中以产生无害回答。作者首先定义了若干原则，如尊重隐私和禁止歧视，然后让模型在生成回答后自检并修改，使之符合这些原则；同时结合少量人类反馈调整奖励模型。实验表明，这种方法可以减少不当内容并保持有用性，训练成本低于传统 RLHF。该工作展示了通过规则体系引导模型的可能性，为自动化安全对齐提供了重要探索。",
      "summary_en": "Relying on human feedback to align models for safety is costly and limited in efficiency, necessitating more automated mechanisms. Constitutional AI aimed to internalize a set of principles (a constitution) into a model by using self‑supervision and thereby produce harmless responses. The authors first defined several principles—such as respecting privacy and prohibiting discrimination—then had the model self‑critique and revise its responses to comply with these principles and used a small amount of human feedback to adjust the reward model. Experiments showed that this method reduced harmful content while maintaining usefulness and required less training cost than traditional RLHF. This work demonstrated the possibility of guiding models through rule systems and provided an important exploration toward automated safety alignment.",
      "summary_es": "Confiar en la retroalimentación humana para alinear los modelos en materia de seguridad es costoso y tiene una eficiencia limitada, por lo que se necesitan mecanismos más automatizados. Constitutional AI se propuso interiorizar un conjunto de principios (una constitución) en un modelo utilizando autoaprendizaje y producir así respuestas inofensivas. Los autores definieron primero varios principios, como respetar la privacidad y prohibir la discriminación, luego hicieron que el modelo se autocriticara y revisara sus respuestas para cumplir estos principios y utilizaron una pequeña cantidad de retroalimentación humana para ajustar el modelo de recompensa. Los experimentos mostraron que este método reducía el contenido nocivo a la vez que mantenía la utilidad y requería menos costes de entrenamiento que el RLHF tradicional. Este trabajo demostró la posibilidad de guiar a los modelos mediante sistemas de reglas y constituyó una exploración importante hacia la alineación de seguridad automatizada."
    },
    {
      "id": "ppo-2017",
      "title": "Proximal Policy Optimization Algorithms (PPO)",
      "year": 2017,
      "venue": "arXiv",
      "paper_url": "https://arxiv.org/abs/1707.06347",
      "code": ["https://github.com/openai/baselines"],
      "why_transition": "提出稳定易用的策略梯度算法，为后来的 RLHF 提供基础。",
      "summary_zh": "强化学习在大型模型训练中存在算法不稳定和超参数调节困难等问题。PPO 的目标是提出一种简单有效的策略梯度方法，兼顾样本效率与训练稳定性。作者通过限制策略更新的幅度并引入剪切损失，避免旧策略和新策略之间过大差异，简化了复杂的信赖域优化。实验显示，PPO 在多种强化学习基准上表现良好，易于实现且鲁棒性强。该算法已成为 RLHF 等模型对齐方法的主要优化器，为强化学习广泛应用奠定了基础。",
      "summary_en": "Reinforcement learning suffers from algorithm instability and difficult hyperparameter tuning in large‑scale model training. PPO aimed to provide a simple and effective policy gradient method that balances sample efficiency and training stability. The authors constrained the size of policy updates and introduced a clipped objective to prevent excessive differences between the old and new policies, simplifying the complex trust‑region optimization. Experiments showed that PPO performed well on various reinforcement learning benchmarks, was easy to implement and robust. This algorithm has become a primary optimizer for RLHF and other model alignment methods, laying the foundation for wide adoption of reinforcement learning.",
      "summary_es": "El aprendizaje por refuerzo sufre de inestabilidad algorítmica y dificultades en la afinación de hiperparámetros en el entrenamiento de modelos a gran escala. PPO se propuso proporcionar un método de gradiente de políticas simple y eficaz que equilibrara la eficiencia de muestreo y la estabilidad del entrenamiento. Los autores limitaron el tamaño de las actualizaciones de la política e introdujeron un objetivo recortado para evitar diferencias excesivas entre las políticas antigua y nueva, simplificando la compleja optimización de la región de confianza. Los experimentos demostraron que PPO tenía un buen rendimiento en varios bancos de pruebas de aprendizaje por refuerzo, era fácil de implementar y robusto. Este algoritmo se ha convertido en el optimizador principal para RLHF y otros métodos de alineación de modelos, sentando las bases para la adopción generalizada del aprendizaje por refuerzo."
    },
    {
      "id": "p-tuning-2021",
      "title": "P‑Tuning v2: Prompt Tuning Can Be Comparable to Fine‑Tuning Universally Across Scales and Tasks",
      "year": 2021,
      "venue": "ACL",
      "paper_url": "https://arxiv.org/abs/2110.07602",
      "code": ["https://github.com/OpenNLP/P-tuning-v2"],
      "why_transition": "提出参数高效的前缀/提示调节方法，为指令学习提供轻量替代。",
      "summary_zh": "全参数微调大型语言模型成本高且易过拟合，需要更高效的方法。P‑Tuning v2 的目标是通过学习可插拔的连续提示向量或前缀，实现与全面微调相近的效果。作者将可训练的连续提示嵌入到每一层 Transformer 中，冻结原模型参数，仅训练少量新参数，并对提示进行层间共享。实验表明，该方法在各类自然语言任务和不同模型规模上都取得与全参微调相当或更好的性能，但训练参数量显著减少。该工作展示了提示调节作为指令学习轻量替代的潜力，为后续 LoRA 等方法提供启示。",
      "summary_en": "Full‑parameter fine‑tuning of large language models is costly and prone to overfitting, creating a need for more efficient methods. P‑Tuning v2 aimed to achieve performance comparable to full fine‑tuning by learning pluggable continuous prompt vectors or prefixes. The authors inserted trainable continuous prompts into each Transformer layer, froze the original model parameters and trained only a small number of new parameters while sharing prompts across layers. Experiments showed that the method achieved performance on par with or better than full fine‑tuning on various NLP tasks and across different model sizes while significantly reducing the number of trainable parameters. This work demonstrated the potential of prompt tuning as a lightweight alternative for instruction learning and inspired subsequent methods such as LoRA.",
      "summary_es": "El afinamiento de todos los parámetros de los grandes modelos lingüísticos es costoso y propenso al sobreajuste, por lo que se necesitan métodos más eficientes. P‑Tuning v2 se propuso lograr un rendimiento comparable al afinamiento completo aprendiendo vectores de aviso continuos o prefijos enchufables. Los autores insertaron avisos continuos entrenables en cada capa Transformer, congelaron los parámetros originales del modelo y entrenaron solo un pequeño número de nuevos parámetros, compartiendo avisos entre capas. Los experimentos demostraron que el método alcanzaba un rendimiento equiparable o mejor que el afinamiento completo en diversas tareas de PLN y en distintos tamaños de modelo, a la vez que reducía significativamente el número de parámetros entrenables. Este trabajo demostró el potencial de la afinación de avisos como alternativa ligera para el aprendizaje de instrucciones e inspiró métodos posteriores como LoRA."
    }
  ],
  "meta": {
    "citations": {
      "earliest": "【90262287243983†L68-L85】",
      "milestone": "【90262287243983†L68-L85】",
      "frontier": "【90262287243983†L68-L85】",
      "survey": "【90262287243983†L68-L85】"
    }
  }
}