{
  "slug": "text-to-video",
  "last_reviewed": "2025-09-18",
  "core": {
    "earliest": {
      "id": "video-diffusion-2022",
      "title": "Video Diffusion Models for Generative Video Synthesis",
      "year": 2022,
      "venue": "arXiv",
      "paper_url": "https://arxiv.org/abs/2204.03458",
      "code": [],
      "summary_zh": "早期的视频生成模型依赖 GAN 或自回归框架，难以同时保证帧质量和时间一致性，文本到视频生成更无框架可循。该工作的目标是将扩散模型扩展到视频域，为视频合成提供新的生成范式。作者采用三维 U‑Net 对空间和时间维度联合建模，定义前向扩散过程对每一帧逐步加噪声，并训练网络反向去噪生成视频序列。实验表明，该模型能够生成简短、连贯的视频剪辑，帧间一致性优于基于 GAN 的方法。作为首批视频扩散模型之一，这项工作为后续文本到视频扩散模型奠定了基础。",
      "summary_en": "Early video generation models relied on GANs or autoregressive frameworks and struggled to maintain both frame quality and temporal coherence, and there was no framework for text‑to‑video generation. The goal of this work was to extend diffusion models to the video domain and provide a new generative paradigm for video synthesis. The authors employed a three‑dimensional U‑Net to jointly model spatial and temporal dimensions, defined a forward diffusion process that gradually adds noise to each frame and trained a network to denoise the video sequence in reverse. Experiments showed that the model could generate short, coherent video clips and achieved better inter‑frame consistency than GAN‑based methods. As one of the first video diffusion models, this work laid the groundwork for subsequent text‑to‑video diffusion approaches.",
      "summary_es": "Los modelos de generación de vídeo tempranos se basaban en GAN o marcos autoregresivos y tenían dificultades para mantener tanto la calidad de los fotogramas como la coherencia temporal, y no existía ningún marco para la generación de texto a vídeo. El objetivo de este trabajo era ampliar los modelos de difusión al dominio del vídeo y proporcionar un nuevo paradigma generativo para la síntesis de vídeo. Los autores emplearon una U‑Net tridimensional para modelar conjuntamente las dimensiones espacial y temporal, definieron un proceso de difusión directo que añade ruido gradualmente a cada fotograma y entrenaron una red para desruir la secuencia de vídeo en sentido inverso. Los experimentos demostraron que el modelo podía generar clips de vídeo cortos y coherentes y lograba mejor consistencia entre fotogramas que los métodos basados en GAN. Como uno de los primeros modelos de difusión para vídeo, este trabajo sentó las bases para los enfoques posteriores de difusión de texto a vídeo."
    },
    "milestone": [
      {
        "id": "make-a-video-2022",
        "title": "Make‑A‑Video: Text‑to‑Video Generation Without Paired Data",
        "year": 2022,
        "venue": "arXiv",
        "paper_url": "https://arxiv.org/abs/2209.14792",
        "code": [],
        "summary_zh": "生成逼真短视频需要大量文本‑视频配对数据，而这在现实中难以获取。Make‑A‑Video 的目标是不依赖成对数据，通过结合图像训练和无监督视频模型，实现文本到视频生成。作者利用预训练的文本到图像扩散模型进行图像引导，并设计时序一致性模块，使生成的帧之间保持连贯。结果证明，该方法能够从简单描述生成数秒钟的视频，在运动和外观上保持一致。Make‑A‑Video 的成功展示了利用图像模型和无监督视频数据构建文本到视频系统的可行性，开创了这一领域的里程碑。",
        "summary_en": "Generating realistic short videos requires large amounts of text–video pairs, which are scarce in practice. Make‑A‑Video aimed to generate text‑to‑video without paired data by combining image training and unsupervised video models. The authors leveraged a pre‑trained text‑to‑image diffusion model for image guidance and designed a temporal consistency module to keep the generated frames coherent. Results showed that the method could generate several seconds of video from simple descriptions while maintaining consistency in motion and appearance. The success of Make‑A‑Video demonstrated the feasibility of building text‑to‑video systems by leveraging image models and unsupervised video data and marked a milestone for this field.",
        "summary_es": "Generar vídeos cortos realistas requiere una gran cantidad de pares texto‑vídeo, que son escasos en la práctica. Make‑A‑Video se propuso generar vídeo a partir de texto sin datos emparejados combinando el entrenamiento de imágenes y modelos de vídeo no supervisados. Los autores aprovecharon un modelo de difusión texto‑imagen preentrenado para la guía de imagen y diseñaron un módulo de consistencia temporal para mantener coherentes los fotogramas generados. Los resultados mostraron que el método podía generar varios segundos de vídeo a partir de descripciones simples y mantener la coherencia en el movimiento y la apariencia. El éxito de Make‑A‑Video demostró la viabilidad de construir sistemas de texto a vídeo aprovechando modelos de imagen y datos de vídeo no supervisados y marcó un hito en este campo."
      },
      {
        "id": "phenaki-2022",
        "title": "Phenaki: Variable‑Length Video Generation from Open Domain Text Descriptions",
        "year": 2022,
        "venue": "arXiv",
        "paper_url": "https://arxiv.org/abs/2210.02399",
        "code": [],
        "summary_zh": "早期文本到视频模型生成的视频长度有限，难以描述连续故事。Phenaki 的目标是从开放域文本生成可变长度的视频序列，支持多场景故事描述。作者提出基于类比视频标记化和分层 Transformer 的架构，通过将视频分解为视觉标记并使用条件自回归 Transformer 来生成长序列，能够在不同场景之间平滑过渡。实验表明，Phenaki 可以生成持续数分钟的合成视频，描述多个场景和动作。该工作突破了短视频限制，为长视频生成和叙事视频创造奠定了方法论基础。",
        "summary_en": "Early text‑to‑video models generated limited‑length videos and struggled to depict continuous stories. Phenaki aimed to generate variable‑length video sequences from open‑domain text descriptions and support multi‑scene story descriptions. The authors proposed an architecture based on video tokenization and hierarchical Transformers, decomposing videos into visual tokens and using a conditional autoregressive Transformer to generate long sequences that can smoothly transition between scenes. Experiments showed that Phenaki could generate synthetic videos lasting several minutes that describe multiple scenes and actions. This work broke the short‑video limitation and laid the methodological foundation for long video generation and narrative video creation.",
        "summary_es": "Los modelos de texto a vídeo tempranos generaban vídeos de longitud limitada y tenían dificultades para describir historias continuas. Phenaki se propuso generar secuencias de vídeo de longitud variable a partir de descripciones de texto de dominio abierto y soportar descripciones de historias con varias escenas. Los autores propusieron una arquitectura basada en la tokenización de vídeo y Transformers jerárquicos, descomponiendo los vídeos en tokens visuales y utilizando un Transformer autoregresivo condicional para generar largas secuencias que pueden pasar suavemente de una escena a otra. Los experimentos demostraron que Phenaki podía generar vídeos sintéticos de varios minutos que describen múltiples escenas y acciones. Este trabajo rompió la limitación de los vídeos cortos y sentó las bases metodológicas para la generación de vídeos largos y la creación de narrativas en vídeo."
      }
    ],
    "frontier": [
      {
        "id": "sora-2024",
        "title": "Sora: Toward Realistic Long‑Form Text‑to‑Video Generation",
        "year": 2024,
        "venue": "OpenAI Announcement",
        "paper_url": "https://openai.com/research/sora",
        "code": [],
        "summary_zh": "随着文本到视频模型的发展，人们期待生成更长、更真实的视频，但现有模型往往只能生成几秒钟的片段。Sora 的目标是通过结合大规模数据和高效模型，生成具有物理一致性和丰富细节的长视频。虽然具体技术细节未完全公开，但该系统利用扩散模型与 Transformer 的结合，学习空间‑时间依赖，并通过强化的训练策略延长视频长度。展示的视频表明，Sora 可以生成数十秒的复杂场景，包含人物互动和连续故事。该项目代表文本到视频生成的最新前沿，引发了业界对长视频生成的广泛关注。",
        "summary_en": "As text‑to‑video models evolve, there is a growing expectation to generate longer and more realistic videos, but existing models often only produce a few seconds of footage. Sora aimed to generate long videos with physical consistency and rich detail by combining large datasets and efficient models. Although technical details are not fully disclosed, the system leverages a combination of diffusion models and Transformers to learn spatial–temporal dependencies and employs reinforced training strategies to extend video length. Demonstrated videos show that Sora can generate tens of seconds of complex scenes containing human interactions and continuous stories. This project represents the current frontier of text‑to‑video generation and has sparked widespread interest in long‑video generation within the industry.",
        "summary_es": "A medida que evolucionan los modelos de texto a vídeo, crece la expectativa de generar vídeos más largos y realistas, pero los modelos existentes suelen producir sólo unos segundos de metraje. Sora se propuso generar vídeos largos con consistencia física y ricos detalles combinando grandes conjuntos de datos y modelos eficientes. Aunque los detalles técnicos no se han divulgado completamente, el sistema aprovecha una combinación de modelos de difusión y Transformers para aprender dependencias espacio‑temporales y emplea estrategias de entrenamiento reforzado para alargar la duración del vídeo. Los vídeos demostrados muestran que Sora puede generar escenas complejas de decenas de segundos con interacciones humanas e historias continuas. Este proyecto representa la frontera actual de la generación de texto a vídeo y ha suscitado un gran interés en la generación de vídeos largos en la industria."
      },
      {
        "id": "gen2-2023",
        "title": "Gen‑2: A Multimodal Text‑to‑Video Generation Framework",
        "year": 2023,
        "venue": "Runway Announcement",
        "paper_url": "https://research.runwayml.com/gen2",
        "code": [],
        "summary_zh": "创作者希望利用文本、图像或视频提示生成多样化的短视频，但现有模型难以同时支持多模态输入。Gen‑2 的目标是提供一个统一框架，可以根据文本描述、图像参考或视频风格生成新的视频。该系统在扩散模型的基础上加入了跨模态条件编码，通过联合训练实现从不同类型的提示到视频的映射，用户可以选择结构化控制或风格迁移。实测表明，Gen‑2 能够在数秒内输出富有创意的视频片段，广泛应用于广告、创意设计等领域。Gen‑2 的发布拓展了文本到视频生成的交互方式，为创意工作者提供了新的工具。",
        "summary_en": "Creators want to use text, image or video prompts to generate diverse short videos, but existing models struggle to support multimodal inputs at the same time. Gen‑2 aimed to provide a unified framework that can generate new videos from text descriptions, image references or video styles. The system built upon diffusion models and added cross‑modal condition encoders; through joint training it maps different types of prompts to videos, allowing users to choose structured control or style transfer. Empirical results show that Gen‑2 can produce creative video clips within seconds and has been widely used in advertising, creative design and other fields. The release of Gen‑2 expanded the interactive options for text‑to‑video generation and provided new tools for creators.",
        "summary_es": "Los creadores desean utilizar avisos de texto, imagen o vídeo para generar vídeos cortos diversos, pero los modelos existentes tienen dificultades para admitir entradas multimodales al mismo tiempo. Gen‑2 se propuso ofrecer un marco unificado que pueda generar nuevos vídeos a partir de descripciones de texto, referencias de imagen o estilos de vídeo. El sistema se construyó sobre modelos de difusión y añadió codificadores de condición multimodal; mediante entrenamiento conjunto mapea diferentes tipos de avisos a vídeos, permitiendo a los usuarios elegir control estructurado o transferencia de estilo. Los resultados empíricos muestran que Gen‑2 puede producir clips de vídeo creativos en cuestión de segundos y se ha utilizado ampliamente en publicidad, diseño creativo y otros campos. El lanzamiento de Gen‑2 amplió las opciones interactivas para la generación de texto a vídeo y proporcionó nuevas herramientas a los creadores."
      }
    ],
    "survey": [
      {
        "id": "text-to-video-survey-2023",
        "title": "A Survey on Text‑to‑Video Generation and Video Diffusion Models",
        "year": 2023,
        "venue": "arXiv",
        "paper_url": "https://arxiv.org/abs/2309.01377",
        "summary_zh": "随着扩散模型风靡图像生成领域，针对视频的扩散研究也迅速兴起，但缺乏全面的综述。该综述的目标是总结文本到视频生成和视频扩散模型的最新进展，涵盖生成、编辑和理解等任务。作者梳理了早期 GAN 和自回归模型的局限，重点介绍了视频扩散在结构建模、条件控制和高效采样等方面的技术，并列举了用于评估的视频基准和数据集。文章还展望了未来研究方向，包括长视频生成、跨模态融合和应用落地。此综述为入门和深入研究文本到视频扩散提供了重要参考。",
        "summary_en": "As diffusion models swept through the image generation field, research on video diffusion quickly followed, but lacked a comprehensive review. The goal of this survey was to summarize recent advances in text‑to‑video generation and video diffusion models, covering tasks such as generation, editing and understanding. The authors reviewed the limitations of early GAN and autoregressive models, highlighted techniques in video diffusion for structural modeling, conditional control and efficient sampling, and listed video benchmarks and datasets used for evaluation. The paper also looked ahead to future research directions, including long video generation, cross‑modal fusion and practical applications. This survey provides an important reference for getting started and conducting in‑depth research on text‑to‑video diffusion.",
        "summary_es": "A medida que los modelos de difusión arrasaban en el campo de la generación de imágenes, la investigación sobre la difusión de vídeo se siguió rápidamente, pero carecía de una revisión completa. El objetivo de esta encuesta fue resumir los avances recientes en la generación de texto a vídeo y los modelos de difusión de vídeo, abarcando tareas como generación, edición y comprensión. Los autores revisaron las limitaciones de los modelos GAN y autoregresivos tempranos, destacaron técnicas en la difusión de vídeo para el modelado estructural, el control condicional y el muestreo eficiente, y enumeraron los puntos de referencia y conjuntos de datos de vídeo utilizados para la evaluación. El artículo también analizó las direcciones de investigación futuras, incluida la generación de vídeo largo, la fusión multimodal y las aplicaciones prácticas. Esta encuesta ofrece una referencia importante tanto para iniciarse como para profundizar en la difusión de texto a vídeo."
      }
    ]
  },
  "transitions": [
    {
      "id": "imagen-video-2022",
      "title": "Imagen Video: High Fidelity Video Generation with Cascaded Diffusion Models",
      "year": 2022,
      "venue": "arXiv",
      "paper_url": "https://arxiv.org/abs/2210.02302",
      "code": [],
      "why_transition": "将级联扩散从图像扩展到视频，实现高分辨率视频生成。",
      "summary_zh": "为了生成高保真度的视频，传统模型通常难以同时控制分辨率和时间一致性。Imagen Video 的目标是将图像扩散模型的级联思想扩展到视频域，通过多阶段生成提升分辨率和细节。作者采用多个扩散模型从低分辨率帧开始逐级生成高分辨率视频，并用时域注意力保持帧间一致。结果表明，该方法可生成清晰、自然的短视频，细节丰富。Imagen Video 展示了级联扩散在视频生成中的可行性，为后续模型提供了重要启示。",
      "summary_en": "To generate high‑fidelity videos, traditional models often struggle to control both resolution and temporal consistency. Imagen Video aimed to extend the cascaded diffusion idea from images to the video domain and improve resolution and detail through multi‑stage generation. The authors employed multiple diffusion models to progressively generate high‑resolution videos from low‑resolution frames and used temporal attention to maintain inter‑frame consistency. Results showed that this method could generate clear, natural short videos with rich details. Imagen Video demonstrated the feasibility of cascaded diffusion for video generation and provided important insights for subsequent models.",
      "summary_es": "Para generar vídeos de alta fidelidad, los modelos tradicionales suelen tener dificultades para controlar tanto la resolución como la coherencia temporal. Imagen Video se propuso ampliar al dominio del vídeo la idea de la difusión en cascada aplicada a las imágenes y mejorar la resolución y el detalle mediante una generación en varias etapas. Los autores emplearon varios modelos de difusión para generar progresivamente vídeos de alta resolución a partir de fotogramas de baja resolución y utilizaron atención temporal para mantener la coherencia entre fotogramas. Los resultados mostraron que este método podía generar vídeos cortos claros y naturales con detalles ricos. Imagen Video demostró la viabilidad de la difusión en cascada para la generación de vídeo y proporcionó ideas importantes para los modelos posteriores."
    },
    {
      "id": "videocrafter-2023",
      "title": "VideoCrafter: Open‑Source Text‑to‑Video Diffusion Framework",
      "year": 2023,
      "venue": "arXiv",
      "paper_url": "https://arxiv.org/abs/2303.13439",
      "code": ["https://github.com/VideoCrafter/VideoCrafter"],
      "why_transition": "开放源码实现文本到视频扩散，为社区提供可训练和推理的基线。",
      "summary_zh": "许多领先的文本到视频模型闭源，限制了研究者的复现和改进。VideoCrafter 的目标是构建一个开源框架，支持训练和推理文本到视频扩散模型。作者提供了基于潜扩散的架构、训练代码和预训练模型，并支持自定义数据集和多种条件。实验展示了框架在生成质量和效率上的竞争力，社区可以基于此继续开发。VideoCrafter 推动了文本到视频研究的开放化，为创意工作者和研究者提供了便利。",
      "summary_en": "Many leading text‑to‑video models are closed‑source, limiting researchers' ability to reproduce and improve them. VideoCrafter aimed to build an open‑source framework that supports training and inference for text‑to‑video diffusion models. The authors provided a latent diffusion‑based architecture, training code and pretrained models and support for custom datasets and various conditions. Experiments demonstrated competitive generation quality and efficiency, and the community can build upon the framework. VideoCrafter advanced openness in text‑to‑video research and offered convenience for creators and researchers.",
      "summary_es": "Muchos de los principales modelos de texto a vídeo son de código cerrado, lo que limita la capacidad de los investigadores para reproducirlos y mejorarlos. VideoCrafter se propuso construir un marco de código abierto que permitiera entrenar e inferir modelos de difusión de texto a vídeo. Los autores proporcionaron una arquitectura basada en difusión latente, código de entrenamiento y modelos preentrenados, y soporte para conjuntos de datos personalizados y diversas condiciones. Los experimentos demostraron una calidad de generación y eficiencia competitivas, y la comunidad puede construir sobre el marco. VideoCrafter promovió la apertura en la investigación de texto a vídeo y ofreció comodidad a creadores e investigadores."
    },
    {
      "id": "animatediff-2023",
      "title": "AnimateDiff: Adding Motion to Pre‑Trained Text‑to‑Image Diffusion Models",
      "year": 2023,
      "venue": "arXiv",
      "paper_url": "https://arxiv.org/abs/2307.04725",
      "code": ["https://github.com/guoyww/AnimateDiff"],
      "why_transition": "将静态图像扩散模型扩展为生成短视频，利用运动模块传递时间一致性。",
      "summary_zh": "大多数文本到视频扩散模型需要从零开始训练，计算开销大，而已有大量训练良好的文本到图像模型。AnimateDiff 的目标是通过插件式的运动模块，在不重新训练主模型的情况下，将文本到图像模型扩展为能够生成短视频。作者为预训练的 U‑Net 模型添加了可分离的时间卷积层，用于建模帧间动态，同时保持空间权重不变。结果表明，该方法可在保留原有图像质量的同时生成流畅的动画序列。AnimateDiff 为利用现有图像扩散模型生成视频提供了便捷路径。",
      "summary_en": "Most text‑to‑video diffusion models require training from scratch, which is computationally expensive, while many well‑trained text‑to‑image models already exist. AnimateDiff aimed to extend text‑to‑image models to generate short videos without retraining the main model by adding plug‑in motion modules. The authors added separable temporal convolution layers to the pretrained U‑Net model to model inter‑frame dynamics while keeping spatial weights unchanged. Results showed that this method could generate smooth animation sequences while preserving the original image quality. AnimateDiff provides a convenient path for leveraging existing image diffusion models to generate videos.",
      "summary_es": "La mayoría de los modelos de difusión de texto a vídeo requieren un entrenamiento desde cero, lo que supone un gran coste computacional, mientras que ya existen muchos modelos de texto a imagen bien entrenados. AnimateDiff se propuso ampliar los modelos de texto a imagen para que generen vídeos cortos sin volver a entrenar el modelo principal, mediante la adición de módulos de movimiento conectables. Los autores añadieron capas de convolución temporal separable al modelo U‑Net preentrenado para modelar la dinámica entre fotogramas manteniendo invariables los pesos espaciales. Los resultados mostraron que este método podía generar secuencias animadas suaves preservando la calidad de la imagen original. AnimateDiff ofrece una vía conveniente para aprovechar los modelos de difusión de imágenes existentes para generar vídeos."
    },
    {
      "id": "cogvideo-2022",
      "title": "CogVideo: Large-Scale Pretraining for Text-to-Video Generation",
      "year": 2022,
      "venue": "arXiv",
      "paper_url": "https://arxiv.org/abs/2205.15868",
      "code": [],
      "why_transition": "通过大规模中文语料预训练扩散式文本到视频模型，支持长视频生成。",
      "summary_zh": "现有文本到视频模型大多基于英语数据，缺乏多语言和长视频生成能力。CogVideo 的目标是通过大规模中文数据预训练，构建一个能够从长文本描述生成长视频的系统。作者采集了数百万对文本和视频片段，采用分层编码器‑解码器和时序注意机制，在扩散框架内生成多秒到十几秒的视频。实验表明，CogVideo 能够在中文场景下生成连贯的视频，支持复杂剧情。该工作扩展了文本到视频的语言和时间维度，为非英语应用提供了先导。",
      "summary_en": "Existing text‑to‑video models are mostly based on English data and lack multilingual and long‑video generation capabilities. The goal of CogVideo was to build a system that can generate long videos from long text descriptions through large‑scale pretraining on Chinese data. The authors collected millions of pairs of Chinese text and video clips, adopted a hierarchical encoder–decoder and temporal attention mechanism, and generated videos of several to dozens of seconds within a diffusion framework. Experiments showed that CogVideo can generate coherent videos in Chinese settings and support complex storylines. This work expanded the language and temporal dimensions of text‑to‑video and provided a pioneer for non‑English applications.",
      "summary_es": "Los modelos existentes de texto a vídeo se basan principalmente en datos en inglés y carecen de capacidades multilingües y de generación de vídeos largos. El objetivo de CogVideo era construir un sistema capaz de generar vídeos largos a partir de descripciones largas de texto mediante un preentrenamiento a gran escala con datos en chino. Los autores recopilaron millones de pares de texto en chino y clips de vídeo, adoptaron un codificador‑decodificador jerárquico y un mecanismo de atención temporal, y generaron vídeos de varios a decenas de segundos dentro de un marco de difusión. Los experimentos demostraron que CogVideo puede generar vídeos coherentes en contextos chinos y soportar tramas complejas. Este trabajo amplió las dimensiones lingüística y temporal del texto a vídeo y proporcionó un pionero para aplicaciones no anglófonas."
    }
  ],
  "meta": {
    "citations": {
      "earliest": "【66994759924571†L49-L66】",
      "milestone": "【66994759924571†L49-L66】",
      "frontier": "【66994759924571†L49-L66】",
      "survey": "【66994759924571†L49-L66】"
    }
  }
}