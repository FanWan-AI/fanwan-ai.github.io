{
  "version": 1,
  "updated_at": "2025-09-27T15:21:58.811Z",
  "by_category": {
    "image_classification": [
      {
        "id": "timm/mobilenetv3_small_100.lamb_in1k",
        "source": "hf",
        "name": "mobilenetv3_small_100.lamb_in1k",
        "url": "https://huggingface.co/timm/mobilenetv3_small_100.lamb_in1k",
        "tags": [
          "timm",
          "pytorch",
          "safetensors",
          "image-classification",
          "transformers",
          "dataset:imagenet-1k",
          "arxiv:2110.00476",
          "arxiv:1905.02244",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 119940003,
          "likes_total": 38,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "MobileNetV3-Small-100是一种专为移动和边缘设备优化的轻量级卷积神经网络。基于MobileNetV3架构，采用深度可分离卷积和压缩激励模块提高效率。此特定变体使用LAMB优化器在ImageNet-1k数据集上训练。核心能力包括图像分类，具有极低计算需求。主要优势在于高速推理、小内存占用和能效高，同时保持合理精度。典型应用场景包括移动视觉应用、嵌入式系统和实时图像识别，特别适用于计算资源受限的环境。该模型通过神经网络架构搜索技术优化，平衡了速度与准确性的权衡。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification"
        ],
        "summary_en": "MobileNetV3-Small-100 is a lightweight convolutional neural network optimized for mobile and edge devices. Based on the MobileNetV3 architecture, it uses depthwise separable convolutions and squeeze-and-excitation modules for efficiency. This specific variant employs LAMB optimizer training on ImageNet-1k. Core capabilities include image classification with minimal computational requirements. Strengths are high speed, low memory footprint, and energy efficiency while maintaining reasonable accuracy. Typical use cases include mobile vision applications, embedded systems, and real-time image recognition where computational resources are constrained.",
        "summary_zh": "MobileNetV3-Small-100是一种专为移动和边缘设备优化的轻量级卷积神经网络。基于MobileNetV3架构，采用深度可分离卷积和压缩激励模块提高效率。此特定变体使用LAMB优化器在ImageNet-1k数据集上训练。核心能力包括图像分类，具有极低计算需求。主要优势在于高速推理、小内存占用和能效高，同时保持合理精度。典型应用场景包括移动视觉应用、嵌入式系统和实时图像识别，特别适用于计算资源受限的环境。该模型通过神经网络架构搜索技术优化，平衡了速度与准确性的权衡。",
        "summary_es": "MobileNetV3-Small-100 is a lightweight convolutional neural network optimized for mobile and edge devices. Based on the MobileNetV3 architecture, it uses depthwise separable convolutions and squeeze-and-excitation modules for efficiency. This specific variant employs LAMB optimizer training on ImageNet-1k. Core capabilities include image classification with minimal computational requirements. Strengths are high speed, low memory footprint, and energy efficiency while maintaining reasonable accuracy. Typical use cases include mobile vision applications, embedded systems, and real-time image recognition where computational resources are constrained."
      },
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 99881324,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems.",
        "summary_zh": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems."
      }
    ],
    "object_detection": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 99881324,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems.",
        "summary_zh": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      }
    ],
    "semantic_segmentation": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      },
      {
        "id": "Bingsu/adetailer",
        "source": "hf",
        "name": "adetailer",
        "url": "https://huggingface.co/Bingsu/adetailer",
        "tags": [
          "ultralytics",
          "pytorch",
          "dataset:wider_face",
          "dataset:skytnt/anime-segmentation",
          "doi:10.57967/hf/3633",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15085828,
          "likes_total": 617,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括对象检测、分割和修复，以提升图像质量。优势在于处理动漫风格内容和真实世界面部图像，利用了 anime-segmentation 和 WiderFace 等数据集。典型应用场景涉及对生成或真实图像进行后处理，以优化细节、去除伪影或增强特定区域，无需人工干预。该工具基于 PyTorch 构建，采用 Apache 2.0 开源许可证。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "ltr",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection, segmentation, and inpainting to improve image quality. Strengths lie in handling anime-style content and real-world facial images, leveraging datasets like anime-segmentation and WiderFace. Typical use cases involve post-processing generated or real images to refine details, remove artifacts, or enhance specific areas without manual intervention. The tool is built on PyTorch and is open-source under Apache 2.0 license.",
        "summary_zh": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括对象检测、分割和修复，以提升图像质量。优势在于处理动漫风格内容和真实世界面部图像，利用了 anime-segmentation 和 WiderFace 等数据集。典型应用场景涉及对生成或真实图像进行后处理，以优化细节、去除伪影或增强特定区域，无需人工干预。该工具基于 PyTorch 构建，采用 Apache 2.0 开源许可证。",
        "summary_es": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection, segmentation, and inpainting to improve image quality. Strengths lie in handling anime-style content and real-world facial images, leveraging datasets like anime-segmentation and WiderFace. Typical use cases involve post-processing generated or real images to refine details, remove artifacts, or enhance specific areas without manual intervention. The tool is built on PyTorch and is open-source under Apache 2.0 license."
      }
    ],
    "instance_segmentation": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      },
      {
        "id": "Bingsu/adetailer",
        "source": "hf",
        "name": "adetailer",
        "url": "https://huggingface.co/Bingsu/adetailer",
        "tags": [
          "ultralytics",
          "pytorch",
          "dataset:wider_face",
          "dataset:skytnt/anime-segmentation",
          "doi:10.57967/hf/3633",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15085828,
          "likes_total": 617,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括对象检测、分割和修复，以提升图像质量。优势在于处理动漫风格内容和真实世界面部图像，利用了 anime-segmentation 和 WiderFace 等数据集。典型应用场景涉及对生成或真实图像进行后处理，以优化细节、去除伪影或增强特定区域，无需人工干预。该工具基于 PyTorch 构建，采用 Apache 2.0 开源许可证。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "ltr",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection, segmentation, and inpainting to improve image quality. Strengths lie in handling anime-style content and real-world facial images, leveraging datasets like anime-segmentation and WiderFace. Typical use cases involve post-processing generated or real images to refine details, remove artifacts, or enhance specific areas without manual intervention. The tool is built on PyTorch and is open-source under Apache 2.0 license.",
        "summary_zh": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括对象检测、分割和修复，以提升图像质量。优势在于处理动漫风格内容和真实世界面部图像，利用了 anime-segmentation 和 WiderFace 等数据集。典型应用场景涉及对生成或真实图像进行后处理，以优化细节、去除伪影或增强特定区域，无需人工干预。该工具基于 PyTorch 构建，采用 Apache 2.0 开源许可证。",
        "summary_es": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection, segmentation, and inpainting to improve image quality. Strengths lie in handling anime-style content and real-world facial images, leveraging datasets like anime-segmentation and WiderFace. Typical use cases involve post-processing generated or real images to refine details, remove artifacts, or enhance specific areas without manual intervention. The tool is built on PyTorch and is open-source under Apache 2.0 license."
      }
    ],
    "panoptic_segmentation": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      },
      {
        "id": "Bingsu/adetailer",
        "source": "hf",
        "name": "adetailer",
        "url": "https://huggingface.co/Bingsu/adetailer",
        "tags": [
          "ultralytics",
          "pytorch",
          "dataset:wider_face",
          "dataset:skytnt/anime-segmentation",
          "doi:10.57967/hf/3633",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15085828,
          "likes_total": 617,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括对象检测、分割和修复，以提升图像质量。优势在于处理动漫风格内容和真实世界面部图像，利用了 anime-segmentation 和 WiderFace 等数据集。典型应用场景涉及对生成或真实图像进行后处理，以优化细节、去除伪影或增强特定区域，无需人工干预。该工具基于 PyTorch 构建，采用 Apache 2.0 开源许可证。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "ltr",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection, segmentation, and inpainting to improve image quality. Strengths lie in handling anime-style content and real-world facial images, leveraging datasets like anime-segmentation and WiderFace. Typical use cases involve post-processing generated or real images to refine details, remove artifacts, or enhance specific areas without manual intervention. The tool is built on PyTorch and is open-source under Apache 2.0 license.",
        "summary_zh": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括对象检测、分割和修复，以提升图像质量。优势在于处理动漫风格内容和真实世界面部图像，利用了 anime-segmentation 和 WiderFace 等数据集。典型应用场景涉及对生成或真实图像进行后处理，以优化细节、去除伪影或增强特定区域，无需人工干预。该工具基于 PyTorch 构建，采用 Apache 2.0 开源许可证。",
        "summary_es": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection, segmentation, and inpainting to improve image quality. Strengths lie in handling anime-style content and real-world facial images, leveraging datasets like anime-segmentation and WiderFace. Typical use cases involve post-processing generated or real images to refine details, remove artifacts, or enhance specific areas without manual intervention. The tool is built on PyTorch and is open-source under Apache 2.0 license."
      }
    ],
    "text_to_image": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 99881324,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems.",
        "summary_zh": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      }
    ],
    "text_to_video": [
      {
        "id": "openai-community/gpt2",
        "source": "hf",
        "name": "gpt2",
        "url": "https://huggingface.co/openai-community/gpt2",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "tflite",
          "rust",
          "onnx",
          "safetensors",
          "gpt2",
          "text-generation",
          "exbert",
          "en",
          "doi:10.57967/hf/0039",
          "license:mit",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 11902438,
          "likes_total": 2955,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本生成任务。其核心能力是通过输入提示预测后续文本内容，实现连贯的文本延续。主要优势包括在零样本设置下无需特定任务训练即可生成高质量文本、能够跨多个领域生成类人文本内容以及开源可访问性。典型应用场景涵盖创意写作辅助、对话系统原型开发、内容摘要生成和代码自动补全。该模型在开放式文本补全方面表现突出，能够在长文本输出中保持上下文相关性，同时支",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "GPT-2 is a transformer-based language model developed by OpenAI for text generation tasks. Its core capability involves predicting subsequent text based on input prompts, enabling coherent continuation of text. Key strengths include strong performance in zero-shot settings without task-specific training, generating human-like text across diverse domains, and open-source availability. Typical use cases encompass creative writing assistance, conversational AI prototyping, content summarization, and code generation. The model demonstrates particular effectiveness in open-ended text completion while maintaining contextual relevance throughout extended outputs.",
        "summary_zh": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本生成任务。其核心能力是通过输入提示预测后续文本内容，实现连贯的文本延续。主要优势包括在零样本设置下无需特定任务训练即可生成高质量文本、能够跨多个领域生成类人文本内容以及开源可访问性。典型应用场景涵盖创意写作辅助、对话系统原型开发、内容摘要生成和代码自动补全。该模型在开放式文本补全方面表现突出，能够在长文本输出中保持上下文相关性，同时支",
        "summary_es": "GPT-2 is a transformer-based language model developed by OpenAI for text generation tasks. Its core capability involves predicting subsequent text based on input prompts, enabling coherent continuation of text. Key strengths include strong performance in zero-shot settings without task-specific training, generating human-like text across diverse domains, and open-source availability. Typical use cases encompass creative writing assistance, conversational AI prototyping, content summarization, and code generation. The model demonstrates particular effectiveness in open-ended text completion while maintaining contextual relevance throughout extended outputs."
      },
      {
        "id": "omni-research/Tarsier2-Recap-7b",
        "source": "hf",
        "name": "Tarsier2-Recap-7b",
        "url": "https://huggingface.co/omni-research/Tarsier2-Recap-7b",
        "tags": [
          "safetensors",
          "video LLM",
          "arxiv:2501.07888",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 10283648,
          "likes_total": 19,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Tarsier2-Recap-7b 是一个拥有70亿参数的专业视频语言模型，专门用于深度视频理解与内容概括。其核心能力在于处理视频流数据并生成全面的文本描述和摘要。该模型擅长从视觉序列中提取关键信息，并将其转化为连贯的叙述文本。典型应用场景包括自动化视频字幕生成、媒体资料库的内容分析、以及教育或纪录片素材的摘要制作。基于arXiv:2501.07888的研究成果，采用safetensors格式和Apache 2.0开源协议，在跨模态理解任务中表现出色，特别在保持内容准确性和结构完整性方面具有",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_video",
          "llm_pretraining"
        ],
        "summary_en": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for comprehensive video understanding and summarization. Its core capability involves processing video content to generate detailed textual descriptions and summaries. The model excels at extracting key information from visual sequences and converting it into coherent narratives. Typical use cases include automated video captioning, content analysis for media archives, and generating summaries for educational or documentary footage. Based on research from arXiv:2501.07888, it employs safetensors format and operates under Apache 2.0 license, demonstrating strong performance in multimodal understanding tasks.",
        "summary_zh": "Tarsier2-Recap-7b 是一个拥有70亿参数的专业视频语言模型，专门用于深度视频理解与内容概括。其核心能力在于处理视频流数据并生成全面的文本描述和摘要。该模型擅长从视觉序列中提取关键信息，并将其转化为连贯的叙述文本。典型应用场景包括自动化视频字幕生成、媒体资料库的内容分析、以及教育或纪录片素材的摘要制作。基于arXiv:2501.07888的研究成果，采用safetensors格式和Apache 2.0开源协议，在跨模态理解任务中表现出色，特别在保持内容准确性和结构完整性方面具有",
        "summary_es": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for comprehensive video understanding and summarization. Its core capability involves processing video content to generate detailed textual descriptions and summaries. The model excels at extracting key information from visual sequences and converting it into coherent narratives. Typical use cases include automated video captioning, content analysis for media archives, and generating summaries for educational or documentary footage. Based on research from arXiv:2501.07888, it employs safetensors format and operates under Apache 2.0 license, demonstrating strong performance in multimodal understanding tasks."
      }
    ],
    "3d_reconstruction": [],
    "nerf": [],
    "super_resolution": [],
    "denoising": [],
    "restoration": [],
    "medical_image_processing": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 99881324,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems.",
        "summary_zh": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      }
    ],
    "remote_sensing_image_processing": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 99881324,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems.",
        "summary_zh": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      }
    ],
    "vqa": [
      {
        "id": "Qwen/Qwen2.5-VL-7B-Instruct",
        "source": "hf",
        "name": "Qwen2.5-VL-7B-Instruct",
        "url": "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "qwen2_5_vl",
          "image-to-text",
          "multimodal",
          "image-text-to-text",
          "conversational",
          "en",
          "arxiv:2309.00071",
          "arxiv:2409.12191",
          "arxiv:2308.12966",
          "license:apache-2.0",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4301984,
          "likes_total": 1267,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态人工智能模型，专为视觉语言理解和指令跟随而设计。该模型能够同时处理图像和文本输入，生成连贯的文本响应。核心功能包括图像描述生成、视觉问答和多模态对话。主要优势在于模型尺寸紧凑、推理效率高，且在视觉推理任务上表现优异。典型应用场景涵盖人工智能助手、内容分析工具以及需要图像文本集成处理的教育应用。该模型支持多语言交互，并与标准部署框架兼容，适用于实际生产环境。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_text_alignment",
          "multimodal_understanding_generation",
          "visual_grounding",
          "vqa",
          "lightweight_multimodal_model"
        ],
        "summary_en": "Qwen2.5-VL-7B-Instruct is a 7-billion parameter multimodal AI model designed for visual-language understanding and instruction following. It processes both images and text inputs to generate coherent textual responses. Core capabilities include image captioning, visual question answering, and multimodal conversations. Strengths encompass its compact size, efficient inference, and strong performance on visual reasoning tasks. Typical use cases involve AI assistants, content analysis tools, and educational applications requiring integrated image-text processing. The model supports multiple languages and is compatible with standard deployment frameworks.",
        "summary_zh": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态人工智能模型，专为视觉语言理解和指令跟随而设计。该模型能够同时处理图像和文本输入，生成连贯的文本响应。核心功能包括图像描述生成、视觉问答和多模态对话。主要优势在于模型尺寸紧凑、推理效率高，且在视觉推理任务上表现优异。典型应用场景涵盖人工智能助手、内容分析工具以及需要图像文本集成处理的教育应用。该模型支持多语言交互，并与标准部署框架兼容，适用于实际生产环境。",
        "summary_es": "Qwen2.5-VL-7B-Instruct is a 7-billion parameter multimodal AI model designed for visual-language understanding and instruction following. It processes both images and text inputs to generate coherent textual responses. Core capabilities include image captioning, visual question answering, and multimodal conversations. Strengths encompass its compact size, efficient inference, and strong performance on visual reasoning tasks. Typical use cases involve AI assistants, content analysis tools, and educational applications requiring integrated image-text processing. The model supports multiple languages and is compatible with standard deployment frameworks."
      }
    ],
    "visual_grounding": [
      {
        "id": "Qwen/Qwen2.5-VL-7B-Instruct",
        "source": "hf",
        "name": "Qwen2.5-VL-7B-Instruct",
        "url": "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "qwen2_5_vl",
          "image-to-text",
          "multimodal",
          "image-text-to-text",
          "conversational",
          "en",
          "arxiv:2309.00071",
          "arxiv:2409.12191",
          "arxiv:2308.12966",
          "license:apache-2.0",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4301984,
          "likes_total": 1267,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态人工智能模型，专为视觉语言理解和指令跟随而设计。该模型能够同时处理图像和文本输入，生成连贯的文本响应。核心功能包括图像描述生成、视觉问答和多模态对话。主要优势在于模型尺寸紧凑、推理效率高，且在视觉推理任务上表现优异。典型应用场景涵盖人工智能助手、内容分析工具以及需要图像文本集成处理的教育应用。该模型支持多语言交互，并与标准部署框架兼容，适用于实际生产环境。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_text_alignment",
          "multimodal_understanding_generation",
          "visual_grounding",
          "vqa",
          "lightweight_multimodal_model"
        ],
        "summary_en": "Qwen2.5-VL-7B-Instruct is a 7-billion parameter multimodal AI model designed for visual-language understanding and instruction following. It processes both images and text inputs to generate coherent textual responses. Core capabilities include image captioning, visual question answering, and multimodal conversations. Strengths encompass its compact size, efficient inference, and strong performance on visual reasoning tasks. Typical use cases involve AI assistants, content analysis tools, and educational applications requiring integrated image-text processing. The model supports multiple languages and is compatible with standard deployment frameworks.",
        "summary_zh": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态人工智能模型，专为视觉语言理解和指令跟随而设计。该模型能够同时处理图像和文本输入，生成连贯的文本响应。核心功能包括图像描述生成、视觉问答和多模态对话。主要优势在于模型尺寸紧凑、推理效率高，且在视觉推理任务上表现优异。典型应用场景涵盖人工智能助手、内容分析工具以及需要图像文本集成处理的教育应用。该模型支持多语言交互，并与标准部署框架兼容，适用于实际生产环境。",
        "summary_es": "Qwen2.5-VL-7B-Instruct is a 7-billion parameter multimodal AI model designed for visual-language understanding and instruction following. It processes both images and text inputs to generate coherent textual responses. Core capabilities include image captioning, visual question answering, and multimodal conversations. Strengths encompass its compact size, efficient inference, and strong performance on visual reasoning tasks. Typical use cases involve AI assistants, content analysis tools, and educational applications requiring integrated image-text processing. The model supports multiple languages and is compatible with standard deployment frameworks."
      }
    ],
    "lightweight_visual_model": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      }
    ],
    "llm_pretraining": [
      {
        "id": "google/electra-base-discriminator",
        "source": "hf",
        "name": "electra-base-discriminator",
        "url": "https://huggingface.co/google/electra-base-discriminator",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "electra",
          "pretraining",
          "en",
          "arxiv:1406.2661",
          "license:apache-2.0",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 16009189,
          "likes_total": 65,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "ELECTRA-base-discriminator是谷歌开发的一种预训练语言模型，采用名为“替换令牌检测”的创新预训练方法。与BERT预测掩码令牌不同，该模型通过区分原始令牌和由小型生成器网络创建的合理替换令牌来进行训练，这种方法显著提高了预训练的计算效率。模型擅长理解文本中的上下文关系，可作为各种自然语言处理任务的强大基础。典型应用场景包括文本分类、命名实体识别和问答系统，通常在特定数据集上进行微调后使用。该模型支持多种框架，具有高效的预",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "llm_pretraining",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ELECTRA-base-discriminator is a pre-trained language model developed by Google that uses a novel pre-training approach called replaced token detection. Instead of predicting masked tokens like BERT, it distinguishes between original and plausible replacement tokens generated by a small generator network. This method makes pre-training more computationally efficient. The model excels at understanding contextual relationships in text and serves as a strong foundation for various natural language processing tasks. Typical use cases include text classification, named entity recognition, and question answering when fine-tuned on specific datasets.",
        "summary_zh": "ELECTRA-base-discriminator是谷歌开发的一种预训练语言模型，采用名为“替换令牌检测”的创新预训练方法。与BERT预测掩码令牌不同，该模型通过区分原始令牌和由小型生成器网络创建的合理替换令牌来进行训练，这种方法显著提高了预训练的计算效率。模型擅长理解文本中的上下文关系，可作为各种自然语言处理任务的强大基础。典型应用场景包括文本分类、命名实体识别和问答系统，通常在特定数据集上进行微调后使用。该模型支持多种框架，具有高效的预",
        "summary_es": "ELECTRA-base-discriminator is a pre-trained language model developed by Google that uses a novel pre-training approach called replaced token detection. Instead of predicting masked tokens like BERT, it distinguishes between original and plausible replacement tokens generated by a small generator network. This method makes pre-training more computationally efficient. The model excels at understanding contextual relationships in text and serves as a strong foundation for various natural language processing tasks. Typical use cases include text classification, named entity recognition, and question answering when fine-tuned on specific datasets."
      },
      {
        "id": "omni-research/Tarsier2-Recap-7b",
        "source": "hf",
        "name": "Tarsier2-Recap-7b",
        "url": "https://huggingface.co/omni-research/Tarsier2-Recap-7b",
        "tags": [
          "safetensors",
          "video LLM",
          "arxiv:2501.07888",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 10283648,
          "likes_total": 19,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Tarsier2-Recap-7b 是一个拥有70亿参数的专业视频语言模型，专门用于深度视频理解与内容概括。其核心能力在于处理视频流数据并生成全面的文本描述和摘要。该模型擅长从视觉序列中提取关键信息，并将其转化为连贯的叙述文本。典型应用场景包括自动化视频字幕生成、媒体资料库的内容分析、以及教育或纪录片素材的摘要制作。基于arXiv:2501.07888的研究成果，采用safetensors格式和Apache 2.0开源协议，在跨模态理解任务中表现出色，特别在保持内容准确性和结构完整性方面具有",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_video",
          "llm_pretraining"
        ],
        "summary_en": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for comprehensive video understanding and summarization. Its core capability involves processing video content to generate detailed textual descriptions and summaries. The model excels at extracting key information from visual sequences and converting it into coherent narratives. Typical use cases include automated video captioning, content analysis for media archives, and generating summaries for educational or documentary footage. Based on research from arXiv:2501.07888, it employs safetensors format and operates under Apache 2.0 license, demonstrating strong performance in multimodal understanding tasks.",
        "summary_zh": "Tarsier2-Recap-7b 是一个拥有70亿参数的专业视频语言模型，专门用于深度视频理解与内容概括。其核心能力在于处理视频流数据并生成全面的文本描述和摘要。该模型擅长从视觉序列中提取关键信息，并将其转化为连贯的叙述文本。典型应用场景包括自动化视频字幕生成、媒体资料库的内容分析、以及教育或纪录片素材的摘要制作。基于arXiv:2501.07888的研究成果，采用safetensors格式和Apache 2.0开源协议，在跨模态理解任务中表现出色，特别在保持内容准确性和结构完整性方面具有",
        "summary_es": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for comprehensive video understanding and summarization. Its core capability involves processing video content to generate detailed textual descriptions and summaries. The model excels at extracting key information from visual sequences and converting it into coherent narratives. Typical use cases include automated video captioning, content analysis for media archives, and generating summaries for educational or documentary footage. Based on research from arXiv:2501.07888, it employs safetensors format and operates under Apache 2.0 license, demonstrating strong performance in multimodal understanding tasks."
      }
    ],
    "instruction_tuning": [
      {
        "id": "google/gemma-3-1b-it",
        "source": "hf",
        "name": "gemma-3-1b-it",
        "url": "https://huggingface.co/google/gemma-3-1b-it",
        "tags": [
          "transformers",
          "safetensors",
          "gemma3_text",
          "text-generation",
          "conversational",
          "arxiv:1905.07830",
          "arxiv:1905.10044",
          "arxiv:1911.11641",
          "arxiv:1904.09728",
          "arxiv:1705.03551",
          "arxiv:1911.01547",
          "arxiv:1907.10641",
          "arxiv:1903.00161",
          "arxiv:2009.03300",
          "arxiv:2304.06364",
          "arxiv:2103.03874",
          "arxiv:2110.14168",
          "arxiv:2311.12022",
          "arxiv:2108.07732",
          "arxiv:2107.03374",
          "arxiv:2210.03057",
          "arxiv:2106.03193",
          "arxiv:1910.11856",
          "arxiv:2502.12404",
          "arxiv:2502.21228",
          "arxiv:2404.16816",
          "arxiv:2104.12756",
          "arxiv:2311.16502",
          "arxiv:2203.10244",
          "arxiv:2404.12390",
          "arxiv:1810.12440",
          "arxiv:1908.02660",
          "arxiv:2312.11805",
          "base_model:google/gemma-3-1b-pt",
          "base_model:finetune:google/gemma-3-1b-pt",
          "license:gemma",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 5753213,
          "likes_total": 629,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Gemma 3 1B-IT是谷歌开发的轻量级11亿参数指令调优语言模型，专为高效部署设计。基于Transformer架构，支持多语言文本生成、代码补全和推理任务。核心优势包括快速推理速度、低资源需求和开放可访问性。典型应用场景涵盖聊天机器人、内容创作、编程辅助和教育工具等需要优先考虑计算效率而非极致性能的场合。该模型在能力与实用部署约束之间实现了良好平衡，适用于资源受限环境下的AI应用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "instruction_tuning"
        ],
        "summary_en": "Gemma 3 1B-IT is Google's lightweight 1.1 billion parameter instruction-tuned language model designed for efficient deployment. Built on transformer architecture, it supports multilingual text generation, code completion, and reasoning tasks. Key strengths include fast inference speed, low resource requirements, and open accessibility. Typical use cases encompass chatbots, content creation, programming assistance, and educational tools where computational efficiency is prioritized over maximum performance. The model balances capability with practical deployment constraints.",
        "summary_zh": "Gemma 3 1B-IT是谷歌开发的轻量级11亿参数指令调优语言模型，专为高效部署设计。基于Transformer架构，支持多语言文本生成、代码补全和推理任务。核心优势包括快速推理速度、低资源需求和开放可访问性。典型应用场景涵盖聊天机器人、内容创作、编程辅助和教育工具等需要优先考虑计算效率而非极致性能的场合。该模型在能力与实用部署约束之间实现了良好平衡，适用于资源受限环境下的AI应用。",
        "summary_es": "Gemma 3 1B-IT is Google's lightweight 1.1 billion parameter instruction-tuned language model designed for efficient deployment. Built on transformer architecture, it supports multilingual text generation, code completion, and reasoning tasks. Key strengths include fast inference speed, low resource requirements, and open accessibility. Typical use cases encompass chatbots, content creation, programming assistance, and educational tools where computational efficiency is prioritized over maximum performance. The model balances capability with practical deployment constraints."
      }
    ],
    "rlhf": [],
    "rag": [],
    "code_generation": [
      {
        "id": "openai-community/gpt2",
        "source": "hf",
        "name": "gpt2",
        "url": "https://huggingface.co/openai-community/gpt2",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "tflite",
          "rust",
          "onnx",
          "safetensors",
          "gpt2",
          "text-generation",
          "exbert",
          "en",
          "doi:10.57967/hf/0039",
          "license:mit",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 11902438,
          "likes_total": 2955,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本生成任务。其核心能力是通过输入提示预测后续文本内容，实现连贯的文本延续。主要优势包括在零样本设置下无需特定任务训练即可生成高质量文本、能够跨多个领域生成类人文本内容以及开源可访问性。典型应用场景涵盖创意写作辅助、对话系统原型开发、内容摘要生成和代码自动补全。该模型在开放式文本补全方面表现突出，能够在长文本输出中保持上下文相关性，同时支",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "GPT-2 is a transformer-based language model developed by OpenAI for text generation tasks. Its core capability involves predicting subsequent text based on input prompts, enabling coherent continuation of text. Key strengths include strong performance in zero-shot settings without task-specific training, generating human-like text across diverse domains, and open-source availability. Typical use cases encompass creative writing assistance, conversational AI prototyping, content summarization, and code generation. The model demonstrates particular effectiveness in open-ended text completion while maintaining contextual relevance throughout extended outputs.",
        "summary_zh": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本生成任务。其核心能力是通过输入提示预测后续文本内容，实现连贯的文本延续。主要优势包括在零样本设置下无需特定任务训练即可生成高质量文本、能够跨多个领域生成类人文本内容以及开源可访问性。典型应用场景涵盖创意写作辅助、对话系统原型开发、内容摘要生成和代码自动补全。该模型在开放式文本补全方面表现突出，能够在长文本输出中保持上下文相关性，同时支",
        "summary_es": "GPT-2 is a transformer-based language model developed by OpenAI for text generation tasks. Its core capability involves predicting subsequent text based on input prompts, enabling coherent continuation of text. Key strengths include strong performance in zero-shot settings without task-specific training, generating human-like text across diverse domains, and open-source availability. Typical use cases encompass creative writing assistance, conversational AI prototyping, content summarization, and code generation. The model demonstrates particular effectiveness in open-ended text completion while maintaining contextual relevance throughout extended outputs."
      },
      {
        "id": "facebook/opt-125m",
        "source": "hf",
        "name": "opt-125m",
        "url": "https://huggingface.co/facebook/opt-125m",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "opt",
          "text-generation",
          "en",
          "arxiv:2205.01068",
          "arxiv:2005.14165",
          "license:other",
          "autotrain_compatible",
          "text-generation-inference",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8602519,
          "likes_total": 218,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "OPT-125M是Meta AI开发的Open Pre-trained Transformer系列中的1.25亿参数仅解码器变压器语言模型，作为较大OPT模型的缩小版本，主要用于研究和实验目的。该模型能够根据输入提示生成连贯文本，支持多种自然语言处理任务。其主要优势包括面向研究用途的开放可访问性、支持多种框架（PyTorch、TensorFlow、JAX）的兼容性以及高效的推理能力。典型应用场景涵盖文本生成实验、变压器架构的教育演示、小规模语言模型性能基准测试，以及作为理解更大规模语言模型工作原理的入门工具。该模型特别适合计",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "OPT-125M is a 125 million parameter decoder-only transformer language model developed by Meta AI as part of the Open Pre-trained Transformer series. It serves as a smaller-scale version of larger OPT models for research and experimentation. The model generates coherent text based on input prompts and supports various natural language processing tasks. Its primary strengths include open accessibility for research purposes, compatibility with multiple frameworks (PyTorch, TensorFlow, JAX), and efficient inference capabilities. Typical use cases encompass text generation experiments, educational demonstrations of transformer architectures, and benchmarking smaller-scale language model performance.",
        "summary_zh": "OPT-125M是Meta AI开发的Open Pre-trained Transformer系列中的1.25亿参数仅解码器变压器语言模型，作为较大OPT模型的缩小版本，主要用于研究和实验目的。该模型能够根据输入提示生成连贯文本，支持多种自然语言处理任务。其主要优势包括面向研究用途的开放可访问性、支持多种框架（PyTorch、TensorFlow、JAX）的兼容性以及高效的推理能力。典型应用场景涵盖文本生成实验、变压器架构的教育演示、小规模语言模型性能基准测试，以及作为理解更大规模语言模型工作原理的入门工具。该模型特别适合计",
        "summary_es": "OPT-125M is a 125 million parameter decoder-only transformer language model developed by Meta AI as part of the Open Pre-trained Transformer series. It serves as a smaller-scale version of larger OPT models for research and experimentation. The model generates coherent text based on input prompts and supports various natural language processing tasks. Its primary strengths include open accessibility for research purposes, compatibility with multiple frameworks (PyTorch, TensorFlow, JAX), and efficient inference capabilities. Typical use cases encompass text generation experiments, educational demonstrations of transformer architectures, and benchmarking smaller-scale language model performance."
      }
    ],
    "structured_reasoning": [],
    "tool_use": [],
    "lora_adapter": [],
    "multilingual_processing": [
      {
        "id": "FacebookAI/xlm-roberta-base",
        "source": "hf",
        "name": "xlm-roberta-base",
        "url": "https://huggingface.co/FacebookAI/xlm-roberta-base",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "onnx",
          "safetensors",
          "xlm-roberta",
          "fill-mask",
          "exbert",
          "multilingual",
          "af",
          "am",
          "ar",
          "as",
          "az",
          "be",
          "bg",
          "bn",
          "br",
          "bs",
          "ca",
          "cs",
          "cy",
          "da",
          "de",
          "el",
          "en",
          "eo",
          "es",
          "et",
          "eu",
          "fa",
          "fi",
          "fr",
          "fy",
          "ga",
          "gd",
          "gl",
          "gu",
          "ha",
          "he",
          "hi",
          "hr",
          "hu",
          "hy",
          "id",
          "is",
          "it",
          "ja",
          "jv",
          "ka",
          "kk",
          "km",
          "kn",
          "ko",
          "ku",
          "ky",
          "la",
          "lo",
          "lt",
          "lv",
          "mg",
          "mk",
          "ml",
          "mn",
          "mr",
          "ms",
          "my",
          "ne",
          "nl",
          "no",
          "om",
          "or",
          "pa",
          "pl",
          "ps",
          "pt",
          "ro",
          "ru",
          "sa",
          "sd",
          "si",
          "sk",
          "sl",
          "so",
          "sq",
          "sr",
          "su",
          "sv",
          "sw",
          "ta",
          "te",
          "th",
          "tl",
          "tr",
          "ug",
          "uk",
          "ur",
          "uz",
          "vi",
          "xh",
          "yi",
          "zh",
          "arxiv:1911.02116",
          "license:mit",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7771691,
          "likes_total": 732,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "xlm-roberta-base是由Facebook AI开发的多语言掩码语言模型，基于RoBERTa架构改进而成。该模型支持100种语言，使用CommonCrawl的大规模多语言文本进行预训练。主要优势在于无需平行语料即可实现跨语言理解，在零样本跨语言迁移任务中表现优异。核心功能包括多语言文本分类、命名实体识别和问答系统。技术特点包括改进的预训练目标、更好的跨语言表示对齐以及高效的多语言处理能力。典型应用场景涵盖多语言内容分析、跨语言信息检索、多语言客服系统以及需要同时处理",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "multilingual_processing",
          "auto_evaluation_models"
        ],
        "summary_en": "xlm-roberta-base is a multilingual masked language model developed by Facebook AI, based on the RoBERTa architecture. It supports 100 languages and is pretrained on large-scale multilingual text from CommonCrawl. The model excels at cross-lingual understanding tasks without requiring parallel data. Core capabilities include text classification, named entity recognition, and question answering across languages. Its strengths lie in robust zero-shot cross-lingual transfer performance and efficient representation learning. Typical use cases involve multilingual content analysis, cross-lingual information retrieval, and building applications that require understanding multiple languages simultaneously.",
        "summary_zh": "xlm-roberta-base是由Facebook AI开发的多语言掩码语言模型，基于RoBERTa架构改进而成。该模型支持100种语言，使用CommonCrawl的大规模多语言文本进行预训练。主要优势在于无需平行语料即可实现跨语言理解，在零样本跨语言迁移任务中表现优异。核心功能包括多语言文本分类、命名实体识别和问答系统。技术特点包括改进的预训练目标、更好的跨语言表示对齐以及高效的多语言处理能力。典型应用场景涵盖多语言内容分析、跨语言信息检索、多语言客服系统以及需要同时处理",
        "summary_es": "xlm-roberta-base is a multilingual masked language model developed by Facebook AI, based on the RoBERTa architecture. It supports 100 languages and is pretrained on large-scale multilingual text from CommonCrawl. The model excels at cross-lingual understanding tasks without requiring parallel data. Core capabilities include text classification, named entity recognition, and question answering across languages. Its strengths lie in robust zero-shot cross-lingual transfer performance and efficient representation learning. Typical use cases involve multilingual content analysis, cross-lingual information retrieval, and building applications that require understanding multiple languages simultaneously."
      },
      {
        "id": "google-t5/t5-small",
        "source": "hf",
        "name": "t5-small",
        "url": "https://huggingface.co/google-t5/t5-small",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "onnx",
          "safetensors",
          "t5",
          "text2text-generation",
          "summarization",
          "translation",
          "en",
          "fr",
          "ro",
          "de",
          "multilingual",
          "dataset:c4",
          "arxiv:1805.12471",
          "arxiv:1708.00055",
          "arxiv:1704.05426",
          "arxiv:1606.05250",
          "arxiv:1808.09121",
          "arxiv:1810.12885",
          "arxiv:1905.10044",
          "arxiv:1910.09700",
          "license:apache-2.0",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4970400,
          "likes_total": 492,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "T5-small是谷歌开发的紧凑型文本到文本转换Transformer模型，将所有自然语言处理任务统一为文本生成问题。该模型基于T5架构，使用统一的前缀提示将分类、翻译等任务转换为文本输出。支持英语、德语、法语等多语言，并在C4数据集上进行预训练。模型体积小巧，适合资源受限环境，同时在摘要生成、问答系统、文本分类等多种NLP应用中保持合理性能。其核心优势在于统一的文本到文本框架简化了多任务处理流程。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "multilingual_processing",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "training_data_anonymization",
          "training_data_copyright",
          "molecular_generation"
        ],
        "summary_en": "T5-small is a compact text-to-text transformer model from Google that treats all NLP tasks as text generation problems. Based on the T5 architecture, it converts inputs like classification or translation into text outputs using consistent prefix prompts. The model supports multiple languages including English, German, and French, and was pre-trained on the C4 dataset. Its small size makes it suitable for resource-constrained environments while maintaining reasonable performance across various NLP applications such as summarization, question answering, and text classification tasks.",
        "summary_zh": "T5-small是谷歌开发的紧凑型文本到文本转换Transformer模型，将所有自然语言处理任务统一为文本生成问题。该模型基于T5架构，使用统一的前缀提示将分类、翻译等任务转换为文本输出。支持英语、德语、法语等多语言，并在C4数据集上进行预训练。模型体积小巧，适合资源受限环境，同时在摘要生成、问答系统、文本分类等多种NLP应用中保持合理性能。其核心优势在于统一的文本到文本框架简化了多任务处理流程。",
        "summary_es": "T5-small is a compact text-to-text transformer model from Google that treats all NLP tasks as text generation problems. Based on the T5 architecture, it converts inputs like classification or translation into text outputs using consistent prefix prompts. The model supports multiple languages including English, German, and French, and was pre-trained on the C4 dataset. Its small size makes it suitable for resource-constrained environments while maintaining reasonable performance across various NLP applications such as summarization, question answering, and text classification tasks."
      }
    ],
    "low_resource_language": [],
    "knowledge_editing": [],
    "nlp_data_synthesis": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "google-bert/bert-base-uncased",
        "source": "hf",
        "name": "bert-base-uncased",
        "url": "https://huggingface.co/google-bert/bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "coreml",
          "onnx",
          "safetensors",
          "bert",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1810.04805",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 55204102,
          "likes_total": 2417,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "BERT-base-uncased是基于Transformer架构的英语预训练语言模型，在Wikipedia和BookCorpus语料上通过掩码语言建模和下一句预测任务进行训练。该模型采用双向编码器设计，能够深度理解词语在上下文中的语义关系。其主要优势在于强大的语境理解能力和通用性，支持文本分类、命名实体识别、问答系统、情感分析等多种自然语言处理任务。作为uncased版本，模型不区分字母大小写，适用于大多数不需要大小写敏感处理的通用文本分析场景。该模型已成为许多下游NLP应用的基础架构。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus. It employs masked language modeling and next sentence prediction to develop deep bidirectional contextual representations. The model excels at understanding word meanings in context and capturing semantic relationships. Its primary applications include text classification, named entity recognition, question answering, and sentiment analysis. As an uncased model, it treats uppercase and lowercase letters identically, making it suitable for general text processing tasks where case sensitivity is not required.",
        "summary_zh": "BERT-base-uncased是基于Transformer架构的英语预训练语言模型，在Wikipedia和BookCorpus语料上通过掩码语言建模和下一句预测任务进行训练。该模型采用双向编码器设计，能够深度理解词语在上下文中的语义关系。其主要优势在于强大的语境理解能力和通用性，支持文本分类、命名实体识别、问答系统、情感分析等多种自然语言处理任务。作为uncased版本，模型不区分字母大小写，适用于大多数不需要大小写敏感处理的通用文本分析场景。该模型已成为许多下游NLP应用的基础架构。",
        "summary_es": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus. It employs masked language modeling and next sentence prediction to develop deep bidirectional contextual representations. The model excels at understanding word meanings in context and capturing semantic relationships. Its primary applications include text classification, named entity recognition, question answering, and sentiment analysis. As an uncased model, it treats uppercase and lowercase letters identically, making it suitable for general text processing tasks where case sensitivity is not required."
      }
    ],
    "nlp_data_distillation": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "google-bert/bert-base-uncased",
        "source": "hf",
        "name": "bert-base-uncased",
        "url": "https://huggingface.co/google-bert/bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "coreml",
          "onnx",
          "safetensors",
          "bert",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1810.04805",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 55204102,
          "likes_total": 2417,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "BERT-base-uncased是基于Transformer架构的英语预训练语言模型，在Wikipedia和BookCorpus语料上通过掩码语言建模和下一句预测任务进行训练。该模型采用双向编码器设计，能够深度理解词语在上下文中的语义关系。其主要优势在于强大的语境理解能力和通用性，支持文本分类、命名实体识别、问答系统、情感分析等多种自然语言处理任务。作为uncased版本，模型不区分字母大小写，适用于大多数不需要大小写敏感处理的通用文本分析场景。该模型已成为许多下游NLP应用的基础架构。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus. It employs masked language modeling and next sentence prediction to develop deep bidirectional contextual representations. The model excels at understanding word meanings in context and capturing semantic relationships. Its primary applications include text classification, named entity recognition, question answering, and sentiment analysis. As an uncased model, it treats uppercase and lowercase letters identically, making it suitable for general text processing tasks where case sensitivity is not required.",
        "summary_zh": "BERT-base-uncased是基于Transformer架构的英语预训练语言模型，在Wikipedia和BookCorpus语料上通过掩码语言建模和下一句预测任务进行训练。该模型采用双向编码器设计，能够深度理解词语在上下文中的语义关系。其主要优势在于强大的语境理解能力和通用性，支持文本分类、命名实体识别、问答系统、情感分析等多种自然语言处理任务。作为uncased版本，模型不区分字母大小写，适用于大多数不需要大小写敏感处理的通用文本分析场景。该模型已成为许多下游NLP应用的基础架构。",
        "summary_es": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus. It employs masked language modeling and next sentence prediction to develop deep bidirectional contextual representations. The model excels at understanding word meanings in context and capturing semantic relationships. Its primary applications include text classification, named entity recognition, question answering, and sentiment analysis. As an uncased model, it treats uppercase and lowercase letters identically, making it suitable for general text processing tasks where case sensitivity is not required."
      }
    ],
    "dialogue_system_optimization": [],
    "nlp_bias_mitigation": [
      {
        "id": "nlpaueb/legal-bert-base-uncased",
        "source": "hf",
        "name": "legal-bert-base-uncased",
        "url": "https://huggingface.co/nlpaueb/legal-bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "bert",
          "pretraining",
          "legal",
          "fill-mask",
          "en",
          "license:cc-by-sa-4.0",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 5574178,
          "likes_total": 271,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Legal-BERT-Base-Uncased 是基于 BERT 架构、专门针对法律文本预训练的英语语言模型，使用欧盟法律文档进行训练。其核心目的是提升法律文档的自然语言处理能力，具备掩码语言建模和法律文本理解功能。主要优势包括针对法律术语的领域专业化训练，以及支持 PyTorch、TensorFlow 和 JAX 等多框架兼容性。典型应用场景涵盖法律文件分析、合同审查、法律研究辅助，以及需要精确理解法律语言的自动化文本处理任务，适用于法律科技和文档处理工作流。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "llm_pretraining",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "nlp_bias_mitigation",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "Legal-BERT-Base-Uncased is a specialized BERT model pretrained on extensive English legal text from the European Union. Its purpose is to enhance natural language processing for legal documents. Core capabilities include masked language modeling for text completion and legal text understanding. Strengths are domain-specific training on legal terminology and compatibility with major ML frameworks. Typical use cases involve legal document analysis, contract review, legal research assistance, and automated legal text processing tasks requiring accurate comprehension of legal language.",
        "summary_zh": "Legal-BERT-Base-Uncased 是基于 BERT 架构、专门针对法律文本预训练的英语语言模型，使用欧盟法律文档进行训练。其核心目的是提升法律文档的自然语言处理能力，具备掩码语言建模和法律文本理解功能。主要优势包括针对法律术语的领域专业化训练，以及支持 PyTorch、TensorFlow 和 JAX 等多框架兼容性。典型应用场景涵盖法律文件分析、合同审查、法律研究辅助，以及需要精确理解法律语言的自动化文本处理任务，适用于法律科技和文档处理工作流。",
        "summary_es": "Legal-BERT-Base-Uncased is a specialized BERT model pretrained on extensive English legal text from the European Union. Its purpose is to enhance natural language processing for legal documents. Core capabilities include masked language modeling for text completion and legal text understanding. Strengths are domain-specific training on legal terminology and compatibility with major ML frameworks. Typical use cases involve legal document analysis, contract review, legal research assistance, and automated legal text processing tasks requiring accurate comprehension of legal language."
      }
    ],
    "image_text_alignment": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 99881324,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems.",
        "summary_zh": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      }
    ],
    "multimodal_understanding_generation": [
      {
        "id": "openai-community/gpt2",
        "source": "hf",
        "name": "gpt2",
        "url": "https://huggingface.co/openai-community/gpt2",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "tflite",
          "rust",
          "onnx",
          "safetensors",
          "gpt2",
          "text-generation",
          "exbert",
          "en",
          "doi:10.57967/hf/0039",
          "license:mit",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 11902438,
          "likes_total": 2955,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本生成任务。其核心能力是通过输入提示预测后续文本内容，实现连贯的文本延续。主要优势包括在零样本设置下无需特定任务训练即可生成高质量文本、能够跨多个领域生成类人文本内容以及开源可访问性。典型应用场景涵盖创意写作辅助、对话系统原型开发、内容摘要生成和代码自动补全。该模型在开放式文本补全方面表现突出，能够在长文本输出中保持上下文相关性，同时支",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "GPT-2 is a transformer-based language model developed by OpenAI for text generation tasks. Its core capability involves predicting subsequent text based on input prompts, enabling coherent continuation of text. Key strengths include strong performance in zero-shot settings without task-specific training, generating human-like text across diverse domains, and open-source availability. Typical use cases encompass creative writing assistance, conversational AI prototyping, content summarization, and code generation. The model demonstrates particular effectiveness in open-ended text completion while maintaining contextual relevance throughout extended outputs.",
        "summary_zh": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本生成任务。其核心能力是通过输入提示预测后续文本内容，实现连贯的文本延续。主要优势包括在零样本设置下无需特定任务训练即可生成高质量文本、能够跨多个领域生成类人文本内容以及开源可访问性。典型应用场景涵盖创意写作辅助、对话系统原型开发、内容摘要生成和代码自动补全。该模型在开放式文本补全方面表现突出，能够在长文本输出中保持上下文相关性，同时支",
        "summary_es": "GPT-2 is a transformer-based language model developed by OpenAI for text generation tasks. Its core capability involves predicting subsequent text based on input prompts, enabling coherent continuation of text. Key strengths include strong performance in zero-shot settings without task-specific training, generating human-like text across diverse domains, and open-source availability. Typical use cases encompass creative writing assistance, conversational AI prototyping, content summarization, and code generation. The model demonstrates particular effectiveness in open-ended text completion while maintaining contextual relevance throughout extended outputs."
      },
      {
        "id": "facebook/opt-125m",
        "source": "hf",
        "name": "opt-125m",
        "url": "https://huggingface.co/facebook/opt-125m",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "opt",
          "text-generation",
          "en",
          "arxiv:2205.01068",
          "arxiv:2005.14165",
          "license:other",
          "autotrain_compatible",
          "text-generation-inference",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8602519,
          "likes_total": 218,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "OPT-125M是Meta AI开发的Open Pre-trained Transformer系列中的1.25亿参数仅解码器变压器语言模型，作为较大OPT模型的缩小版本，主要用于研究和实验目的。该模型能够根据输入提示生成连贯文本，支持多种自然语言处理任务。其主要优势包括面向研究用途的开放可访问性、支持多种框架（PyTorch、TensorFlow、JAX）的兼容性以及高效的推理能力。典型应用场景涵盖文本生成实验、变压器架构的教育演示、小规模语言模型性能基准测试，以及作为理解更大规模语言模型工作原理的入门工具。该模型特别适合计",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "OPT-125M is a 125 million parameter decoder-only transformer language model developed by Meta AI as part of the Open Pre-trained Transformer series. It serves as a smaller-scale version of larger OPT models for research and experimentation. The model generates coherent text based on input prompts and supports various natural language processing tasks. Its primary strengths include open accessibility for research purposes, compatibility with multiple frameworks (PyTorch, TensorFlow, JAX), and efficient inference capabilities. Typical use cases encompass text generation experiments, educational demonstrations of transformer architectures, and benchmarking smaller-scale language model performance.",
        "summary_zh": "OPT-125M是Meta AI开发的Open Pre-trained Transformer系列中的1.25亿参数仅解码器变压器语言模型，作为较大OPT模型的缩小版本，主要用于研究和实验目的。该模型能够根据输入提示生成连贯文本，支持多种自然语言处理任务。其主要优势包括面向研究用途的开放可访问性、支持多种框架（PyTorch、TensorFlow、JAX）的兼容性以及高效的推理能力。典型应用场景涵盖文本生成实验、变压器架构的教育演示、小规模语言模型性能基准测试，以及作为理解更大规模语言模型工作原理的入门工具。该模型特别适合计",
        "summary_es": "OPT-125M is a 125 million parameter decoder-only transformer language model developed by Meta AI as part of the Open Pre-trained Transformer series. It serves as a smaller-scale version of larger OPT models for research and experimentation. The model generates coherent text based on input prompts and supports various natural language processing tasks. Its primary strengths include open accessibility for research purposes, compatibility with multiple frameworks (PyTorch, TensorFlow, JAX), and efficient inference capabilities. Typical use cases encompass text generation experiments, educational demonstrations of transformer architectures, and benchmarking smaller-scale language model performance."
      }
    ],
    "asr": [
      {
        "id": "pyannote/speaker-diarization-3.1",
        "source": "hf",
        "name": "speaker-diarization-3.1",
        "url": "https://huggingface.co/pyannote/speaker-diarization-3.1",
        "tags": [
          "pyannote-audio",
          "pyannote",
          "pyannote-audio-pipeline",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "automatic-speech-recognition",
          "arxiv:2111.14448",
          "arxiv:2012.01477",
          "license:mit",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 16673680,
          "likes_total": 1169,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote speaker-diarization-3.1 是一个音频处理流程，专门用于自动分割和标记录音中的说话人身份。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该系统擅长处理多人对话场景，具有高时间精度，特别适用于会议转录、广播监控和对话分析等典型用例。基于 arXiv:2012.01477 和 arXiv:2111.14448 的研究成果，这款 MIT 许可的工具能够处理音频并输出结构化的时间线，准确识别“谁在何时说话”，在各种声学条件下均表现出稳定性能。该工具通过先进的深度学习技术实现对连续语音流的实",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "asr",
          "speaker_separation",
          "vector_retrieval",
          "graph_augmented_reco",
          "auto_evaluation_models",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote speaker-diarization-3.1 is an audio processing pipeline that automatically segments and labels speaker identities in audio recordings. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The system excels at handling multi-speaker conversations with high temporal precision, making it particularly strong for meeting transcriptions, broadcast monitoring, and conversational analysis. Based on research from arXiv:2012.01477 and arXiv:2111.14448, this MIT-licensed tool processes audio to output structured timelines identifying 'who spoke when' with robust performance across various acoustic conditions.",
        "summary_zh": "Pyannote speaker-diarization-3.1 是一个音频处理流程，专门用于自动分割和标记录音中的说话人身份。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该系统擅长处理多人对话场景，具有高时间精度，特别适用于会议转录、广播监控和对话分析等典型用例。基于 arXiv:2012.01477 和 arXiv:2111.14448 的研究成果，这款 MIT 许可的工具能够处理音频并输出结构化的时间线，准确识别“谁在何时说话”，在各种声学条件下均表现出稳定性能。该工具通过先进的深度学习技术实现对连续语音流的实",
        "summary_es": "Pyannote speaker-diarization-3.1 is an audio processing pipeline that automatically segments and labels speaker identities in audio recordings. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The system excels at handling multi-speaker conversations with high temporal precision, making it particularly strong for meeting transcriptions, broadcast monitoring, and conversational analysis. Based on research from arXiv:2012.01477 and arXiv:2111.14448, this MIT-licensed tool processes audio to output structured timelines identifying 'who spoke when' with robust performance across various acoustic conditions."
      },
      {
        "id": "pyannote/voice-activity-detection",
        "source": "hf",
        "name": "voice-activity-detection",
        "url": "https://huggingface.co/pyannote/voice-activity-detection",
        "tags": [
          "pyannote-audio",
          "pyannote",
          "pyannote-audio-pipeline",
          "audio",
          "voice",
          "speech",
          "speaker",
          "voice-activity-detection",
          "automatic-speech-recognition",
          "dataset:ami",
          "dataset:dihard",
          "dataset:voxconverse",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4987990,
          "likes_total": 211,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "pyannote/语音活动检测模型是一款专门用于检测音频录音中语音片段的工具。其主要目的是识别人类语音存在与静默或背景噪声的时段。核心能力包括使用深度学习算法对语音区域进行精确的时间分割。关键优势在于对AMI、DIHARD和VoxConverse等挑战性数据集的高准确性，以及在各种声学条件下的稳健性能。典型应用场景包括语音识别系统的预处理、会议转录、播客编辑和说话人日志管道。该MIT许可的模型针对英语语音检测进行了优化，适用于需要精确语音边",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "asr",
          "speaker_separation",
          "vector_retrieval",
          "graph_augmented_reco",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The pyannote/voice-activity-detection model is a specialized tool for detecting speech segments in audio recordings. Its primary purpose is to identify when human speech is present versus silence or background noise. Core capabilities include precise temporal segmentation of speech regions using deep learning algorithms. Key strengths include high accuracy on challenging datasets like AMI, DIHARD, and VoxConverse, and robust performance across diverse acoustic conditions. Typical use cases involve preprocessing for speech recognition systems, meeting transcription, podcast editing, and speaker diarization pipelines. The MIT-licensed model is optimized for English speech detection.",
        "summary_zh": "pyannote/语音活动检测模型是一款专门用于检测音频录音中语音片段的工具。其主要目的是识别人类语音存在与静默或背景噪声的时段。核心能力包括使用深度学习算法对语音区域进行精确的时间分割。关键优势在于对AMI、DIHARD和VoxConverse等挑战性数据集的高准确性，以及在各种声学条件下的稳健性能。典型应用场景包括语音识别系统的预处理、会议转录、播客编辑和说话人日志管道。该MIT许可的模型针对英语语音检测进行了优化，适用于需要精确语音边",
        "summary_es": "The pyannote/voice-activity-detection model is a specialized tool for detecting speech segments in audio recordings. Its primary purpose is to identify when human speech is present versus silence or background noise. Core capabilities include precise temporal segmentation of speech regions using deep learning algorithms. Key strengths include high accuracy on challenging datasets like AMI, DIHARD, and VoxConverse, and robust performance across diverse acoustic conditions. Typical use cases involve preprocessing for speech recognition systems, meeting transcription, podcast editing, and speaker diarization pipelines. The MIT-licensed model is optimized for English speech detection."
      }
    ],
    "tts": [
      {
        "id": "coqui/XTTS-v2",
        "source": "hf",
        "name": "XTTS-v2",
        "url": "https://huggingface.co/coqui/XTTS-v2",
        "tags": [
          "coqui",
          "text-to-speech",
          "license:other",
          "region:us"
        ],
        "stats": {
          "downloads_total": 5562668,
          "likes_total": 3059,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "XTTS-v2是由Coqui开发的多语言文本转语音模型，能够根据文本输入生成自然流畅的语音。其核心能力在于仅需6秒语音样本即可实现高质量的声音克隆，支持英语、西班牙语、法语、中文等13种语言。主要优势包括跨语言声音复制、情感语调控制和高效率推理。典型应用场景涵盖有声读物播讲、语音助手定制、内容本地化以及视障用户的辅助工具。该模型在声音相似度和自然度方面达到业界领先水平，特别适合需要个性化语音合成的商业和教育应用，同",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "tts"
        ],
        "summary_en": "XTTS-v2 is a multilingual text-to-speech model developed by Coqui that generates natural speech from text input. Its core capability lies in producing high-quality audio using just a 6-second voice sample for cloning, supporting 13 languages including English, Spanish, French, and Mandarin. Key strengths include cross-lingual voice cloning, emotional tone control, and efficient inference. Typical use cases encompass audiobook narration, voice assistant customization, content localization, and accessibility tools for visually impaired users. The model achieves state-of-the-art performance in voice similarity and naturalness.",
        "summary_zh": "XTTS-v2是由Coqui开发的多语言文本转语音模型，能够根据文本输入生成自然流畅的语音。其核心能力在于仅需6秒语音样本即可实现高质量的声音克隆，支持英语、西班牙语、法语、中文等13种语言。主要优势包括跨语言声音复制、情感语调控制和高效率推理。典型应用场景涵盖有声读物播讲、语音助手定制、内容本地化以及视障用户的辅助工具。该模型在声音相似度和自然度方面达到业界领先水平，特别适合需要个性化语音合成的商业和教育应用，同",
        "summary_es": "XTTS-v2 is a multilingual text-to-speech model developed by Coqui that generates natural speech from text input. Its core capability lies in producing high-quality audio using just a 6-second voice sample for cloning, supporting 13 languages including English, Spanish, French, and Mandarin. Key strengths include cross-lingual voice cloning, emotional tone control, and efficient inference. Typical use cases encompass audiobook narration, voice assistant customization, content localization, and accessibility tools for visually impaired users. The model achieves state-of-the-art performance in voice similarity and naturalness."
      }
    ],
    "slu": [],
    "speaker_separation": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      },
      {
        "id": "pyannote/wespeaker-voxceleb-resnet34-LM",
        "source": "hf",
        "name": "wespeaker-voxceleb-resnet34-LM",
        "url": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "wespeaker",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-recognition",
          "speaker-verification",
          "speaker-identification",
          "speaker-embedding",
          "dataset:voxceleb",
          "license:cc-by-4.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 17739550,
          "likes_total": 76,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "wespeaker-voxceleb-resnet34-LM 是一个专门用于说话人识别的模型，其核心功能是从音频中提取具有区分性的说话人嵌入向量。该模型采用基于 VoxCeleb 数据集训练的 ResNet-34 架构，并集成了语言模型评分机制以提升验证准确性。主要优势包括在不同声学条件下的稳定表现和高效的说话人区分能力。典型应用场景涵盖说话人验证、身份识别任务以及需要区分不同说话人的语音日记系统。该模型能够处理原始语音信号并生成固定维度的表征向量，适用于需要精确说话人辨别的",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "graph_augmented_reco",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech signals to generate fixed-dimensional vectors that uniquely represent speaker characteristics. The model employs a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved verification accuracy. Key strengths include robust performance across diverse acoustic conditions and effective speaker discrimination. Typical use cases encompass speaker verification, identification tasks, and speaker diarization systems where distinguishing between different speakers is essential.",
        "summary_zh": "wespeaker-voxceleb-resnet34-LM 是一个专门用于说话人识别的模型，其核心功能是从音频中提取具有区分性的说话人嵌入向量。该模型采用基于 VoxCeleb 数据集训练的 ResNet-34 架构，并集成了语言模型评分机制以提升验证准确性。主要优势包括在不同声学条件下的稳定表现和高效的说话人区分能力。典型应用场景涵盖说话人验证、身份识别任务以及需要区分不同说话人的语音日记系统。该模型能够处理原始语音信号并生成固定维度的表征向量，适用于需要精确说话人辨别的",
        "summary_es": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech signals to generate fixed-dimensional vectors that uniquely represent speaker characteristics. The model employs a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved verification accuracy. Key strengths include robust performance across diverse acoustic conditions and effective speaker discrimination. Typical use cases encompass speaker verification, identification tasks, and speaker diarization systems where distinguishing between different speakers is essential."
      }
    ],
    "noise_separation": [],
    "full_duplex_dialogue": [],
    "avsr": [],
    "multimodal_dialogue_system": [],
    "lightweight_multimodal_model": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      }
    ],
    "gnn": [],
    "kg_construction": [],
    "kg_reasoning": [],
    "general_recommendation": [],
    "vertical_recommendation": [],
    "vector_retrieval": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      },
      {
        "id": "pyannote/wespeaker-voxceleb-resnet34-LM",
        "source": "hf",
        "name": "wespeaker-voxceleb-resnet34-LM",
        "url": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "wespeaker",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-recognition",
          "speaker-verification",
          "speaker-identification",
          "speaker-embedding",
          "dataset:voxceleb",
          "license:cc-by-4.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 17739550,
          "likes_total": 76,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "wespeaker-voxceleb-resnet34-LM 是一个专门用于说话人识别的模型，其核心功能是从音频中提取具有区分性的说话人嵌入向量。该模型采用基于 VoxCeleb 数据集训练的 ResNet-34 架构，并集成了语言模型评分机制以提升验证准确性。主要优势包括在不同声学条件下的稳定表现和高效的说话人区分能力。典型应用场景涵盖说话人验证、身份识别任务以及需要区分不同说话人的语音日记系统。该模型能够处理原始语音信号并生成固定维度的表征向量，适用于需要精确说话人辨别的",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "graph_augmented_reco",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech signals to generate fixed-dimensional vectors that uniquely represent speaker characteristics. The model employs a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved verification accuracy. Key strengths include robust performance across diverse acoustic conditions and effective speaker discrimination. Typical use cases encompass speaker verification, identification tasks, and speaker diarization systems where distinguishing between different speakers is essential.",
        "summary_zh": "wespeaker-voxceleb-resnet34-LM 是一个专门用于说话人识别的模型，其核心功能是从音频中提取具有区分性的说话人嵌入向量。该模型采用基于 VoxCeleb 数据集训练的 ResNet-34 架构，并集成了语言模型评分机制以提升验证准确性。主要优势包括在不同声学条件下的稳定表现和高效的说话人区分能力。典型应用场景涵盖说话人验证、身份识别任务以及需要区分不同说话人的语音日记系统。该模型能够处理原始语音信号并生成固定维度的表征向量，适用于需要精确说话人辨别的",
        "summary_es": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech signals to generate fixed-dimensional vectors that uniquely represent speaker characteristics. The model employs a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved verification accuracy. Key strengths include robust performance across diverse acoustic conditions and effective speaker discrimination. Typical use cases encompass speaker verification, identification tasks, and speaker diarization systems where distinguishing between different speakers is essential."
      }
    ],
    "vector_db_optimization": [],
    "metric_learning": [],
    "contrastive_learning": [],
    "ltr": [
      {
        "id": "Bingsu/adetailer",
        "source": "hf",
        "name": "adetailer",
        "url": "https://huggingface.co/Bingsu/adetailer",
        "tags": [
          "ultralytics",
          "pytorch",
          "dataset:wider_face",
          "dataset:skytnt/anime-segmentation",
          "doi:10.57967/hf/3633",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15085828,
          "likes_total": 617,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括对象检测、分割和修复，以提升图像质量。优势在于处理动漫风格内容和真实世界面部图像，利用了 anime-segmentation 和 WiderFace 等数据集。典型应用场景涉及对生成或真实图像进行后处理，以优化细节、去除伪影或增强特定区域，无需人工干预。该工具基于 PyTorch 构建，采用 Apache 2.0 开源许可证。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "ltr",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection, segmentation, and inpainting to improve image quality. Strengths lie in handling anime-style content and real-world facial images, leveraging datasets like anime-segmentation and WiderFace. Typical use cases involve post-processing generated or real images to refine details, remove artifacts, or enhance specific areas without manual intervention. The tool is built on PyTorch and is open-source under Apache 2.0 license.",
        "summary_zh": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括对象检测、分割和修复，以提升图像质量。优势在于处理动漫风格内容和真实世界面部图像，利用了 anime-segmentation 和 WiderFace 等数据集。典型应用场景涉及对生成或真实图像进行后处理，以优化细节、去除伪影或增强特定区域，无需人工干预。该工具基于 PyTorch 构建，采用 Apache 2.0 开源许可证。",
        "summary_es": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection, segmentation, and inpainting to improve image quality. Strengths lie in handling anime-style content and real-world facial images, leveraging datasets like anime-segmentation and WiderFace. Typical use cases involve post-processing generated or real images to refine details, remove artifacts, or enhance specific areas without manual intervention. The tool is built on PyTorch and is open-source under Apache 2.0 license."
      }
    ],
    "neural_retrieval": [
      {
        "id": "colbert-ir/colbertv2.0",
        "source": "hf",
        "name": "colbertv2.0",
        "url": "https://huggingface.co/colbert-ir/colbertv2.0",
        "tags": [
          "transformers",
          "pytorch",
          "onnx",
          "safetensors",
          "bert",
          "ColBERT",
          "en",
          "arxiv:2004.12832",
          "arxiv:2007.00814",
          "arxiv:2101.00436",
          "arxiv:2112.01488",
          "arxiv:2205.09707",
          "license:mit",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6840796,
          "likes_total": 286,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "ColBERTv2.0是一种基于神经网络的检索模型，通过延迟交互机制提升信息检索效果。它使用BERT分别编码查询和文档，然后通过令牌级嵌入之间的高效MaxSim操作计算相关性。该方法在效果和可扩展性之间取得平衡，支持十亿级规模搜索。核心优势包括高准确性、与现有索引兼容性以及高效的GPU利用率。典型应用场景涵盖网络搜索、企业文档检索和问答系统，适用于需要精确匹配和大规模处理的关键任务。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "neural_retrieval"
        ],
        "summary_en": "ColBERTv2.0 is a neural retrieval model that enhances information retrieval through late interaction. It encodes queries and documents separately using BERT, then computes relevance via efficient MaxSim operations between their token-level embeddings. This approach balances effectiveness with scalability, supporting billion-scale searches. Core strengths include high accuracy, compatibility with existing indexes, and efficient GPU utilization. Typical use cases involve web search, enterprise document retrieval, and question-answering systems where precise matching and scalability are critical.",
        "summary_zh": "ColBERTv2.0是一种基于神经网络的检索模型，通过延迟交互机制提升信息检索效果。它使用BERT分别编码查询和文档，然后通过令牌级嵌入之间的高效MaxSim操作计算相关性。该方法在效果和可扩展性之间取得平衡，支持十亿级规模搜索。核心优势包括高准确性、与现有索引兼容性以及高效的GPU利用率。典型应用场景涵盖网络搜索、企业文档检索和问答系统，适用于需要精确匹配和大规模处理的关键任务。",
        "summary_es": "ColBERTv2.0 is a neural retrieval model that enhances information retrieval through late interaction. It encodes queries and documents separately using BERT, then computes relevance via efficient MaxSim operations between their token-level embeddings. This approach balances effectiveness with scalability, supporting billion-scale searches. Core strengths include high accuracy, compatibility with existing indexes, and efficient GPU utilization. Typical use cases involve web search, enterprise document retrieval, and question-answering systems where precise matching and scalability are critical."
      }
    ],
    "graph_augmented_reco": [
      {
        "id": "pyannote/wespeaker-voxceleb-resnet34-LM",
        "source": "hf",
        "name": "wespeaker-voxceleb-resnet34-LM",
        "url": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "wespeaker",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-recognition",
          "speaker-verification",
          "speaker-identification",
          "speaker-embedding",
          "dataset:voxceleb",
          "license:cc-by-4.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 17739550,
          "likes_total": 76,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "wespeaker-voxceleb-resnet34-LM 是一个专门用于说话人识别的模型，其核心功能是从音频中提取具有区分性的说话人嵌入向量。该模型采用基于 VoxCeleb 数据集训练的 ResNet-34 架构，并集成了语言模型评分机制以提升验证准确性。主要优势包括在不同声学条件下的稳定表现和高效的说话人区分能力。典型应用场景涵盖说话人验证、身份识别任务以及需要区分不同说话人的语音日记系统。该模型能够处理原始语音信号并生成固定维度的表征向量，适用于需要精确说话人辨别的",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "graph_augmented_reco",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech signals to generate fixed-dimensional vectors that uniquely represent speaker characteristics. The model employs a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved verification accuracy. Key strengths include robust performance across diverse acoustic conditions and effective speaker discrimination. Typical use cases encompass speaker verification, identification tasks, and speaker diarization systems where distinguishing between different speakers is essential.",
        "summary_zh": "wespeaker-voxceleb-resnet34-LM 是一个专门用于说话人识别的模型，其核心功能是从音频中提取具有区分性的说话人嵌入向量。该模型采用基于 VoxCeleb 数据集训练的 ResNet-34 架构，并集成了语言模型评分机制以提升验证准确性。主要优势包括在不同声学条件下的稳定表现和高效的说话人区分能力。典型应用场景涵盖说话人验证、身份识别任务以及需要区分不同说话人的语音日记系统。该模型能够处理原始语音信号并生成固定维度的表征向量，适用于需要精确说话人辨别的",
        "summary_es": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech signals to generate fixed-dimensional vectors that uniquely represent speaker characteristics. The model employs a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved verification accuracy. Key strengths include robust performance across diverse acoustic conditions and effective speaker discrimination. Typical use cases encompass speaker verification, identification tasks, and speaker diarization systems where distinguishing between different speakers is essential."
      },
      {
        "id": "pyannote/speaker-diarization-3.1",
        "source": "hf",
        "name": "speaker-diarization-3.1",
        "url": "https://huggingface.co/pyannote/speaker-diarization-3.1",
        "tags": [
          "pyannote-audio",
          "pyannote",
          "pyannote-audio-pipeline",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "automatic-speech-recognition",
          "arxiv:2111.14448",
          "arxiv:2012.01477",
          "license:mit",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 16673680,
          "likes_total": 1169,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote speaker-diarization-3.1 是一个音频处理流程，专门用于自动分割和标记录音中的说话人身份。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该系统擅长处理多人对话场景，具有高时间精度，特别适用于会议转录、广播监控和对话分析等典型用例。基于 arXiv:2012.01477 和 arXiv:2111.14448 的研究成果，这款 MIT 许可的工具能够处理音频并输出结构化的时间线，准确识别“谁在何时说话”，在各种声学条件下均表现出稳定性能。该工具通过先进的深度学习技术实现对连续语音流的实",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "asr",
          "speaker_separation",
          "vector_retrieval",
          "graph_augmented_reco",
          "auto_evaluation_models",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote speaker-diarization-3.1 is an audio processing pipeline that automatically segments and labels speaker identities in audio recordings. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The system excels at handling multi-speaker conversations with high temporal precision, making it particularly strong for meeting transcriptions, broadcast monitoring, and conversational analysis. Based on research from arXiv:2012.01477 and arXiv:2111.14448, this MIT-licensed tool processes audio to output structured timelines identifying 'who spoke when' with robust performance across various acoustic conditions.",
        "summary_zh": "Pyannote speaker-diarization-3.1 是一个音频处理流程，专门用于自动分割和标记录音中的说话人身份。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该系统擅长处理多人对话场景，具有高时间精度，特别适用于会议转录、广播监控和对话分析等典型用例。基于 arXiv:2012.01477 和 arXiv:2111.14448 的研究成果，这款 MIT 许可的工具能够处理音频并输出结构化的时间线，准确识别“谁在何时说话”，在各种声学条件下均表现出稳定性能。该工具通过先进的深度学习技术实现对连续语音流的实",
        "summary_es": "Pyannote speaker-diarization-3.1 is an audio processing pipeline that automatically segments and labels speaker identities in audio recordings. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The system excels at handling multi-speaker conversations with high temporal precision, making it particularly strong for meeting transcriptions, broadcast monitoring, and conversational analysis. Based on research from arXiv:2012.01477 and arXiv:2111.14448, this MIT-licensed tool processes audio to output structured timelines identifying 'who spoke when' with robust performance across various acoustic conditions."
      }
    ],
    "model_compression": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      }
    ],
    "model_quantization": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      }
    ],
    "model_distillation": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      }
    ],
    "compilation_optimization": [],
    "inference_acceleration": [
      {
        "id": "sentence-transformers/all-MiniLM-L6-v2",
        "source": "hf",
        "name": "all-MiniLM-L6-v2",
        "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
        "tags": [
          "sentence-transformers",
          "pytorch",
          "tf",
          "rust",
          "onnx",
          "safetensors",
          "openvino",
          "bert",
          "feature-extraction",
          "sentence-similarity",
          "transformers",
          "en",
          "dataset:s2orc",
          "dataset:flax-sentence-embeddings/stackexchange_xml",
          "dataset:ms_marco",
          "dataset:gooaq",
          "dataset:yahoo_answers_topics",
          "dataset:code_search_net",
          "dataset:search_qa",
          "dataset:eli5",
          "dataset:snli",
          "dataset:multi_nli",
          "dataset:wikihow",
          "dataset:natural_questions",
          "dataset:trivia_qa",
          "dataset:embedding-data/sentence-compression",
          "dataset:embedding-data/flickr30k-captions",
          "dataset:embedding-data/altlex",
          "dataset:embedding-data/simple-wiki",
          "dataset:embedding-data/QQP",
          "dataset:embedding-data/SPECTER",
          "dataset:embedding-data/PAQ_pairs",
          "dataset:embedding-data/WikiAnswers",
          "arxiv:1904.06472",
          "arxiv:2102.07033",
          "arxiv:2104.08727",
          "arxiv:1704.05179",
          "arxiv:1810.09305",
          "license:apache-2.0",
          "autotrain_compatible",
          "text-embeddings-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 91098802,
          "likes_total": 3932,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "all-MiniLM-L6-v2 是一个紧凑型句子转换模型，专门用于高效生成文本嵌入向量。该模型将文本映射到384维向量空间，支持语义相似度计算。核心功能包括句子嵌入生成、语义搜索和文本聚类。主要优势在于模型体积小（80MB）、推理速度快，同时保持竞争力的性能表现。典型应用场景涵盖信息检索、重复内容检测、推荐系统和文本分类任务。模型采用知识蒸馏技术，在QQP、WikiAnswers、MS MARCO等多个数据集上进行训练，适用于需要平衡效率与准确性的自然语言处理应用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "inference_acceleration"
        ],
        "summary_en": "The all-MiniLM-L6-v2 model is a compact sentence transformer designed for efficient text embedding generation. It maps text to 384-dimensional vectors, enabling semantic similarity comparisons. Core capabilities include sentence embeddings, semantic search, and clustering. Strengths lie in its small size (80MB) and fast inference while maintaining competitive performance. Typical use cases encompass information retrieval, duplicate detection, recommendation systems, and text classification tasks. The model was trained using knowledge distillation techniques on diverse datasets including QQP, WikiAnswers, and MS MARCO.",
        "summary_zh": "all-MiniLM-L6-v2 是一个紧凑型句子转换模型，专门用于高效生成文本嵌入向量。该模型将文本映射到384维向量空间，支持语义相似度计算。核心功能包括句子嵌入生成、语义搜索和文本聚类。主要优势在于模型体积小（80MB）、推理速度快，同时保持竞争力的性能表现。典型应用场景涵盖信息检索、重复内容检测、推荐系统和文本分类任务。模型采用知识蒸馏技术，在QQP、WikiAnswers、MS MARCO等多个数据集上进行训练，适用于需要平衡效率与准确性的自然语言处理应用。",
        "summary_es": "The all-MiniLM-L6-v2 model is a compact sentence transformer designed for efficient text embedding generation. It maps text to 384-dimensional vectors, enabling semantic similarity comparisons. Core capabilities include sentence embeddings, semantic search, and clustering. Strengths lie in its small size (80MB) and fast inference while maintaining competitive performance. Typical use cases encompass information retrieval, duplicate detection, recommendation systems, and text classification tasks. The model was trained using knowledge distillation techniques on diverse datasets including QQP, WikiAnswers, and MS MARCO."
      },
      {
        "id": "sentence-transformers/all-mpnet-base-v2",
        "source": "hf",
        "name": "all-mpnet-base-v2",
        "url": "https://huggingface.co/sentence-transformers/all-mpnet-base-v2",
        "tags": [
          "sentence-transformers",
          "pytorch",
          "onnx",
          "safetensors",
          "openvino",
          "mpnet",
          "fill-mask",
          "feature-extraction",
          "sentence-similarity",
          "transformers",
          "text-embeddings-inference",
          "en",
          "dataset:s2orc",
          "dataset:flax-sentence-embeddings/stackexchange_xml",
          "dataset:ms_marco",
          "dataset:gooaq",
          "dataset:yahoo_answers_topics",
          "dataset:code_search_net",
          "dataset:search_qa",
          "dataset:eli5",
          "dataset:snli",
          "dataset:multi_nli",
          "dataset:wikihow",
          "dataset:natural_questions",
          "dataset:trivia_qa",
          "dataset:embedding-data/sentence-compression",
          "dataset:embedding-data/flickr30k-captions",
          "dataset:embedding-data/altlex",
          "dataset:embedding-data/simple-wiki",
          "dataset:embedding-data/QQP",
          "dataset:embedding-data/SPECTER",
          "dataset:embedding-data/PAQ_pairs",
          "dataset:embedding-data/WikiAnswers",
          "arxiv:1904.06472",
          "arxiv:2102.07033",
          "arxiv:2104.08727",
          "arxiv:1704.05179",
          "arxiv:1810.09305",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 16899677,
          "likes_total": 1163,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "all-mpnet-base-v2是基于MPNet架构的句子嵌入模型，旨在将文本转换为密集向量表示。其核心能力是生成高质量的语义相似性任务嵌入向量，优势在于在语义文本相似性基准测试中表现优异，并能高效处理多样化文本类型。典型应用场景包括语义搜索、信息检索、相似文档聚类和复述检测。该模型在QQP、WikiAnswers和MS MARCO等多个数据集上进行训练，以增强跨领域的泛化能力。训练结合了对比学习目标，优化了嵌入空间中的语义关系捕获。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "inference_acceleration"
        ],
        "summary_en": "all-mpnet-base-v2 is a sentence embedding model based on MPNet architecture, designed to convert text into dense vector representations. Its core capability is generating high-quality embeddings for semantic similarity tasks. Strengths include superior performance on semantic textual similarity benchmarks and efficient handling of diverse text types. Typical use cases encompass semantic search, information retrieval, clustering similar documents, and paraphrase detection. The model was trained on multiple datasets including QQP, WikiAnswers, and MS MARCO to enhance generalization across domains.",
        "summary_zh": "all-mpnet-base-v2是基于MPNet架构的句子嵌入模型，旨在将文本转换为密集向量表示。其核心能力是生成高质量的语义相似性任务嵌入向量，优势在于在语义文本相似性基准测试中表现优异，并能高效处理多样化文本类型。典型应用场景包括语义搜索、信息检索、相似文档聚类和复述检测。该模型在QQP、WikiAnswers和MS MARCO等多个数据集上进行训练，以增强跨领域的泛化能力。训练结合了对比学习目标，优化了嵌入空间中的语义关系捕获。",
        "summary_es": "all-mpnet-base-v2 is a sentence embedding model based on MPNet architecture, designed to convert text into dense vector representations. Its core capability is generating high-quality embeddings for semantic similarity tasks. Strengths include superior performance on semantic textual similarity benchmarks and efficient handling of diverse text types. Typical use cases encompass semantic search, information retrieval, clustering similar documents, and paraphrase detection. The model was trained on multiple datasets including QQP, WikiAnswers, and MS MARCO to enhance generalization across domains."
      }
    ],
    "federated_learning": [],
    "privacy_computing": [],
    "adversarial_attack": [],
    "adversarial_defense": [],
    "red_teaming": [],
    "content_moderation": [],
    "auto_evaluation_models": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 99881324,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems.",
        "summary_zh": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      }
    ],
    "edge_hw_sw_co_design": [],
    "xai": [],
    "ai_ethics_risk_assessment": [],
    "training_data_anonymization": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "google-bert/bert-base-uncased",
        "source": "hf",
        "name": "bert-base-uncased",
        "url": "https://huggingface.co/google-bert/bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "coreml",
          "onnx",
          "safetensors",
          "bert",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1810.04805",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 55204102,
          "likes_total": 2417,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "BERT-base-uncased是基于Transformer架构的英语预训练语言模型，在Wikipedia和BookCorpus语料上通过掩码语言建模和下一句预测任务进行训练。该模型采用双向编码器设计，能够深度理解词语在上下文中的语义关系。其主要优势在于强大的语境理解能力和通用性，支持文本分类、命名实体识别、问答系统、情感分析等多种自然语言处理任务。作为uncased版本，模型不区分字母大小写，适用于大多数不需要大小写敏感处理的通用文本分析场景。该模型已成为许多下游NLP应用的基础架构。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus. It employs masked language modeling and next sentence prediction to develop deep bidirectional contextual representations. The model excels at understanding word meanings in context and capturing semantic relationships. Its primary applications include text classification, named entity recognition, question answering, and sentiment analysis. As an uncased model, it treats uppercase and lowercase letters identically, making it suitable for general text processing tasks where case sensitivity is not required.",
        "summary_zh": "BERT-base-uncased是基于Transformer架构的英语预训练语言模型，在Wikipedia和BookCorpus语料上通过掩码语言建模和下一句预测任务进行训练。该模型采用双向编码器设计，能够深度理解词语在上下文中的语义关系。其主要优势在于强大的语境理解能力和通用性，支持文本分类、命名实体识别、问答系统、情感分析等多种自然语言处理任务。作为uncased版本，模型不区分字母大小写，适用于大多数不需要大小写敏感处理的通用文本分析场景。该模型已成为许多下游NLP应用的基础架构。",
        "summary_es": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus. It employs masked language modeling and next sentence prediction to develop deep bidirectional contextual representations. The model excels at understanding word meanings in context and capturing semantic relationships. Its primary applications include text classification, named entity recognition, question answering, and sentiment analysis. As an uncased model, it treats uppercase and lowercase letters identically, making it suitable for general text processing tasks where case sensitivity is not required."
      }
    ],
    "training_data_copyright": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "google-bert/bert-base-uncased",
        "source": "hf",
        "name": "bert-base-uncased",
        "url": "https://huggingface.co/google-bert/bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "coreml",
          "onnx",
          "safetensors",
          "bert",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1810.04805",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 55204102,
          "likes_total": 2417,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "BERT-base-uncased是基于Transformer架构的英语预训练语言模型，在Wikipedia和BookCorpus语料上通过掩码语言建模和下一句预测任务进行训练。该模型采用双向编码器设计，能够深度理解词语在上下文中的语义关系。其主要优势在于强大的语境理解能力和通用性，支持文本分类、命名实体识别、问答系统、情感分析等多种自然语言处理任务。作为uncased版本，模型不区分字母大小写，适用于大多数不需要大小写敏感处理的通用文本分析场景。该模型已成为许多下游NLP应用的基础架构。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus. It employs masked language modeling and next sentence prediction to develop deep bidirectional contextual representations. The model excels at understanding word meanings in context and capturing semantic relationships. Its primary applications include text classification, named entity recognition, question answering, and sentiment analysis. As an uncased model, it treats uppercase and lowercase letters identically, making it suitable for general text processing tasks where case sensitivity is not required.",
        "summary_zh": "BERT-base-uncased是基于Transformer架构的英语预训练语言模型，在Wikipedia和BookCorpus语料上通过掩码语言建模和下一句预测任务进行训练。该模型采用双向编码器设计，能够深度理解词语在上下文中的语义关系。其主要优势在于强大的语境理解能力和通用性，支持文本分类、命名实体识别、问答系统、情感分析等多种自然语言处理任务。作为uncased版本，模型不区分字母大小写，适用于大多数不需要大小写敏感处理的通用文本分析场景。该模型已成为许多下游NLP应用的基础架构。",
        "summary_es": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus. It employs masked language modeling and next sentence prediction to develop deep bidirectional contextual representations. The model excels at understanding word meanings in context and capturing semantic relationships. Its primary applications include text classification, named entity recognition, question answering, and sentiment analysis. As an uncased model, it treats uppercase and lowercase letters identically, making it suitable for general text processing tasks where case sensitivity is not required."
      }
    ],
    "model_monitoring": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      }
    ],
    "model_iterative_update": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      }
    ],
    "fluid_simulation": [],
    "material_design": [],
    "drug_molecule_prediction": [],
    "robotic_vision": [
      {
        "id": "openai/clip-vit-base-patch32",
        "source": "hf",
        "name": "clip-vit-base-patch32",
        "url": "https://huggingface.co/openai/clip-vit-base-patch32",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "clip",
          "zero-shot-image-classification",
          "vision",
          "arxiv:2103.00020",
          "arxiv:1908.04913",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15040512,
          "likes_total": 770,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "CLIP-ViT-Base-Patch32是OpenAI开发的多模态人工智能模型，旨在连接视觉与语言理解。该模型采用Vision Transformer架构，使用32x32像素块处理图像，并结合文本编码器分析自然语言描述。通过在4亿张图像-文本对上训练，模型能够从自然语言监督中学习视觉概念。核心功能是零样本图像分类，无需针对特定任务进行训练即可根据文本描述对图像进行分类。主要优势包括强大的跨任务泛化能力和对分布变化的鲁棒性。典型应用场景涵盖内容审核、图像搜索、视觉问答等视觉理解任务，能",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "robotic_vision"
        ],
        "summary_en": "CLIP-ViT-Base-Patch32 is a multimodal AI model developed by OpenAI that connects vision and language. It uses a Vision Transformer (ViT) architecture with 32x32 pixel patches to encode images and a text encoder to process natural language descriptions. The model learns visual concepts from natural language supervision by training on 400 million image-text pairs. Its core capability is zero-shot image classification, where it can classify images into categories described in text without task-specific training. Key strengths include strong generalization across diverse visual tasks and robustness to distribution shifts. Typical use cases include content moderation, image search, and visual question answering.",
        "summary_zh": "CLIP-ViT-Base-Patch32是OpenAI开发的多模态人工智能模型，旨在连接视觉与语言理解。该模型采用Vision Transformer架构，使用32x32像素块处理图像，并结合文本编码器分析自然语言描述。通过在4亿张图像-文本对上训练，模型能够从自然语言监督中学习视觉概念。核心功能是零样本图像分类，无需针对特定任务进行训练即可根据文本描述对图像进行分类。主要优势包括强大的跨任务泛化能力和对分布变化的鲁棒性。典型应用场景涵盖内容审核、图像搜索、视觉问答等视觉理解任务，能",
        "summary_es": "CLIP-ViT-Base-Patch32 is a multimodal AI model developed by OpenAI that connects vision and language. It uses a Vision Transformer (ViT) architecture with 32x32 pixel patches to encode images and a text encoder to process natural language descriptions. The model learns visual concepts from natural language supervision by training on 400 million image-text pairs. Its core capability is zero-shot image classification, where it can classify images into categories described in text without task-specific training. Key strengths include strong generalization across diverse visual tasks and robustness to distribution shifts. Typical use cases include content moderation, image search, and visual question answering."
      },
      {
        "id": "openai/clip-vit-large-patch14",
        "source": "hf",
        "name": "clip-vit-large-patch14",
        "url": "https://huggingface.co/openai/clip-vit-large-patch14",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "safetensors",
          "clip",
          "zero-shot-image-classification",
          "vision",
          "arxiv:2103.00020",
          "arxiv:1908.04913",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7853847,
          "likes_total": 1866,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "CLIP-ViT-Large-Patch14是OpenAI开发的多模态人工智能模型，旨在实现视觉与语言的联合理解。其主要目的是通过同时处理图像和文本来实现零样本图像分类，无需针对特定任务进行训练。核心能力包括生成图像和文本的联合嵌入表示，使不同模态之间能够直接比较。关键优势在于其能够处理多样化的视觉概念，并通过自然语言监督实现强大的泛化能力。典型应用场景包括基于内容的图像检索、视觉问答、图像描述生成以及多模态搜索。该模型采用Vision Transformer架构，使用较",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "robotic_vision"
        ],
        "summary_en": "CLIP-ViT-Large-Patch14 is a multimodal AI model developed by OpenAI that connects vision and language. Its primary purpose is to understand images and text simultaneously, enabling zero-shot image classification without task-specific training. Core capabilities include generating joint embeddings for images and text, allowing direct comparison across modalities. Key strengths are its versatility across diverse visual concepts and robust generalization from natural language supervision. Typical use cases encompass content-based image retrieval, visual question answering, image captioning, and multimodal search applications. The model uses a Vision Transformer architecture with large patch size for efficient image processing.",
        "summary_zh": "CLIP-ViT-Large-Patch14是OpenAI开发的多模态人工智能模型，旨在实现视觉与语言的联合理解。其主要目的是通过同时处理图像和文本来实现零样本图像分类，无需针对特定任务进行训练。核心能力包括生成图像和文本的联合嵌入表示，使不同模态之间能够直接比较。关键优势在于其能够处理多样化的视觉概念，并通过自然语言监督实现强大的泛化能力。典型应用场景包括基于内容的图像检索、视觉问答、图像描述生成以及多模态搜索。该模型采用Vision Transformer架构，使用较",
        "summary_es": "CLIP-ViT-Large-Patch14 is a multimodal AI model developed by OpenAI that connects vision and language. Its primary purpose is to understand images and text simultaneously, enabling zero-shot image classification without task-specific training. Core capabilities include generating joint embeddings for images and text, allowing direct comparison across modalities. Key strengths are its versatility across diverse visual concepts and robust generalization from natural language supervision. Typical use cases encompass content-based image retrieval, visual question answering, image captioning, and multimodal search applications. The model uses a Vision Transformer architecture with large patch size for efficient image processing."
      }
    ],
    "robot_motion_planning": [],
    "robot_environment_interaction": [],
    "molecular_generation": [
      {
        "id": "openai-community/gpt2",
        "source": "hf",
        "name": "gpt2",
        "url": "https://huggingface.co/openai-community/gpt2",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "tflite",
          "rust",
          "onnx",
          "safetensors",
          "gpt2",
          "text-generation",
          "exbert",
          "en",
          "doi:10.57967/hf/0039",
          "license:mit",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 11902438,
          "likes_total": 2955,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本生成任务。其核心能力是通过输入提示预测后续文本内容，实现连贯的文本延续。主要优势包括在零样本设置下无需特定任务训练即可生成高质量文本、能够跨多个领域生成类人文本内容以及开源可访问性。典型应用场景涵盖创意写作辅助、对话系统原型开发、内容摘要生成和代码自动补全。该模型在开放式文本补全方面表现突出，能够在长文本输出中保持上下文相关性，同时支",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "GPT-2 is a transformer-based language model developed by OpenAI for text generation tasks. Its core capability involves predicting subsequent text based on input prompts, enabling coherent continuation of text. Key strengths include strong performance in zero-shot settings without task-specific training, generating human-like text across diverse domains, and open-source availability. Typical use cases encompass creative writing assistance, conversational AI prototyping, content summarization, and code generation. The model demonstrates particular effectiveness in open-ended text completion while maintaining contextual relevance throughout extended outputs.",
        "summary_zh": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本生成任务。其核心能力是通过输入提示预测后续文本内容，实现连贯的文本延续。主要优势包括在零样本设置下无需特定任务训练即可生成高质量文本、能够跨多个领域生成类人文本内容以及开源可访问性。典型应用场景涵盖创意写作辅助、对话系统原型开发、内容摘要生成和代码自动补全。该模型在开放式文本补全方面表现突出，能够在长文本输出中保持上下文相关性，同时支",
        "summary_es": "GPT-2 is a transformer-based language model developed by OpenAI for text generation tasks. Its core capability involves predicting subsequent text based on input prompts, enabling coherent continuation of text. Key strengths include strong performance in zero-shot settings without task-specific training, generating human-like text across diverse domains, and open-source availability. Typical use cases encompass creative writing assistance, conversational AI prototyping, content summarization, and code generation. The model demonstrates particular effectiveness in open-ended text completion while maintaining contextual relevance throughout extended outputs."
      },
      {
        "id": "facebook/opt-125m",
        "source": "hf",
        "name": "opt-125m",
        "url": "https://huggingface.co/facebook/opt-125m",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "opt",
          "text-generation",
          "en",
          "arxiv:2205.01068",
          "arxiv:2005.14165",
          "license:other",
          "autotrain_compatible",
          "text-generation-inference",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8602519,
          "likes_total": 218,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "OPT-125M是Meta AI开发的Open Pre-trained Transformer系列中的1.25亿参数仅解码器变压器语言模型，作为较大OPT模型的缩小版本，主要用于研究和实验目的。该模型能够根据输入提示生成连贯文本，支持多种自然语言处理任务。其主要优势包括面向研究用途的开放可访问性、支持多种框架（PyTorch、TensorFlow、JAX）的兼容性以及高效的推理能力。典型应用场景涵盖文本生成实验、变压器架构的教育演示、小规模语言模型性能基准测试，以及作为理解更大规模语言模型工作原理的入门工具。该模型特别适合计",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "OPT-125M is a 125 million parameter decoder-only transformer language model developed by Meta AI as part of the Open Pre-trained Transformer series. It serves as a smaller-scale version of larger OPT models for research and experimentation. The model generates coherent text based on input prompts and supports various natural language processing tasks. Its primary strengths include open accessibility for research purposes, compatibility with multiple frameworks (PyTorch, TensorFlow, JAX), and efficient inference capabilities. Typical use cases encompass text generation experiments, educational demonstrations of transformer architectures, and benchmarking smaller-scale language model performance.",
        "summary_zh": "OPT-125M是Meta AI开发的Open Pre-trained Transformer系列中的1.25亿参数仅解码器变压器语言模型，作为较大OPT模型的缩小版本，主要用于研究和实验目的。该模型能够根据输入提示生成连贯文本，支持多种自然语言处理任务。其主要优势包括面向研究用途的开放可访问性、支持多种框架（PyTorch、TensorFlow、JAX）的兼容性以及高效的推理能力。典型应用场景涵盖文本生成实验、变压器架构的教育演示、小规模语言模型性能基准测试，以及作为理解更大规模语言模型工作原理的入门工具。该模型特别适合计",
        "summary_es": "OPT-125M is a 125 million parameter decoder-only transformer language model developed by Meta AI as part of the Open Pre-trained Transformer series. It serves as a smaller-scale version of larger OPT models for research and experimentation. The model generates coherent text based on input prompts and supports various natural language processing tasks. Its primary strengths include open accessibility for research purposes, compatibility with multiple frameworks (PyTorch, TensorFlow, JAX), and efficient inference capabilities. Typical use cases encompass text generation experiments, educational demonstrations of transformer architectures, and benchmarking smaller-scale language model performance."
      }
    ],
    "bioinformatics_analysis": [],
    "time_series_forecasting": [
      {
        "id": "Datadog/Toto-Open-Base-1.0",
        "source": "hf",
        "name": "Toto-Open-Base-1.0",
        "url": "https://huggingface.co/Datadog/Toto-Open-Base-1.0",
        "tags": [
          "transformers",
          "safetensors",
          "time-series-forecasting",
          "foundation models",
          "pretrained models",
          "time series foundation models",
          "time series",
          "time-series",
          "timeseries",
          "forecasting",
          "observability",
          "dataset:Salesforce/GiftEvalPretrain",
          "dataset:autogluon/chronos_datasets",
          "arxiv:2505.14766",
          "license:apache-2.0",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6133487,
          "likes_total": 112,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Toto-Open-Base-1.0是由Datadog开发的预训练时间序列基础模型，专为预测应用设计。其核心能力在于分析时间模式以预测各类数据集的未来数值。主要优势包括与多种端点的兼容性以及在时间序列数据上的稳健性能。典型应用场景涵盖可观测性监控、资源规划和业务指标异常检测。该模型采用Transformer架构，基于Salesforce/GiftEvalPretrain和Autogluon/chronos_datasets等数据集训练而成。作为基础模型，它为运营智能领域的专业预测任务提供支持，适用于需要时序分析的商业和技术环境。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_forecasting",
          "time_series_anomaly_detection"
        ],
        "summary_en": "Toto-Open-Base-1.0 is a pretrained time series foundation model developed by Datadog for forecasting applications. Its core capability involves analyzing temporal patterns to predict future values across diverse datasets. Key strengths include compatibility with various endpoints and robust performance on time-series data. Typical use cases span observability monitoring, resource planning, and anomaly detection in business metrics. The model leverages transformer architecture and is trained on datasets like Salesforce/GiftEvalPretrain and Autogluon/chronos_datasets. It serves as a base model for specialized forecasting tasks in operational intelligence contexts.",
        "summary_zh": "Toto-Open-Base-1.0是由Datadog开发的预训练时间序列基础模型，专为预测应用设计。其核心能力在于分析时间模式以预测各类数据集的未来数值。主要优势包括与多种端点的兼容性以及在时间序列数据上的稳健性能。典型应用场景涵盖可观测性监控、资源规划和业务指标异常检测。该模型采用Transformer架构，基于Salesforce/GiftEvalPretrain和Autogluon/chronos_datasets等数据集训练而成。作为基础模型，它为运营智能领域的专业预测任务提供支持，适用于需要时序分析的商业和技术环境。",
        "summary_es": "Toto-Open-Base-1.0 is a pretrained time series foundation model developed by Datadog for forecasting applications. Its core capability involves analyzing temporal patterns to predict future values across diverse datasets. Key strengths include compatibility with various endpoints and robust performance on time-series data. Typical use cases span observability monitoring, resource planning, and anomaly detection in business metrics. The model leverages transformer architecture and is trained on datasets like Salesforce/GiftEvalPretrain and Autogluon/chronos_datasets. It serves as a base model for specialized forecasting tasks in operational intelligence contexts."
      },
      {
        "id": "thuml/sundial-base-128m",
        "source": "hf",
        "name": "sundial-base-128m",
        "url": "https://huggingface.co/thuml/sundial-base-128m",
        "tags": [
          "safetensors",
          "sundial",
          "time series",
          "time-series",
          "forecasting",
          "foundation models",
          "pretrained models",
          "generative models",
          "time series foundation models",
          "time-series-forecasting",
          "custom_code",
          "dataset:thuml/UTSD",
          "dataset:Salesforce/lotsa_data",
          "dataset:autogluon/chronos_datasets",
          "arxiv:2502.00816",
          "arxiv:2403.07815",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 5884615,
          "likes_total": 47,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Sundial-base-128m是一个拥有1.28亿参数的时间序列基础模型，专门设计用于预测应用。该模型作为预训练的生成模型，可通过微调适应各种时间序列预测任务。它利用多个数据集进行训练，包括Salesforce/lotsa_data、AutoGluon/chronos_datasets和thuml/UTSD。核心能力包括处理多样化的时间序列模式并生成未来预测。主要优势在于高效的参数利用和基础模型的灵活性。典型应用场景涵盖需求预测、金融预测和工业监控等领域，其中历史模式可用于推断未来趋势。该模型基于Apache 2.0许可证发布，支持安全张量格式。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "lightweight_visual_model",
          "code_generation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_forecasting",
          "time_series_anomaly_detection"
        ],
        "summary_en": "Sundial-base-128m is a 128-million parameter time series foundation model designed for forecasting applications. It serves as a pretrained generative model that can be fine-tuned for various time series prediction tasks. The model leverages multiple datasets including Salesforce/lotsa_data, AutoGluon/chronos_datasets, and thuml/UTSD for training. Its core capabilities include handling diverse time series patterns and generating future predictions. Strengths include efficient parameter usage and foundation model flexibility. Typical use cases span demand forecasting, financial prediction, and industrial monitoring applications where historical patterns inform future trends.",
        "summary_zh": "Sundial-base-128m是一个拥有1.28亿参数的时间序列基础模型，专门设计用于预测应用。该模型作为预训练的生成模型，可通过微调适应各种时间序列预测任务。它利用多个数据集进行训练，包括Salesforce/lotsa_data、AutoGluon/chronos_datasets和thuml/UTSD。核心能力包括处理多样化的时间序列模式并生成未来预测。主要优势在于高效的参数利用和基础模型的灵活性。典型应用场景涵盖需求预测、金融预测和工业监控等领域，其中历史模式可用于推断未来趋势。该模型基于Apache 2.0许可证发布，支持安全张量格式。",
        "summary_es": "Sundial-base-128m is a 128-million parameter time series foundation model designed for forecasting applications. It serves as a pretrained generative model that can be fine-tuned for various time series prediction tasks. The model leverages multiple datasets including Salesforce/lotsa_data, AutoGluon/chronos_datasets, and thuml/UTSD for training. Its core capabilities include handling diverse time series patterns and generating future predictions. Strengths include efficient parameter usage and foundation model flexibility. Typical use cases span demand forecasting, financial prediction, and industrial monitoring applications where historical patterns inform future trends."
      }
    ],
    "time_series_anomaly_detection": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 99881324,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems.",
        "summary_zh": "该项目专门用于图像分类，旨在检测图像中的NSFW（不适宜工作场所）内容。基于Vision Transformer架构构建，能够高精度识别露骨或不适宜的视觉材料。模型服务于内容审核目的，帮助平台自动过滤不合适图像。核心能力包括对图像进行安全或NSFW的二元分类。优势在于对各种图像类型的稳健性能以及与标准部署工具的兼容性。典型应用场景涉及社交媒体平台、消息应用和内容托管服务实施自动化内容过滤系统，有效维护网络环境的适宜性，防止不当内容的传",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content in images. Built on Vision Transformer architecture, it can identify explicit or inappropriate visual material with high accuracy. The model serves content moderation purposes, helping platforms automatically filter unsuitable images. Its core capabilities include binary classification of images as safe or NSFW. Strengths include robust performance across diverse image types and compatibility with standard deployment tools. Typical use cases involve social media platforms, messaging apps, and content hosting services implementing automated content filtering systems."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 61525079,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications.",
        "summary_zh": "该项目基于谷歌Vision Transformer（ViT-base-patch16-224-in21k）架构，在FairFace人脸数据集上进行微调，专门用于从面部图像中识别年龄组别。其核心功能是处理224×224像素输入图像，并输出年龄分类概率。主要优势包括ViT模型卓越的视觉识别能力，以及FairFace数据集涵盖多种族群的平衡样本分布，有助于减少人口统计偏差。典型应用场景涉及人口统计分析、适龄内容过滤系统开发，以及计算机视觉领域算法公平性的学术研究。该项目通过微调预训练模型，实现了针对年龄属性的高效分类，适用于需要 demographic 特",
        "summary_es": "This project fine-tunes Google's Vision Transformer (ViT-base-patch16-224-in21k) on the FairFace dataset for age classification from facial images. Its primary purpose is to predict age groups in photographs with improved fairness across diverse demographics. Core capabilities include processing 224x224 pixel images and outputting age category probabilities. Key strengths are the ViT architecture's strong visual recognition performance and the FairFace dataset's balanced racial/ethnic representation. Typical use cases span demographic analysis, age-appropriate content filtering, and research on algorithmic fairness in computer vision applications."
      }
    ],
    "radar_understanding": [],
    "lidar_understanding": [],
    "low_resource_medical_ai": [],
    "low_resource_voice_assistant": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18441470,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该模型擅长识别说话人开始/结束讲话的时间点，并能检测多人同时说话的情况。典型应用场景包括会议转录、广播内容监控和呼叫中心分析。这款 MIT 许可的模型通过处理音频输出带有说话人标签的精确时间片段，为需要准确分离多说话人环境中语音的应用提供支持，特别适用于",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at identifying when speakers begin/end talking and detecting multiple speakers talking simultaneously. Typical use cases include meeting transcription, broadcast monitoring, and call center analytics. The MIT-licensed model processes audio to output precise temporal segments with speaker labels, supporting applications requiring accurate speaker separation in multi-speaker environments."
      },
      {
        "id": "pyannote/wespeaker-voxceleb-resnet34-LM",
        "source": "hf",
        "name": "wespeaker-voxceleb-resnet34-LM",
        "url": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "wespeaker",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-recognition",
          "speaker-verification",
          "speaker-identification",
          "speaker-embedding",
          "dataset:voxceleb",
          "license:cc-by-4.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 17739550,
          "likes_total": 76,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T15:07:54.533Z",
        "added_at": "2025-09-27",
        "summary": "wespeaker-voxceleb-resnet34-LM 是一个专门用于说话人识别的模型，其核心功能是从音频中提取具有区分性的说话人嵌入向量。该模型采用基于 VoxCeleb 数据集训练的 ResNet-34 架构，并集成了语言模型评分机制以提升验证准确性。主要优势包括在不同声学条件下的稳定表现和高效的说话人区分能力。典型应用场景涵盖说话人验证、身份识别任务以及需要区分不同说话人的语音日记系统。该模型能够处理原始语音信号并生成固定维度的表征向量，适用于需要精确说话人辨别的",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996743292767944,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "graph_augmented_reco",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech signals to generate fixed-dimensional vectors that uniquely represent speaker characteristics. The model employs a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved verification accuracy. Key strengths include robust performance across diverse acoustic conditions and effective speaker discrimination. Typical use cases encompass speaker verification, identification tasks, and speaker diarization systems where distinguishing between different speakers is essential.",
        "summary_zh": "wespeaker-voxceleb-resnet34-LM 是一个专门用于说话人识别的模型，其核心功能是从音频中提取具有区分性的说话人嵌入向量。该模型采用基于 VoxCeleb 数据集训练的 ResNet-34 架构，并集成了语言模型评分机制以提升验证准确性。主要优势包括在不同声学条件下的稳定表现和高效的说话人区分能力。典型应用场景涵盖说话人验证、身份识别任务以及需要区分不同说话人的语音日记系统。该模型能够处理原始语音信号并生成固定维度的表征向量，适用于需要精确说话人辨别的",
        "summary_es": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech signals to generate fixed-dimensional vectors that uniquely represent speaker characteristics. The model employs a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved verification accuracy. Key strengths include robust performance across diverse acoustic conditions and effective speaker discrimination. Typical use cases encompass speaker verification, identification tasks, and speaker diarization systems where distinguishing between different speakers is essential."
      }
    ],
    "ar_vr_interaction": [],
    "multimodal_temporal_fusion": [],
    "robot_dialogue_logic": []
  }
}