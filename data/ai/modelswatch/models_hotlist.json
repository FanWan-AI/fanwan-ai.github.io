{
  "version": 1,
  "updated_at": "2025-09-27T07:27:08.054Z",
  "by_category": {
    "image_classification": [
      {
        "id": "timm/mobilenetv3_small_100.lamb_in1k",
        "source": "hf",
        "name": "mobilenetv3_small_100.lamb_in1k",
        "url": "https://huggingface.co/timm/mobilenetv3_small_100.lamb_in1k",
        "tags": [
          "timm",
          "pytorch",
          "safetensors",
          "image-classification",
          "transformers",
          "dataset:imagenet-1k",
          "arxiv:2110.00476",
          "arxiv:1905.02244",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 126091014,
          "likes_total": 38,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "MobileNetV3-Small-100是一种轻量级卷积神经网络，专为移动和嵌入式视觉应用优化设计。该模型采用倒残差结构，结合压缩激励模块和硬swish激活函数，实现高效计算。通过神经架构搜索技术优化，在准确性和速度之间取得良好平衡。基于ImageNet-1K数据集使用LAMB优化器进行预训练，擅长图像分类任务，具有极低计算需求。典型应用场景包括移动应用程序、边缘设备和实时视觉系统，特别适用于对功耗效率要求严格的场合。该架构代表了MobileNet系列的进化，在计算效率指标上表现优",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification"
        ],
        "summary_en": "MobileNetV3-Small-100 is a lightweight convolutional neural network optimized for mobile and embedded vision applications. It uses inverted residual blocks with squeeze-and-excitation modules and hard-swish activations for efficient computation. The model achieves a balance between accuracy and speed through neural architecture search optimization. Pre-trained on ImageNet-1K using the LAMB optimizer, it excels in image classification tasks with minimal computational requirements. Typical use cases include mobile apps, edge devices, and real-time vision systems where power efficiency is critical. The architecture represents an evolution of MobileNet designs with improved performance-per-compute metrics.",
        "summary_zh": "MobileNetV3-Small-100是一种轻量级卷积神经网络，专为移动和嵌入式视觉应用优化设计。该模型采用倒残差结构，结合压缩激励模块和硬swish激活函数，实现高效计算。通过神经架构搜索技术优化，在准确性和速度之间取得良好平衡。基于ImageNet-1K数据集使用LAMB优化器进行预训练，擅长图像分类任务，具有极低计算需求。典型应用场景包括移动应用程序、边缘设备和实时视觉系统，特别适用于对功耗效率要求严格的场合。该架构代表了MobileNet系列的进化，在计算效率指标上表现优",
        "summary_es": "MobileNetV3-Small-100 is a lightweight convolutional neural network optimized for mobile and embedded vision applications. It uses inverted residual blocks with squeeze-and-excitation modules and hard-swish activations for efficient computation. The model achieves a balance between accuracy and speed through neural architecture search optimization. Pre-trained on ImageNet-1K using the LAMB optimizer, it excels in image classification tasks with minimal computational requirements. Typical use cases include mobile apps, edge devices, and real-time vision systems where power efficiency is critical. The architecture represents an evolution of MobileNet designs with improved performance-per-compute metrics."
      },
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 98988953,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards.",
        "summary_zh": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards."
      },
      {
        "id": "trpakov/vit-face-expression",
        "source": "hf",
        "name": "vit-face-expression",
        "url": "https://huggingface.co/trpakov/vit-face-expression",
        "tags": [
          "transformers",
          "pytorch",
          "onnx",
          "safetensors",
          "vit",
          "image-classification",
          "doi:10.57967/hf/2289",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8789751,
          "likes_total": 80,
          "downloads_7d": 559936,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "vit-face-expression项目是一个基于Vision Transformer架构的面部表情分类模型，专门用于从面部图像中识别七种基本情绪（愤怒、厌恶、恐惧、快乐、中性、悲伤、惊讶）。其核心能力在于利用Transformer技术实现高精度的表情分类，主要优势包括在基准数据集上的优异表现、高效的图像处理能力以及PyTorch和ONNX格式的灵活部署支持。典型应用场景涵盖人机交互研究、情感计算系统开发、心理学实验的自动化表情分析，以及需要实时情绪识别的智能应用系统。该项目提供完整的模型权重和转换工具，便",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.13169832496182968,
        "task_keys": [
          "image_classification",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models"
        ],
        "summary_en": "The vit-face-expression project is a Vision Transformer model designed for facial expression classification. Its core capability is identifying seven basic emotions (anger, disgust, fear, happiness, neutrality, sadness, surprise) from facial images. Key strengths include high accuracy on benchmark datasets and efficient processing using transformer architecture. Typical use cases encompass human-computer interaction research, emotion-aware applications, and psychological studies requiring automated facial expression analysis. The model is implemented in PyTorch with ONNX support for deployment flexibility.",
        "summary_zh": "vit-face-expression项目是一个基于Vision Transformer架构的面部表情分类模型，专门用于从面部图像中识别七种基本情绪（愤怒、厌恶、恐惧、快乐、中性、悲伤、惊讶）。其核心能力在于利用Transformer技术实现高精度的表情分类，主要优势包括在基准数据集上的优异表现、高效的图像处理能力以及PyTorch和ONNX格式的灵活部署支持。典型应用场景涵盖人机交互研究、情感计算系统开发、心理学实验的自动化表情分析，以及需要实时情绪识别的智能应用系统。该项目提供完整的模型权重和转换工具，便",
        "summary_es": "The vit-face-expression project is a Vision Transformer model designed for facial expression classification. Its core capability is identifying seven basic emotions (anger, disgust, fear, happiness, neutrality, sadness, surprise) from facial images. Key strengths include high accuracy on benchmark datasets and efficient processing using transformer architecture. Typical use cases encompass human-computer interaction research, emotion-aware applications, and psychological studies requiring automated facial expression analysis. The model is implemented in PyTorch with ONNX support for deployment flexibility."
      }
    ],
    "object_detection": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 98988953,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards.",
        "summary_zh": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "tech4humans/yolov8s-signature-detector",
        "source": "hf",
        "name": "yolov8s-signature-detector",
        "url": "https://huggingface.co/tech4humans/yolov8s-signature-detector",
        "tags": [
          "ultralytics",
          "tensorboard",
          "onnx",
          "object-detection",
          "signature-detection",
          "yolo",
          "yolov8",
          "pytorch",
          "dataset:tech4humans/signature-detection",
          "base_model:Ultralytics/YOLOv8",
          "base_model:quantized:Ultralytics/YOLOv8",
          "license:agpl-3.0",
          "model-index",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 41151738,
          "likes_total": 44,
          "downloads_7d": 0,
          "likes_7d": 5
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "yolov8s-signature-detector 是一个专门用于检测文档中签名的目标检测模型。该模型基于Ultralytics YOLOv8架构构建，采用量化版本以实现高效推理。它使用专门的签名检测数据集进行训练，支持ONNX和PyTorch等多种部署格式。主要优势在于能够准确定位文档图像中的签名区域，适用于文档处理自动化、数字归档和验证系统等场景。模型经过生产环境优化，具有端点兼容性和完整的监控工具支持，特别适合需要批量处理文档签名的企业应用。该技术可帮助机构实现签名检测的自动化流程，",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.2660648042182313,
        "task_keys": [
          "object_detection"
        ],
        "summary_en": "The yolov8s-signature-detector is an object detection model specifically designed to identify signatures in documents. Built on Ultralytics YOLOv8 architecture, it uses a quantized version for efficient inference. The model was trained on a specialized signature detection dataset and supports multiple deployment formats including ONNX and PyTorch. Its primary strength lies in accurate signature localization within document images, making it suitable for document processing automation, digital archiving, and verification systems. The model is optimized for production use with endpoints compatibility and comprehensive monitoring tools.",
        "summary_zh": "yolov8s-signature-detector 是一个专门用于检测文档中签名的目标检测模型。该模型基于Ultralytics YOLOv8架构构建，采用量化版本以实现高效推理。它使用专门的签名检测数据集进行训练，支持ONNX和PyTorch等多种部署格式。主要优势在于能够准确定位文档图像中的签名区域，适用于文档处理自动化、数字归档和验证系统等场景。模型经过生产环境优化，具有端点兼容性和完整的监控工具支持，特别适合需要批量处理文档签名的企业应用。该技术可帮助机构实现签名检测的自动化流程，",
        "summary_es": "The yolov8s-signature-detector is an object detection model specifically designed to identify signatures in documents. Built on Ultralytics YOLOv8 architecture, it uses a quantized version for efficient inference. The model was trained on a specialized signature detection dataset and supports multiple deployment formats including ONNX and PyTorch. Its primary strength lies in accurate signature localization within document images, making it suitable for document processing automation, digital archiving, and verification systems. The model is optimized for production use with endpoints compatibility and comprehensive monitoring tools."
      }
    ],
    "semantic_segmentation": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "Bingsu/adetailer",
        "source": "hf",
        "name": "adetailer",
        "url": "https://huggingface.co/Bingsu/adetailer",
        "tags": [
          "ultralytics",
          "pytorch",
          "dataset:wider_face",
          "dataset:skytnt/anime-segmentation",
          "doi:10.57967/hf/3633",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15202018,
          "likes_total": 617,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括使用预训练模型进行物体检测、精确区域识别的分割技术，以及自动修复和细化处理。主要优势在于处理低分辨率或模糊输入、改善面部特征，并能有效处理动漫风格内容。典型应用场景包括AI生成流程中的图像预处理、照片修复，以及无需手动干预即可增强数字艺术作品中的特定元素。该工具基于PyTorch开发，整合了动漫分割和面部检测数据",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "ltr",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection using pre-trained models, segmentation for precise area identification, and automated inpainting/refinement. Key strengths are handling low-resolution or blurry inputs, improving facial features, and working with anime-style content. Typical use cases involve image preprocessing for AI generation pipelines, photo restoration, and enhancing specific elements in digital artwork without manual intervention.",
        "summary_zh": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括使用预训练模型进行物体检测、精确区域识别的分割技术，以及自动修复和细化处理。主要优势在于处理低分辨率或模糊输入、改善面部特征，并能有效处理动漫风格内容。典型应用场景包括AI生成流程中的图像预处理、照片修复，以及无需手动干预即可增强数字艺术作品中的特定元素。该工具基于PyTorch开发，整合了动漫分割和面部检测数据",
        "summary_es": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection using pre-trained models, segmentation for precise area identification, and automated inpainting/refinement. Key strengths are handling low-resolution or blurry inputs, improving facial features, and working with anime-style content. Typical use cases involve image preprocessing for AI generation pipelines, photo restoration, and enhancing specific elements in digital artwork without manual intervention."
      },
      {
        "id": "pyannote/segmentation",
        "source": "hf",
        "name": "segmentation",
        "url": "https://huggingface.co/pyannote/segmentation",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "arxiv:2104.04045",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6591894,
          "likes_total": 641,
          "downloads_7d": 0,
          "likes_7d": 3
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.32376268146237963,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition.",
        "summary_zh": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "summary_es": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition."
      }
    ],
    "instance_segmentation": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "Bingsu/adetailer",
        "source": "hf",
        "name": "adetailer",
        "url": "https://huggingface.co/Bingsu/adetailer",
        "tags": [
          "ultralytics",
          "pytorch",
          "dataset:wider_face",
          "dataset:skytnt/anime-segmentation",
          "doi:10.57967/hf/3633",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15202018,
          "likes_total": 617,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括使用预训练模型进行物体检测、精确区域识别的分割技术，以及自动修复和细化处理。主要优势在于处理低分辨率或模糊输入、改善面部特征，并能有效处理动漫风格内容。典型应用场景包括AI生成流程中的图像预处理、照片修复，以及无需手动干预即可增强数字艺术作品中的特定元素。该工具基于PyTorch开发，整合了动漫分割和面部检测数据",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "ltr",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection using pre-trained models, segmentation for precise area identification, and automated inpainting/refinement. Key strengths are handling low-resolution or blurry inputs, improving facial features, and working with anime-style content. Typical use cases involve image preprocessing for AI generation pipelines, photo restoration, and enhancing specific elements in digital artwork without manual intervention.",
        "summary_zh": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括使用预训练模型进行物体检测、精确区域识别的分割技术，以及自动修复和细化处理。主要优势在于处理低分辨率或模糊输入、改善面部特征，并能有效处理动漫风格内容。典型应用场景包括AI生成流程中的图像预处理、照片修复，以及无需手动干预即可增强数字艺术作品中的特定元素。该工具基于PyTorch开发，整合了动漫分割和面部检测数据",
        "summary_es": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection using pre-trained models, segmentation for precise area identification, and automated inpainting/refinement. Key strengths are handling low-resolution or blurry inputs, improving facial features, and working with anime-style content. Typical use cases involve image preprocessing for AI generation pipelines, photo restoration, and enhancing specific elements in digital artwork without manual intervention."
      },
      {
        "id": "pyannote/segmentation",
        "source": "hf",
        "name": "segmentation",
        "url": "https://huggingface.co/pyannote/segmentation",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "arxiv:2104.04045",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6591894,
          "likes_total": 641,
          "downloads_7d": 0,
          "likes_7d": 3
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.32376268146237963,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition.",
        "summary_zh": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "summary_es": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition."
      }
    ],
    "panoptic_segmentation": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "Bingsu/adetailer",
        "source": "hf",
        "name": "adetailer",
        "url": "https://huggingface.co/Bingsu/adetailer",
        "tags": [
          "ultralytics",
          "pytorch",
          "dataset:wider_face",
          "dataset:skytnt/anime-segmentation",
          "doi:10.57967/hf/3633",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15202018,
          "likes_total": 617,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括使用预训练模型进行物体检测、精确区域识别的分割技术，以及自动修复和细化处理。主要优势在于处理低分辨率或模糊输入、改善面部特征，并能有效处理动漫风格内容。典型应用场景包括AI生成流程中的图像预处理、照片修复，以及无需手动干预即可增强数字艺术作品中的特定元素。该工具基于PyTorch开发，整合了动漫分割和面部检测数据",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "ltr",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection using pre-trained models, segmentation for precise area identification, and automated inpainting/refinement. Key strengths are handling low-resolution or blurry inputs, improving facial features, and working with anime-style content. Typical use cases involve image preprocessing for AI generation pipelines, photo restoration, and enhancing specific elements in digital artwork without manual intervention.",
        "summary_zh": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括使用预训练模型进行物体检测、精确区域识别的分割技术，以及自动修复和细化处理。主要优势在于处理低分辨率或模糊输入、改善面部特征，并能有效处理动漫风格内容。典型应用场景包括AI生成流程中的图像预处理、照片修复，以及无需手动干预即可增强数字艺术作品中的特定元素。该工具基于PyTorch开发，整合了动漫分割和面部检测数据",
        "summary_es": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection using pre-trained models, segmentation for precise area identification, and automated inpainting/refinement. Key strengths are handling low-resolution or blurry inputs, improving facial features, and working with anime-style content. Typical use cases involve image preprocessing for AI generation pipelines, photo restoration, and enhancing specific elements in digital artwork without manual intervention."
      },
      {
        "id": "pyannote/segmentation",
        "source": "hf",
        "name": "segmentation",
        "url": "https://huggingface.co/pyannote/segmentation",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "arxiv:2104.04045",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6591894,
          "likes_total": 641,
          "downloads_7d": 0,
          "likes_7d": 3
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.32376268146237963,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition.",
        "summary_zh": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "summary_es": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition."
      }
    ],
    "text_to_image": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 98988953,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards.",
        "summary_zh": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "text_to_video": [
      {
        "id": "openai-community/gpt2",
        "source": "hf",
        "name": "gpt2",
        "url": "https://huggingface.co/openai-community/gpt2",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "tflite",
          "rust",
          "onnx",
          "safetensors",
          "gpt2",
          "text-generation",
          "exbert",
          "en",
          "doi:10.57967/hf/0039",
          "license:mit",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 12038015,
          "likes_total": 2955,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本预测和内容创作。其核心功能包括根据输入提示生成连贯的后续文本、完成句子结构以及创作多样化内容。该模型的优势在于无需微调即可实现高质量文本生成、支持多种写作风格且完全开源。典型应用场景涵盖创意写作辅助、聊天机器人对话生成、文本摘要制作以及自然语言处理研究。GPT-2特别擅长保持长文本的上下文一致性，在维持主题连贯性方面表现突出，为开发者和研究者提",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "GPT-2 is a transformer-based language model developed by OpenAI for text generation. Its primary purpose is to predict subsequent text based on input, enabling coherent continuation of prompts. Core capabilities include generating human-like text, completing sentences, and creating content across various domains. Strengths lie in its strong performance without fine-tuning, versatility across writing styles, and open-source availability. Typical use cases encompass creative writing assistance, chatbot responses, content summarization, and language modeling research. The model demonstrates particular effectiveness in maintaining contextual coherence throughout extended text passages.",
        "summary_zh": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本预测和内容创作。其核心功能包括根据输入提示生成连贯的后续文本、完成句子结构以及创作多样化内容。该模型的优势在于无需微调即可实现高质量文本生成、支持多种写作风格且完全开源。典型应用场景涵盖创意写作辅助、聊天机器人对话生成、文本摘要制作以及自然语言处理研究。GPT-2特别擅长保持长文本的上下文一致性，在维持主题连贯性方面表现突出，为开发者和研究者提",
        "summary_es": "GPT-2 is a transformer-based language model developed by OpenAI for text generation. Its primary purpose is to predict subsequent text based on input, enabling coherent continuation of prompts. Core capabilities include generating human-like text, completing sentences, and creating content across various domains. Strengths lie in its strong performance without fine-tuning, versatility across writing styles, and open-source availability. Typical use cases encompass creative writing assistance, chatbot responses, content summarization, and language modeling research. The model demonstrates particular effectiveness in maintaining contextual coherence throughout extended text passages."
      },
      {
        "id": "omni-research/Tarsier2-Recap-7b",
        "source": "hf",
        "name": "Tarsier2-Recap-7b",
        "url": "https://huggingface.co/omni-research/Tarsier2-Recap-7b",
        "tags": [
          "safetensors",
          "video LLM",
          "arxiv:2501.07888",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 10123337,
          "likes_total": 19,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Tarsier2-Recap-7b 是一个拥有70亿参数的专业视频语言模型，专门用于视频内容的理解与摘要生成。其核心能力在于处理视频序列并生成全面的文本摘要，能够从视觉信息中提取关键内容并转化为连贯的叙述。该模型在视频内容分析、教育视频处理和媒体监控等场景中表现优异，特别擅长将复杂的视觉信息转化为结构化的文本输出。基于arXiv:2501.07888的研究成果，采用safetensors格式和Apache 2.0开源协议，主要服务于需要将视频内容自动转换为详细文字摘要的专业应用需求。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_video",
          "llm_pretraining"
        ],
        "summary_en": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for comprehensive video understanding and summarization. Its core capability involves processing video content to generate detailed textual recaps. The model excels at extracting key information from visual sequences and converting it into coherent narratives. Typical use cases include automated video summarization for content analysis, educational video processing, and media monitoring applications. Based on research from arXiv:2501.07888, it employs safetensors format and operates under Apache 2.0 license, serving as a specialized tool for converting visual information into structured textual outputs.",
        "summary_zh": "Tarsier2-Recap-7b 是一个拥有70亿参数的专业视频语言模型，专门用于视频内容的理解与摘要生成。其核心能力在于处理视频序列并生成全面的文本摘要，能够从视觉信息中提取关键内容并转化为连贯的叙述。该模型在视频内容分析、教育视频处理和媒体监控等场景中表现优异，特别擅长将复杂的视觉信息转化为结构化的文本输出。基于arXiv:2501.07888的研究成果，采用safetensors格式和Apache 2.0开源协议，主要服务于需要将视频内容自动转换为详细文字摘要的专业应用需求。",
        "summary_es": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for comprehensive video understanding and summarization. Its core capability involves processing video content to generate detailed textual recaps. The model excels at extracting key information from visual sequences and converting it into coherent narratives. Typical use cases include automated video summarization for content analysis, educational video processing, and media monitoring applications. Based on research from arXiv:2501.07888, it employs safetensors format and operates under Apache 2.0 license, serving as a specialized tool for converting visual information into structured textual outputs."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "3d_reconstruction": [],
    "nerf": [],
    "super_resolution": [],
    "denoising": [],
    "restoration": [],
    "medical_image_processing": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 98988953,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards.",
        "summary_zh": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "trpakov/vit-face-expression",
        "source": "hf",
        "name": "vit-face-expression",
        "url": "https://huggingface.co/trpakov/vit-face-expression",
        "tags": [
          "transformers",
          "pytorch",
          "onnx",
          "safetensors",
          "vit",
          "image-classification",
          "doi:10.57967/hf/2289",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8789751,
          "likes_total": 80,
          "downloads_7d": 559936,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "vit-face-expression项目是一个基于Vision Transformer架构的面部表情分类模型，专门用于从面部图像中识别七种基本情绪（愤怒、厌恶、恐惧、快乐、中性、悲伤、惊讶）。其核心能力在于利用Transformer技术实现高精度的表情分类，主要优势包括在基准数据集上的优异表现、高效的图像处理能力以及PyTorch和ONNX格式的灵活部署支持。典型应用场景涵盖人机交互研究、情感计算系统开发、心理学实验的自动化表情分析，以及需要实时情绪识别的智能应用系统。该项目提供完整的模型权重和转换工具，便",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.13169832496182968,
        "task_keys": [
          "image_classification",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models"
        ],
        "summary_en": "The vit-face-expression project is a Vision Transformer model designed for facial expression classification. Its core capability is identifying seven basic emotions (anger, disgust, fear, happiness, neutrality, sadness, surprise) from facial images. Key strengths include high accuracy on benchmark datasets and efficient processing using transformer architecture. Typical use cases encompass human-computer interaction research, emotion-aware applications, and psychological studies requiring automated facial expression analysis. The model is implemented in PyTorch with ONNX support for deployment flexibility.",
        "summary_zh": "vit-face-expression项目是一个基于Vision Transformer架构的面部表情分类模型，专门用于从面部图像中识别七种基本情绪（愤怒、厌恶、恐惧、快乐、中性、悲伤、惊讶）。其核心能力在于利用Transformer技术实现高精度的表情分类，主要优势包括在基准数据集上的优异表现、高效的图像处理能力以及PyTorch和ONNX格式的灵活部署支持。典型应用场景涵盖人机交互研究、情感计算系统开发、心理学实验的自动化表情分析，以及需要实时情绪识别的智能应用系统。该项目提供完整的模型权重和转换工具，便",
        "summary_es": "The vit-face-expression project is a Vision Transformer model designed for facial expression classification. Its core capability is identifying seven basic emotions (anger, disgust, fear, happiness, neutrality, sadness, surprise) from facial images. Key strengths include high accuracy on benchmark datasets and efficient processing using transformer architecture. Typical use cases encompass human-computer interaction research, emotion-aware applications, and psychological studies requiring automated facial expression analysis. The model is implemented in PyTorch with ONNX support for deployment flexibility."
      }
    ],
    "remote_sensing_image_processing": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 98988953,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards.",
        "summary_zh": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "trpakov/vit-face-expression",
        "source": "hf",
        "name": "vit-face-expression",
        "url": "https://huggingface.co/trpakov/vit-face-expression",
        "tags": [
          "transformers",
          "pytorch",
          "onnx",
          "safetensors",
          "vit",
          "image-classification",
          "doi:10.57967/hf/2289",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8789751,
          "likes_total": 80,
          "downloads_7d": 559936,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "vit-face-expression项目是一个基于Vision Transformer架构的面部表情分类模型，专门用于从面部图像中识别七种基本情绪（愤怒、厌恶、恐惧、快乐、中性、悲伤、惊讶）。其核心能力在于利用Transformer技术实现高精度的表情分类，主要优势包括在基准数据集上的优异表现、高效的图像处理能力以及PyTorch和ONNX格式的灵活部署支持。典型应用场景涵盖人机交互研究、情感计算系统开发、心理学实验的自动化表情分析，以及需要实时情绪识别的智能应用系统。该项目提供完整的模型权重和转换工具，便",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.13169832496182968,
        "task_keys": [
          "image_classification",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models"
        ],
        "summary_en": "The vit-face-expression project is a Vision Transformer model designed for facial expression classification. Its core capability is identifying seven basic emotions (anger, disgust, fear, happiness, neutrality, sadness, surprise) from facial images. Key strengths include high accuracy on benchmark datasets and efficient processing using transformer architecture. Typical use cases encompass human-computer interaction research, emotion-aware applications, and psychological studies requiring automated facial expression analysis. The model is implemented in PyTorch with ONNX support for deployment flexibility.",
        "summary_zh": "vit-face-expression项目是一个基于Vision Transformer架构的面部表情分类模型，专门用于从面部图像中识别七种基本情绪（愤怒、厌恶、恐惧、快乐、中性、悲伤、惊讶）。其核心能力在于利用Transformer技术实现高精度的表情分类，主要优势包括在基准数据集上的优异表现、高效的图像处理能力以及PyTorch和ONNX格式的灵活部署支持。典型应用场景涵盖人机交互研究、情感计算系统开发、心理学实验的自动化表情分析，以及需要实时情绪识别的智能应用系统。该项目提供完整的模型权重和转换工具，便",
        "summary_es": "The vit-face-expression project is a Vision Transformer model designed for facial expression classification. Its core capability is identifying seven basic emotions (anger, disgust, fear, happiness, neutrality, sadness, surprise) from facial images. Key strengths include high accuracy on benchmark datasets and efficient processing using transformer architecture. Typical use cases encompass human-computer interaction research, emotion-aware applications, and psychological studies requiring automated facial expression analysis. The model is implemented in PyTorch with ONNX support for deployment flexibility."
      }
    ],
    "vqa": [
      {
        "id": "Qwen/Qwen2.5-VL-7B-Instruct",
        "source": "hf",
        "name": "Qwen2.5-VL-7B-Instruct",
        "url": "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "qwen2_5_vl",
          "image-to-text",
          "multimodal",
          "image-text-to-text",
          "conversational",
          "en",
          "arxiv:2309.00071",
          "arxiv:2409.12191",
          "arxiv:2308.12966",
          "license:apache-2.0",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4324866,
          "likes_total": 1266,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态语言模型，专为视觉-语言理解与生成任务设计。其核心能力包括处理图像和文本输入以生成上下文相关的响应，支持对话交互、图像描述和视觉问答。主要优势在于高效的参数规模、与推理端点的兼容性以及Apache 2.0开源许可。典型应用场景涵盖人工智能助手、内容分析和教育工具，适用于需要整合图像与文本处理的领域，基于Transformer架构实现稳定性能。该模型通过多模态数据训练，提升了在复杂视觉语言任务中的实用性。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.0999698378058775,
        "task_keys": [
          "image_text_alignment",
          "multimodal_understanding_generation",
          "vqa",
          "visual_grounding",
          "lightweight_visual_model",
          "multimodal_dialogue_system"
        ],
        "summary_en": "Qwen2.5-VL-7B-Instruct is a 7-billion-parameter multimodal language model designed for visual-language understanding and generation tasks. Its core capabilities include processing both images and text to generate contextual responses, supporting conversational interactions, image captioning, and visual question answering. Key strengths encompass its efficient parameter size, compatibility with inference endpoints, and Apache 2.0 licensing for open use. Typical applications involve AI assistants, content analysis, and educational tools where integrated image-text processing is required, leveraging transformer architecture for robust performance.",
        "summary_zh": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态语言模型，专为视觉-语言理解与生成任务设计。其核心能力包括处理图像和文本输入以生成上下文相关的响应，支持对话交互、图像描述和视觉问答。主要优势在于高效的参数规模、与推理端点的兼容性以及Apache 2.0开源许可。典型应用场景涵盖人工智能助手、内容分析和教育工具，适用于需要整合图像与文本处理的领域，基于Transformer架构实现稳定性能。该模型通过多模态数据训练，提升了在复杂视觉语言任务中的实用性。",
        "summary_es": "Qwen2.5-VL-7B-Instruct is a 7-billion-parameter multimodal language model designed for visual-language understanding and generation tasks. Its core capabilities include processing both images and text to generate contextual responses, supporting conversational interactions, image captioning, and visual question answering. Key strengths encompass its efficient parameter size, compatibility with inference endpoints, and Apache 2.0 licensing for open use. Typical applications involve AI assistants, content analysis, and educational tools where integrated image-text processing is required, leveraging transformer architecture for robust performance."
      }
    ],
    "visual_grounding": [
      {
        "id": "Qwen/Qwen2.5-VL-7B-Instruct",
        "source": "hf",
        "name": "Qwen2.5-VL-7B-Instruct",
        "url": "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "qwen2_5_vl",
          "image-to-text",
          "multimodal",
          "image-text-to-text",
          "conversational",
          "en",
          "arxiv:2309.00071",
          "arxiv:2409.12191",
          "arxiv:2308.12966",
          "license:apache-2.0",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4324866,
          "likes_total": 1266,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态语言模型，专为视觉-语言理解与生成任务设计。其核心能力包括处理图像和文本输入以生成上下文相关的响应，支持对话交互、图像描述和视觉问答。主要优势在于高效的参数规模、与推理端点的兼容性以及Apache 2.0开源许可。典型应用场景涵盖人工智能助手、内容分析和教育工具，适用于需要整合图像与文本处理的领域，基于Transformer架构实现稳定性能。该模型通过多模态数据训练，提升了在复杂视觉语言任务中的实用性。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.0999698378058775,
        "task_keys": [
          "image_text_alignment",
          "multimodal_understanding_generation",
          "vqa",
          "visual_grounding",
          "lightweight_visual_model",
          "multimodal_dialogue_system"
        ],
        "summary_en": "Qwen2.5-VL-7B-Instruct is a 7-billion-parameter multimodal language model designed for visual-language understanding and generation tasks. Its core capabilities include processing both images and text to generate contextual responses, supporting conversational interactions, image captioning, and visual question answering. Key strengths encompass its efficient parameter size, compatibility with inference endpoints, and Apache 2.0 licensing for open use. Typical applications involve AI assistants, content analysis, and educational tools where integrated image-text processing is required, leveraging transformer architecture for robust performance.",
        "summary_zh": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态语言模型，专为视觉-语言理解与生成任务设计。其核心能力包括处理图像和文本输入以生成上下文相关的响应，支持对话交互、图像描述和视觉问答。主要优势在于高效的参数规模、与推理端点的兼容性以及Apache 2.0开源许可。典型应用场景涵盖人工智能助手、内容分析和教育工具，适用于需要整合图像与文本处理的领域，基于Transformer架构实现稳定性能。该模型通过多模态数据训练，提升了在复杂视觉语言任务中的实用性。",
        "summary_es": "Qwen2.5-VL-7B-Instruct is a 7-billion-parameter multimodal language model designed for visual-language understanding and generation tasks. Its core capabilities include processing both images and text to generate contextual responses, supporting conversational interactions, image captioning, and visual question answering. Key strengths encompass its efficient parameter size, compatibility with inference endpoints, and Apache 2.0 licensing for open use. Typical applications involve AI assistants, content analysis, and educational tools where integrated image-text processing is required, leveraging transformer architecture for robust performance."
      }
    ],
    "lightweight_visual_model": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "llm_pretraining": [
      {
        "id": "google/electra-base-discriminator",
        "source": "hf",
        "name": "electra-base-discriminator",
        "url": "https://huggingface.co/google/electra-base-discriminator",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "electra",
          "pretraining",
          "en",
          "arxiv:1406.2661",
          "license:apache-2.0",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 13835783,
          "likes_total": 65,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "ELECTRA-base-discriminator是谷歌基于ELECTRA框架开发的预训练自然语言处理模型。其主要目的是通过创新的预训练方法实现高效语言理解，该方法使用生成器替换文本标记，判别器检测被替换的标记。核心能力包括掩码语言建模、文本分类和序列标注。优势在于比BERT类模型具有更快的预训练速度、更好的计算效率和更强的下游任务性能。典型应用场景涵盖情感分析、命名实体识别、问答系统和文本分类等多个领域，适用于各种自然语言处理任务。该模型支持多种深度",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "llm_pretraining",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ELECTRA-base-discriminator is a pre-trained natural language processing model developed by Google using the ELECTRA framework. Its primary purpose is efficient language understanding through a novel pre-training approach where a generator replaces tokens and a discriminator detects replacements. Core capabilities include masked language modeling, text classification, and sequence labeling. Strengths are faster pre-training, better computational efficiency, and strong performance on downstream tasks compared to BERT-style models. Typical use cases encompass sentiment analysis, named entity recognition, question answering, and text classification across various domains.",
        "summary_zh": "ELECTRA-base-discriminator是谷歌基于ELECTRA框架开发的预训练自然语言处理模型。其主要目的是通过创新的预训练方法实现高效语言理解，该方法使用生成器替换文本标记，判别器检测被替换的标记。核心能力包括掩码语言建模、文本分类和序列标注。优势在于比BERT类模型具有更快的预训练速度、更好的计算效率和更强的下游任务性能。典型应用场景涵盖情感分析、命名实体识别、问答系统和文本分类等多个领域，适用于各种自然语言处理任务。该模型支持多种深度",
        "summary_es": "ELECTRA-base-discriminator is a pre-trained natural language processing model developed by Google using the ELECTRA framework. Its primary purpose is efficient language understanding through a novel pre-training approach where a generator replaces tokens and a discriminator detects replacements. Core capabilities include masked language modeling, text classification, and sequence labeling. Strengths are faster pre-training, better computational efficiency, and strong performance on downstream tasks compared to BERT-style models. Typical use cases encompass sentiment analysis, named entity recognition, question answering, and text classification across various domains."
      },
      {
        "id": "omni-research/Tarsier2-Recap-7b",
        "source": "hf",
        "name": "Tarsier2-Recap-7b",
        "url": "https://huggingface.co/omni-research/Tarsier2-Recap-7b",
        "tags": [
          "safetensors",
          "video LLM",
          "arxiv:2501.07888",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 10123337,
          "likes_total": 19,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Tarsier2-Recap-7b 是一个拥有70亿参数的专业视频语言模型，专门用于视频内容的理解与摘要生成。其核心能力在于处理视频序列并生成全面的文本摘要，能够从视觉信息中提取关键内容并转化为连贯的叙述。该模型在视频内容分析、教育视频处理和媒体监控等场景中表现优异，特别擅长将复杂的视觉信息转化为结构化的文本输出。基于arXiv:2501.07888的研究成果，采用safetensors格式和Apache 2.0开源协议，主要服务于需要将视频内容自动转换为详细文字摘要的专业应用需求。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_video",
          "llm_pretraining"
        ],
        "summary_en": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for comprehensive video understanding and summarization. Its core capability involves processing video content to generate detailed textual recaps. The model excels at extracting key information from visual sequences and converting it into coherent narratives. Typical use cases include automated video summarization for content analysis, educational video processing, and media monitoring applications. Based on research from arXiv:2501.07888, it employs safetensors format and operates under Apache 2.0 license, serving as a specialized tool for converting visual information into structured textual outputs.",
        "summary_zh": "Tarsier2-Recap-7b 是一个拥有70亿参数的专业视频语言模型，专门用于视频内容的理解与摘要生成。其核心能力在于处理视频序列并生成全面的文本摘要，能够从视觉信息中提取关键内容并转化为连贯的叙述。该模型在视频内容分析、教育视频处理和媒体监控等场景中表现优异，特别擅长将复杂的视觉信息转化为结构化的文本输出。基于arXiv:2501.07888的研究成果，采用safetensors格式和Apache 2.0开源协议，主要服务于需要将视频内容自动转换为详细文字摘要的专业应用需求。",
        "summary_es": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for comprehensive video understanding and summarization. Its core capability involves processing video content to generate detailed textual recaps. The model excels at extracting key information from visual sequences and converting it into coherent narratives. Typical use cases include automated video summarization for content analysis, educational video processing, and media monitoring applications. Based on research from arXiv:2501.07888, it employs safetensors format and operates under Apache 2.0 license, serving as a specialized tool for converting visual information into structured textual outputs."
      },
      {
        "id": "trl-internal-testing/tiny-Qwen2ForCausalLM-2.5",
        "source": "hf",
        "name": "tiny-Qwen2ForCausalLM-2.5",
        "url": "https://huggingface.co/trl-internal-testing/tiny-Qwen2ForCausalLM-2.5",
        "tags": [
          "transformers",
          "safetensors",
          "qwen2",
          "text-generation",
          "trl",
          "conversational",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4364467,
          "likes_total": 1,
          "downloads_7d": 41919,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "tiny-Qwen2ForCausalLM-2.5 是基于Qwen2架构的极小规模语言模型，专为测试和开发目的设计。该模型具备基础文本生成和对话AI功能，核心优势在于其微小体积，适用于快速原型开发、算法验证和教育演示。典型应用场景包括测试强化学习框架、评估模型训练流程以及轻量级AI应用基准测试。作为实验性工具，它为研究人员和开发者提供了处理基于Transformer的语言模型的实用平台，特别适合在计算资源有限的环境中进行模型验证和教学演示。该模型支持多种部署方式，便于集成",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.36973267086523376,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "llm_pretraining",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "tiny-Qwen2ForCausalLM-2.5 is a minimal-scale language model derived from Qwen2 architecture, designed for testing and development purposes. Its core capabilities include basic text generation and conversational AI functions. The model's strengths lie in its small size, making it suitable for rapid prototyping, algorithm validation, and educational demonstrations. Typical use cases encompass testing reinforcement learning frameworks, evaluating model training pipelines, and benchmarking lightweight AI applications. It serves as a practical tool for researchers and developers working with transformer-based language models in experimental settings.",
        "summary_zh": "tiny-Qwen2ForCausalLM-2.5 是基于Qwen2架构的极小规模语言模型，专为测试和开发目的设计。该模型具备基础文本生成和对话AI功能，核心优势在于其微小体积，适用于快速原型开发、算法验证和教育演示。典型应用场景包括测试强化学习框架、评估模型训练流程以及轻量级AI应用基准测试。作为实验性工具，它为研究人员和开发者提供了处理基于Transformer的语言模型的实用平台，特别适合在计算资源有限的环境中进行模型验证和教学演示。该模型支持多种部署方式，便于集成",
        "summary_es": "tiny-Qwen2ForCausalLM-2.5 is a minimal-scale language model derived from Qwen2 architecture, designed for testing and development purposes. Its core capabilities include basic text generation and conversational AI functions. The model's strengths lie in its small size, making it suitable for rapid prototyping, algorithm validation, and educational demonstrations. Typical use cases encompass testing reinforcement learning frameworks, evaluating model training pipelines, and benchmarking lightweight AI applications. It serves as a practical tool for researchers and developers working with transformer-based language models in experimental settings."
      }
    ],
    "instruction_tuning": [
      {
        "id": "google/gemma-3-1b-it",
        "source": "hf",
        "name": "gemma-3-1b-it",
        "url": "https://huggingface.co/google/gemma-3-1b-it",
        "tags": [
          "transformers",
          "safetensors",
          "gemma3_text",
          "text-generation",
          "conversational",
          "arxiv:1905.07830",
          "arxiv:1905.10044",
          "arxiv:1911.11641",
          "arxiv:1904.09728",
          "arxiv:1705.03551",
          "arxiv:1911.01547",
          "arxiv:1907.10641",
          "arxiv:1903.00161",
          "arxiv:2009.03300",
          "arxiv:2304.06364",
          "arxiv:2103.03874",
          "arxiv:2110.14168",
          "arxiv:2311.12022",
          "arxiv:2108.07732",
          "arxiv:2107.03374",
          "arxiv:2210.03057",
          "arxiv:2106.03193",
          "arxiv:1910.11856",
          "arxiv:2502.12404",
          "arxiv:2502.21228",
          "arxiv:2404.16816",
          "arxiv:2104.12756",
          "arxiv:2311.16502",
          "arxiv:2203.10244",
          "arxiv:2404.12390",
          "arxiv:1810.12440",
          "arxiv:1908.02660",
          "arxiv:2312.11805",
          "base_model:google/gemma-3-1b-pt",
          "base_model:finetune:google/gemma-3-1b-pt",
          "license:gemma",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 5836125,
          "likes_total": 629,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Gemma 3 1B IT 是谷歌开发的拥有 11 亿参数的指令微调语言模型。其主要目的是作为一个紧凑而功能强大的基础模型，用于自然语言理解和生成任务。核心能力包括文本补全、问答、摘要和代码生成。关键优势在于其小巧的模型尺寸，使其能够在资源受限的设备上高效部署；其开放可访问性便于研究和开发；以及基于谷歌训练方法的稳健性能。典型应用场景包括人工智能应用原型开发、教育工具、边缘计算部署，以及在较大模型不切实际的情况下使用的",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "instruction_tuning"
        ],
        "summary_en": "Gemma 3 1B IT is a 1.1-billion parameter instruction-tuned language model developed by Google. Its primary purpose is to serve as a compact yet capable foundation for natural language understanding and generation tasks. Core capabilities include text completion, question answering, summarization, and code generation. Key strengths are its small size enabling efficient deployment on resource-constrained devices, open accessibility for research and development, and robust performance derived from Google's training methodologies. Typical use cases encompass prototyping AI applications, educational tools, edge computing deployments, and lightweight chatbots where larger models are impractical.",
        "summary_zh": "Gemma 3 1B IT 是谷歌开发的拥有 11 亿参数的指令微调语言模型。其主要目的是作为一个紧凑而功能强大的基础模型，用于自然语言理解和生成任务。核心能力包括文本补全、问答、摘要和代码生成。关键优势在于其小巧的模型尺寸，使其能够在资源受限的设备上高效部署；其开放可访问性便于研究和开发；以及基于谷歌训练方法的稳健性能。典型应用场景包括人工智能应用原型开发、教育工具、边缘计算部署，以及在较大模型不切实际的情况下使用的",
        "summary_es": "Gemma 3 1B IT is a 1.1-billion parameter instruction-tuned language model developed by Google. Its primary purpose is to serve as a compact yet capable foundation for natural language understanding and generation tasks. Core capabilities include text completion, question answering, summarization, and code generation. Key strengths are its small size enabling efficient deployment on resource-constrained devices, open accessibility for research and development, and robust performance derived from Google's training methodologies. Typical use cases encompass prototyping AI applications, educational tools, edge computing deployments, and lightweight chatbots where larger models are impractical."
      }
    ],
    "rlhf": [],
    "rag": [],
    "code_generation": [
      {
        "id": "openai-community/gpt2",
        "source": "hf",
        "name": "gpt2",
        "url": "https://huggingface.co/openai-community/gpt2",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "tflite",
          "rust",
          "onnx",
          "safetensors",
          "gpt2",
          "text-generation",
          "exbert",
          "en",
          "doi:10.57967/hf/0039",
          "license:mit",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 12038015,
          "likes_total": 2955,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本预测和内容创作。其核心功能包括根据输入提示生成连贯的后续文本、完成句子结构以及创作多样化内容。该模型的优势在于无需微调即可实现高质量文本生成、支持多种写作风格且完全开源。典型应用场景涵盖创意写作辅助、聊天机器人对话生成、文本摘要制作以及自然语言处理研究。GPT-2特别擅长保持长文本的上下文一致性，在维持主题连贯性方面表现突出，为开发者和研究者提",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "GPT-2 is a transformer-based language model developed by OpenAI for text generation. Its primary purpose is to predict subsequent text based on input, enabling coherent continuation of prompts. Core capabilities include generating human-like text, completing sentences, and creating content across various domains. Strengths lie in its strong performance without fine-tuning, versatility across writing styles, and open-source availability. Typical use cases encompass creative writing assistance, chatbot responses, content summarization, and language modeling research. The model demonstrates particular effectiveness in maintaining contextual coherence throughout extended text passages.",
        "summary_zh": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本预测和内容创作。其核心功能包括根据输入提示生成连贯的后续文本、完成句子结构以及创作多样化内容。该模型的优势在于无需微调即可实现高质量文本生成、支持多种写作风格且完全开源。典型应用场景涵盖创意写作辅助、聊天机器人对话生成、文本摘要制作以及自然语言处理研究。GPT-2特别擅长保持长文本的上下文一致性，在维持主题连贯性方面表现突出，为开发者和研究者提",
        "summary_es": "GPT-2 is a transformer-based language model developed by OpenAI for text generation. Its primary purpose is to predict subsequent text based on input, enabling coherent continuation of prompts. Core capabilities include generating human-like text, completing sentences, and creating content across various domains. Strengths lie in its strong performance without fine-tuning, versatility across writing styles, and open-source availability. Typical use cases encompass creative writing assistance, chatbot responses, content summarization, and language modeling research. The model demonstrates particular effectiveness in maintaining contextual coherence throughout extended text passages."
      },
      {
        "id": "facebook/opt-125m",
        "source": "hf",
        "name": "opt-125m",
        "url": "https://huggingface.co/facebook/opt-125m",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "opt",
          "text-generation",
          "en",
          "arxiv:2205.01068",
          "arxiv:2005.14165",
          "license:other",
          "autotrain_compatible",
          "text-generation-inference",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8845823,
          "likes_total": 218,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "OPT-125M是Meta AI开发的1.25亿参数仅解码器Transformer模型，旨在促进大语言模型的开放研究。基于GPT-3架构，通过自回归预测生成连贯文本。主要目标是为研究缩放定律、训练动态和模型行为提供可替代专有模型的开放方案。核心能力包括文本补全、对话生成和小样本学习。优势在于透明的训练方法、可复现的设计及研究友好许可。典型应用场景涵盖学术实验、模型可解释性研究和Transformer语言生成的教学演示。该模型作为OPT系列最小版本，为计算资源有限的研究者提供了基础",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "OPT-125M is a 125 million parameter decoder-only transformer developed by Meta AI for open research in large language models. Based on GPT-3 architecture, it generates coherent text through autoregressive prediction. Its primary purpose is providing an accessible alternative to proprietary models for studying scaling laws, training dynamics, and model behavior. Core capabilities include text completion, dialogue generation, and few-shot learning. Strengths include transparent training methodology, reproducible design, and research-friendly licensing. Typical use cases encompass academic experiments, model interpretability studies, and educational demonstrations of transformer-based language generation.",
        "summary_zh": "OPT-125M是Meta AI开发的1.25亿参数仅解码器Transformer模型，旨在促进大语言模型的开放研究。基于GPT-3架构，通过自回归预测生成连贯文本。主要目标是为研究缩放定律、训练动态和模型行为提供可替代专有模型的开放方案。核心能力包括文本补全、对话生成和小样本学习。优势在于透明的训练方法、可复现的设计及研究友好许可。典型应用场景涵盖学术实验、模型可解释性研究和Transformer语言生成的教学演示。该模型作为OPT系列最小版本，为计算资源有限的研究者提供了基础",
        "summary_es": "OPT-125M is a 125 million parameter decoder-only transformer developed by Meta AI for open research in large language models. Based on GPT-3 architecture, it generates coherent text through autoregressive prediction. Its primary purpose is providing an accessible alternative to proprietary models for studying scaling laws, training dynamics, and model behavior. Core capabilities include text completion, dialogue generation, and few-shot learning. Strengths include transparent training methodology, reproducible design, and research-friendly licensing. Typical use cases encompass academic experiments, model interpretability studies, and educational demonstrations of transformer-based language generation."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "structured_reasoning": [],
    "tool_use": [],
    "lora_adapter": [],
    "multilingual_processing": [
      {
        "id": "FacebookAI/xlm-roberta-base",
        "source": "hf",
        "name": "xlm-roberta-base",
        "url": "https://huggingface.co/FacebookAI/xlm-roberta-base",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "onnx",
          "safetensors",
          "xlm-roberta",
          "fill-mask",
          "exbert",
          "multilingual",
          "af",
          "am",
          "ar",
          "as",
          "az",
          "be",
          "bg",
          "bn",
          "br",
          "bs",
          "ca",
          "cs",
          "cy",
          "da",
          "de",
          "el",
          "en",
          "eo",
          "es",
          "et",
          "eu",
          "fa",
          "fi",
          "fr",
          "fy",
          "ga",
          "gd",
          "gl",
          "gu",
          "ha",
          "he",
          "hi",
          "hr",
          "hu",
          "hy",
          "id",
          "is",
          "it",
          "ja",
          "jv",
          "ka",
          "kk",
          "km",
          "kn",
          "ko",
          "ku",
          "ky",
          "la",
          "lo",
          "lt",
          "lv",
          "mg",
          "mk",
          "ml",
          "mn",
          "mr",
          "ms",
          "my",
          "ne",
          "nl",
          "no",
          "om",
          "or",
          "pa",
          "pl",
          "ps",
          "pt",
          "ro",
          "ru",
          "sa",
          "sd",
          "si",
          "sk",
          "sl",
          "so",
          "sq",
          "sr",
          "su",
          "sv",
          "sw",
          "ta",
          "te",
          "th",
          "tl",
          "tr",
          "ug",
          "uk",
          "ur",
          "uz",
          "vi",
          "xh",
          "yi",
          "zh",
          "arxiv:1911.02116",
          "license:mit",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7789212,
          "likes_total": 732,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "xlm-roberta-base是由Facebook AI开发的多语言掩码语言模型，基于RoBERTa架构构建。该模型在涵盖100种语言的CommonCrawl数据上进行预训练，能够实现无需显式翻译的跨语言理解。其核心能力包括文本分类、命名实体识别和跨语言问答等任务，特别擅长处理需要语言无关表示的应用场景。主要优势在于通过从高资源语言迁移学习来有效处理低资源语言问题。典型应用包括多语言内容分析、跨语言信息检索以及为多样化语言环境构建自然语言处理系统。该模型支持包括中文、英文、",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "multilingual_processing",
          "auto_evaluation_models"
        ],
        "summary_en": "xlm-roberta-base is a multilingual masked language model developed by Facebook AI, based on the RoBERTa architecture. It is pretrained on CommonCrawl data covering 100 languages, enabling cross-lingual understanding without explicit translation. The model excels at tasks requiring language-agnostic representations, such as text classification, named entity recognition, and question answering across multiple languages. Its key strength lies in handling low-resource languages by leveraging transfer learning from high-resource languages. Typical applications include multilingual content analysis, cross-lingual information retrieval, and building NLP systems for diverse linguistic environments.",
        "summary_zh": "xlm-roberta-base是由Facebook AI开发的多语言掩码语言模型，基于RoBERTa架构构建。该模型在涵盖100种语言的CommonCrawl数据上进行预训练，能够实现无需显式翻译的跨语言理解。其核心能力包括文本分类、命名实体识别和跨语言问答等任务，特别擅长处理需要语言无关表示的应用场景。主要优势在于通过从高资源语言迁移学习来有效处理低资源语言问题。典型应用包括多语言内容分析、跨语言信息检索以及为多样化语言环境构建自然语言处理系统。该模型支持包括中文、英文、",
        "summary_es": "xlm-roberta-base is a multilingual masked language model developed by Facebook AI, based on the RoBERTa architecture. It is pretrained on CommonCrawl data covering 100 languages, enabling cross-lingual understanding without explicit translation. The model excels at tasks requiring language-agnostic representations, such as text classification, named entity recognition, and question answering across multiple languages. Its key strength lies in handling low-resource languages by leveraging transfer learning from high-resource languages. Typical applications include multilingual content analysis, cross-lingual information retrieval, and building NLP systems for diverse linguistic environments."
      },
      {
        "id": "google-t5/t5-small",
        "source": "hf",
        "name": "t5-small",
        "url": "https://huggingface.co/google-t5/t5-small",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "onnx",
          "safetensors",
          "t5",
          "text2text-generation",
          "summarization",
          "translation",
          "en",
          "fr",
          "ro",
          "de",
          "multilingual",
          "dataset:c4",
          "arxiv:1805.12471",
          "arxiv:1708.00055",
          "arxiv:1704.05426",
          "arxiv:1606.05250",
          "arxiv:1808.09121",
          "arxiv:1810.12885",
          "arxiv:1905.10044",
          "arxiv:1910.09700",
          "license:apache-2.0",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4867673,
          "likes_total": 492,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "T5-small是谷歌开发的紧凑型文本到文本转换模型，采用统一框架将所有自然语言处理任务重新表述为文本生成问题。该模型基于编码器-解码器架构，通过简单的文本前缀（如“翻译英语到德语：”或“总结：”）处理多样化任务。使用C4大型语料库进行去噪目标预训练。核心优势在于一致的任务处理方式和高效的迁移学习能力。典型应用涵盖多语言翻译、文本摘要、问答系统和文本分类等场景。模型设计注重通用性和灵活性，支持英语、德语、法语等多种语言",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "multilingual_processing",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "training_data_anonymization",
          "training_data_copyright",
          "molecular_generation"
        ],
        "summary_en": "T5-small is a compact text-to-text transformer model developed by Google, implementing the unified framework where all NLP tasks are reformulated as text generation problems. Based on the encoder-decoder architecture, it handles diverse tasks through simple text prefixes like 'translate English to German:' or 'summarize:'. The model was pre-trained on the large C4 corpus using a denoising objective. Its core strength lies in consistent task handling and transfer learning efficiency. Typical applications include translation, summarization, question answering, and text classification across multiple languages.",
        "summary_zh": "T5-small是谷歌开发的紧凑型文本到文本转换模型，采用统一框架将所有自然语言处理任务重新表述为文本生成问题。该模型基于编码器-解码器架构，通过简单的文本前缀（如“翻译英语到德语：”或“总结：”）处理多样化任务。使用C4大型语料库进行去噪目标预训练。核心优势在于一致的任务处理方式和高效的迁移学习能力。典型应用涵盖多语言翻译、文本摘要、问答系统和文本分类等场景。模型设计注重通用性和灵活性，支持英语、德语、法语等多种语言",
        "summary_es": "T5-small is a compact text-to-text transformer model developed by Google, implementing the unified framework where all NLP tasks are reformulated as text generation problems. Based on the encoder-decoder architecture, it handles diverse tasks through simple text prefixes like 'translate English to German:' or 'summarize:'. The model was pre-trained on the large C4 corpus using a denoising objective. Its core strength lies in consistent task handling and transfer learning efficiency. Typical applications include translation, summarization, question answering, and text classification across multiple languages."
      },
      {
        "id": "jinaai/jina-embeddings-v3",
        "source": "hf",
        "name": "jina-embeddings-v3",
        "url": "https://huggingface.co/jinaai/jina-embeddings-v3",
        "tags": [
          "transformers",
          "pytorch",
          "onnx",
          "safetensors",
          "feature-extraction",
          "sentence-similarity",
          "mteb",
          "sentence-transformers",
          "custom_code",
          "multilingual",
          "af",
          "am",
          "ar",
          "as",
          "az",
          "be",
          "bg",
          "bn",
          "br",
          "bs",
          "ca",
          "cs",
          "cy",
          "da",
          "de",
          "el",
          "en",
          "eo",
          "es",
          "et",
          "eu",
          "fa",
          "fi",
          "fr",
          "fy",
          "ga",
          "gd",
          "gl",
          "gu",
          "ha",
          "he",
          "hi",
          "hr",
          "hu",
          "hy",
          "id",
          "is",
          "it",
          "ja",
          "jv",
          "ka",
          "kk",
          "km",
          "kn",
          "ko",
          "ku",
          "ky",
          "la",
          "lo",
          "lt",
          "lv",
          "mg",
          "mk",
          "ml",
          "mn",
          "mr",
          "ms",
          "my",
          "ne",
          "nl",
          "no",
          "om",
          "or",
          "pa",
          "pl",
          "ps",
          "pt",
          "ro",
          "ru",
          "sa",
          "sd",
          "si",
          "sk",
          "sl",
          "so",
          "sq",
          "sr",
          "su",
          "sv",
          "sw",
          "ta",
          "te",
          "th",
          "tl",
          "tr",
          "ug",
          "uk",
          "ur",
          "uz",
          "vi",
          "xh",
          "yi",
          "zh",
          "arxiv:2409.10173",
          "license:cc-by-nc-4.0",
          "model-index",
          "region:eu"
        ],
        "stats": {
          "downloads_total": 4464170,
          "likes_total": 1071,
          "downloads_7d": 138558,
          "likes_7d": 3
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Jina Embeddings V3 是一款多语言文本嵌入模型，旨在将100多种语言的文本转换为数值表示。其核心能力在于为不同语言输入生成高质量、捕捉语义含义的嵌入向量。主要优势包括无需语言检测的原生多语言支持、跨语言性能一致性以及8K上下文窗口。该模型特别适用于检索增强生成、语义搜索和聚类应用场景。可作为现有嵌入模型的直接替代品，同时提供增强的跨语言能力。典型用例涵盖多语言文档检索、跨语言相似性匹配以及构建国际化人工智能应用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.18964106784775644,
        "task_keys": [
          "lightweight_visual_model",
          "code_generation",
          "multilingual_processing",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update"
        ],
        "summary_en": "Jina Embeddings V3 is a multilingual text embedding model designed to convert text into numerical representations across 100+ languages. Its core capability lies in generating high-quality embeddings that capture semantic meaning for diverse linguistic inputs. Key strengths include native multilingual support without language detection, consistent performance across languages, and an 8K context window. The model is particularly effective for retrieval-augmented generation, semantic search, and clustering applications. It serves as a drop-in replacement for existing embedding models while offering enhanced cross-lingual capabilities. Typical use cases include multilingual document retrieval, cross-language similarity matching, and building international AI applications.",
        "summary_zh": "Jina Embeddings V3 是一款多语言文本嵌入模型，旨在将100多种语言的文本转换为数值表示。其核心能力在于为不同语言输入生成高质量、捕捉语义含义的嵌入向量。主要优势包括无需语言检测的原生多语言支持、跨语言性能一致性以及8K上下文窗口。该模型特别适用于检索增强生成、语义搜索和聚类应用场景。可作为现有嵌入模型的直接替代品，同时提供增强的跨语言能力。典型用例涵盖多语言文档检索、跨语言相似性匹配以及构建国际化人工智能应用。",
        "summary_es": "Jina Embeddings V3 is a multilingual text embedding model designed to convert text into numerical representations across 100+ languages. Its core capability lies in generating high-quality embeddings that capture semantic meaning for diverse linguistic inputs. Key strengths include native multilingual support without language detection, consistent performance across languages, and an 8K context window. The model is particularly effective for retrieval-augmented generation, semantic search, and clustering applications. It serves as a drop-in replacement for existing embedding models while offering enhanced cross-lingual capabilities. Typical use cases include multilingual document retrieval, cross-language similarity matching, and building international AI applications."
      }
    ],
    "low_resource_language": [],
    "knowledge_editing": [],
    "nlp_data_synthesis": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "google-bert/bert-base-uncased",
        "source": "hf",
        "name": "bert-base-uncased",
        "url": "https://huggingface.co/google-bert/bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "coreml",
          "onnx",
          "safetensors",
          "bert",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1810.04805",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 55171860,
          "likes_total": 2417,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "BERT-base-uncased 是基于Transformer架构的英语预训练模型，使用Wikipedia和BookCorpus数据进行训练，采用掩码语言建模和下一句预测目标。核心能力在于双向理解文本上下文，支持多种自然语言处理任务。主要优势包括在文本分类、命名实体识别和问答任务上的优异表现，且无需针对特定任务修改模型架构。典型应用场景包括情感分析、信息抽取和文本理解等下游任务的微调。该模型作为英语语言处理的基础模型，具有广泛的适用性和良好的迁移学习能力。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus using masked language modeling and next sentence prediction. Its core capability is bidirectional context understanding for various NLP tasks. Key strengths include strong performance on text classification, named entity recognition, and question answering without task-specific architecture changes. Typical use cases involve fine-tuning for sentiment analysis, information extraction, and text understanding applications. The model serves as a versatile baseline for English language processing.",
        "summary_zh": "BERT-base-uncased 是基于Transformer架构的英语预训练模型，使用Wikipedia和BookCorpus数据进行训练，采用掩码语言建模和下一句预测目标。核心能力在于双向理解文本上下文，支持多种自然语言处理任务。主要优势包括在文本分类、命名实体识别和问答任务上的优异表现，且无需针对特定任务修改模型架构。典型应用场景包括情感分析、信息抽取和文本理解等下游任务的微调。该模型作为英语语言处理的基础模型，具有广泛的适用性和良好的迁移学习能力。",
        "summary_es": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus using masked language modeling and next sentence prediction. Its core capability is bidirectional context understanding for various NLP tasks. Key strengths include strong performance on text classification, named entity recognition, and question answering without task-specific architecture changes. Typical use cases involve fine-tuning for sentiment analysis, information extraction, and text understanding applications. The model serves as a versatile baseline for English language processing."
      },
      {
        "id": "FacebookAI/roberta-large",
        "source": "hf",
        "name": "roberta-large",
        "url": "https://huggingface.co/FacebookAI/roberta-large",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "onnx",
          "safetensors",
          "roberta",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1907.11692",
          "arxiv:1806.02847",
          "license:mit",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 13086830,
          "likes_total": 247,
          "downloads_7d": 833951,
          "likes_7d": 1
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "RoBERTa-large是由Facebook AI开发的BERT预训练优化模型，移除了BERT的下句预测目标，采用更大批次和更多数据进行训练。该模型包含24层、16个注意力头和3.55亿参数，核心能力包括掩码语言建模、文本分类和自然语言推理。其优势在于GLUE和SQuAD等基准测试中表现优于原始BERT，典型应用涵盖情感分析、问答系统和文本分类等多个领域。模型基于BookCorpus和Wikipedia数据训练，支持PyTorch、TensorFlow和JAX框架，适用于各种自然语言处理任务。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.4257887794444891,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "RoBERTa-large is a robustly optimized BERT pretraining approach developed by Facebook AI. It removes BERT's next-sentence prediction objective and trains with larger batches and more data. The model features 24 layers, 16 attention heads, and 355 million parameters. Its core capabilities include masked language modeling, text classification, and natural language inference. Strengths include superior performance on GLUE and SQuAD benchmarks compared to original BERT. Typical use cases span sentiment analysis, question answering, and text classification tasks across various domains.",
        "summary_zh": "RoBERTa-large是由Facebook AI开发的BERT预训练优化模型，移除了BERT的下句预测目标，采用更大批次和更多数据进行训练。该模型包含24层、16个注意力头和3.55亿参数，核心能力包括掩码语言建模、文本分类和自然语言推理。其优势在于GLUE和SQuAD等基准测试中表现优于原始BERT，典型应用涵盖情感分析、问答系统和文本分类等多个领域。模型基于BookCorpus和Wikipedia数据训练，支持PyTorch、TensorFlow和JAX框架，适用于各种自然语言处理任务。",
        "summary_es": "RoBERTa-large is a robustly optimized BERT pretraining approach developed by Facebook AI. It removes BERT's next-sentence prediction objective and trains with larger batches and more data. The model features 24 layers, 16 attention heads, and 355 million parameters. Its core capabilities include masked language modeling, text classification, and natural language inference. Strengths include superior performance on GLUE and SQuAD benchmarks compared to original BERT. Typical use cases span sentiment analysis, question answering, and text classification tasks across various domains."
      }
    ],
    "nlp_data_distillation": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "google-bert/bert-base-uncased",
        "source": "hf",
        "name": "bert-base-uncased",
        "url": "https://huggingface.co/google-bert/bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "coreml",
          "onnx",
          "safetensors",
          "bert",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1810.04805",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 55171860,
          "likes_total": 2417,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "BERT-base-uncased 是基于Transformer架构的英语预训练模型，使用Wikipedia和BookCorpus数据进行训练，采用掩码语言建模和下一句预测目标。核心能力在于双向理解文本上下文，支持多种自然语言处理任务。主要优势包括在文本分类、命名实体识别和问答任务上的优异表现，且无需针对特定任务修改模型架构。典型应用场景包括情感分析、信息抽取和文本理解等下游任务的微调。该模型作为英语语言处理的基础模型，具有广泛的适用性和良好的迁移学习能力。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus using masked language modeling and next sentence prediction. Its core capability is bidirectional context understanding for various NLP tasks. Key strengths include strong performance on text classification, named entity recognition, and question answering without task-specific architecture changes. Typical use cases involve fine-tuning for sentiment analysis, information extraction, and text understanding applications. The model serves as a versatile baseline for English language processing.",
        "summary_zh": "BERT-base-uncased 是基于Transformer架构的英语预训练模型，使用Wikipedia和BookCorpus数据进行训练，采用掩码语言建模和下一句预测目标。核心能力在于双向理解文本上下文，支持多种自然语言处理任务。主要优势包括在文本分类、命名实体识别和问答任务上的优异表现，且无需针对特定任务修改模型架构。典型应用场景包括情感分析、信息抽取和文本理解等下游任务的微调。该模型作为英语语言处理的基础模型，具有广泛的适用性和良好的迁移学习能力。",
        "summary_es": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus using masked language modeling and next sentence prediction. Its core capability is bidirectional context understanding for various NLP tasks. Key strengths include strong performance on text classification, named entity recognition, and question answering without task-specific architecture changes. Typical use cases involve fine-tuning for sentiment analysis, information extraction, and text understanding applications. The model serves as a versatile baseline for English language processing."
      },
      {
        "id": "FacebookAI/roberta-large",
        "source": "hf",
        "name": "roberta-large",
        "url": "https://huggingface.co/FacebookAI/roberta-large",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "onnx",
          "safetensors",
          "roberta",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1907.11692",
          "arxiv:1806.02847",
          "license:mit",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 13086830,
          "likes_total": 247,
          "downloads_7d": 833951,
          "likes_7d": 1
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "RoBERTa-large是由Facebook AI开发的BERT预训练优化模型，移除了BERT的下句预测目标，采用更大批次和更多数据进行训练。该模型包含24层、16个注意力头和3.55亿参数，核心能力包括掩码语言建模、文本分类和自然语言推理。其优势在于GLUE和SQuAD等基准测试中表现优于原始BERT，典型应用涵盖情感分析、问答系统和文本分类等多个领域。模型基于BookCorpus和Wikipedia数据训练，支持PyTorch、TensorFlow和JAX框架，适用于各种自然语言处理任务。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.4257887794444891,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "RoBERTa-large is a robustly optimized BERT pretraining approach developed by Facebook AI. It removes BERT's next-sentence prediction objective and trains with larger batches and more data. The model features 24 layers, 16 attention heads, and 355 million parameters. Its core capabilities include masked language modeling, text classification, and natural language inference. Strengths include superior performance on GLUE and SQuAD benchmarks compared to original BERT. Typical use cases span sentiment analysis, question answering, and text classification tasks across various domains.",
        "summary_zh": "RoBERTa-large是由Facebook AI开发的BERT预训练优化模型，移除了BERT的下句预测目标，采用更大批次和更多数据进行训练。该模型包含24层、16个注意力头和3.55亿参数，核心能力包括掩码语言建模、文本分类和自然语言推理。其优势在于GLUE和SQuAD等基准测试中表现优于原始BERT，典型应用涵盖情感分析、问答系统和文本分类等多个领域。模型基于BookCorpus和Wikipedia数据训练，支持PyTorch、TensorFlow和JAX框架，适用于各种自然语言处理任务。",
        "summary_es": "RoBERTa-large is a robustly optimized BERT pretraining approach developed by Facebook AI. It removes BERT's next-sentence prediction objective and trains with larger batches and more data. The model features 24 layers, 16 attention heads, and 355 million parameters. Its core capabilities include masked language modeling, text classification, and natural language inference. Strengths include superior performance on GLUE and SQuAD benchmarks compared to original BERT. Typical use cases span sentiment analysis, question answering, and text classification tasks across various domains."
      }
    ],
    "dialogue_system_optimization": [],
    "nlp_bias_mitigation": [
      {
        "id": "nlpaueb/legal-bert-base-uncased",
        "source": "hf",
        "name": "legal-bert-base-uncased",
        "url": "https://huggingface.co/nlpaueb/legal-bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "bert",
          "pretraining",
          "legal",
          "fill-mask",
          "en",
          "license:cc-by-sa-4.0",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 5590506,
          "likes_total": 271,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该BERT模型专为英语法律文本预训练，数据源自EUR-Lex及欧洲法院文献，旨在精准理解法律语言。核心功能涵盖文本分类、命名实体识别与法律文档分析。相较于通用BERT模型，其突出优势在于法律任务中的卓越表现，典型应用场景包括合同审阅、法律研究，以及法院判决与立法分析的自动化处理。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "llm_pretraining",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "nlp_bias_mitigation",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "This BERT model is specifically pretrained on English legal texts from EUR-Lex and the European Court of Justice. Its purpose is to understand and process legal language. Core capabilities include text classification, named entity recognition, and legal document analysis. Its notable strength is superior performance on legal tasks compared to general-purpose BERT models. Typical use cases involve contract review, legal research, and automating the analysis of court rulings or legislation.",
        "summary_zh": "该BERT模型专为英语法律文本预训练，数据源自EUR-Lex及欧洲法院文献，旨在精准理解法律语言。核心功能涵盖文本分类、命名实体识别与法律文档分析。相较于通用BERT模型，其突出优势在于法律任务中的卓越表现，典型应用场景包括合同审阅、法律研究，以及法院判决与立法分析的自动化处理。",
        "summary_es": "This BERT model is specifically pretrained on English legal texts from EUR-Lex and the European Court of Justice. Its purpose is to understand and process legal language. Core capabilities include text classification, named entity recognition, and legal document analysis. Its notable strength is superior performance on legal tasks compared to general-purpose BERT models. Typical use cases involve contract review, legal research, and automating the analysis of court rulings or legislation."
      }
    ],
    "image_text_alignment": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 98988953,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards.",
        "summary_zh": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "multimodal_understanding_generation": [
      {
        "id": "openai-community/gpt2",
        "source": "hf",
        "name": "gpt2",
        "url": "https://huggingface.co/openai-community/gpt2",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "tflite",
          "rust",
          "onnx",
          "safetensors",
          "gpt2",
          "text-generation",
          "exbert",
          "en",
          "doi:10.57967/hf/0039",
          "license:mit",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 12038015,
          "likes_total": 2955,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本预测和内容创作。其核心功能包括根据输入提示生成连贯的后续文本、完成句子结构以及创作多样化内容。该模型的优势在于无需微调即可实现高质量文本生成、支持多种写作风格且完全开源。典型应用场景涵盖创意写作辅助、聊天机器人对话生成、文本摘要制作以及自然语言处理研究。GPT-2特别擅长保持长文本的上下文一致性，在维持主题连贯性方面表现突出，为开发者和研究者提",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "GPT-2 is a transformer-based language model developed by OpenAI for text generation. Its primary purpose is to predict subsequent text based on input, enabling coherent continuation of prompts. Core capabilities include generating human-like text, completing sentences, and creating content across various domains. Strengths lie in its strong performance without fine-tuning, versatility across writing styles, and open-source availability. Typical use cases encompass creative writing assistance, chatbot responses, content summarization, and language modeling research. The model demonstrates particular effectiveness in maintaining contextual coherence throughout extended text passages.",
        "summary_zh": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本预测和内容创作。其核心功能包括根据输入提示生成连贯的后续文本、完成句子结构以及创作多样化内容。该模型的优势在于无需微调即可实现高质量文本生成、支持多种写作风格且完全开源。典型应用场景涵盖创意写作辅助、聊天机器人对话生成、文本摘要制作以及自然语言处理研究。GPT-2特别擅长保持长文本的上下文一致性，在维持主题连贯性方面表现突出，为开发者和研究者提",
        "summary_es": "GPT-2 is a transformer-based language model developed by OpenAI for text generation. Its primary purpose is to predict subsequent text based on input, enabling coherent continuation of prompts. Core capabilities include generating human-like text, completing sentences, and creating content across various domains. Strengths lie in its strong performance without fine-tuning, versatility across writing styles, and open-source availability. Typical use cases encompass creative writing assistance, chatbot responses, content summarization, and language modeling research. The model demonstrates particular effectiveness in maintaining contextual coherence throughout extended text passages."
      },
      {
        "id": "facebook/opt-125m",
        "source": "hf",
        "name": "opt-125m",
        "url": "https://huggingface.co/facebook/opt-125m",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "opt",
          "text-generation",
          "en",
          "arxiv:2205.01068",
          "arxiv:2005.14165",
          "license:other",
          "autotrain_compatible",
          "text-generation-inference",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8845823,
          "likes_total": 218,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "OPT-125M是Meta AI开发的1.25亿参数仅解码器Transformer模型，旨在促进大语言模型的开放研究。基于GPT-3架构，通过自回归预测生成连贯文本。主要目标是为研究缩放定律、训练动态和模型行为提供可替代专有模型的开放方案。核心能力包括文本补全、对话生成和小样本学习。优势在于透明的训练方法、可复现的设计及研究友好许可。典型应用场景涵盖学术实验、模型可解释性研究和Transformer语言生成的教学演示。该模型作为OPT系列最小版本，为计算资源有限的研究者提供了基础",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "OPT-125M is a 125 million parameter decoder-only transformer developed by Meta AI for open research in large language models. Based on GPT-3 architecture, it generates coherent text through autoregressive prediction. Its primary purpose is providing an accessible alternative to proprietary models for studying scaling laws, training dynamics, and model behavior. Core capabilities include text completion, dialogue generation, and few-shot learning. Strengths include transparent training methodology, reproducible design, and research-friendly licensing. Typical use cases encompass academic experiments, model interpretability studies, and educational demonstrations of transformer-based language generation.",
        "summary_zh": "OPT-125M是Meta AI开发的1.25亿参数仅解码器Transformer模型，旨在促进大语言模型的开放研究。基于GPT-3架构，通过自回归预测生成连贯文本。主要目标是为研究缩放定律、训练动态和模型行为提供可替代专有模型的开放方案。核心能力包括文本补全、对话生成和小样本学习。优势在于透明的训练方法、可复现的设计及研究友好许可。典型应用场景涵盖学术实验、模型可解释性研究和Transformer语言生成的教学演示。该模型作为OPT系列最小版本，为计算资源有限的研究者提供了基础",
        "summary_es": "OPT-125M is a 125 million parameter decoder-only transformer developed by Meta AI for open research in large language models. Based on GPT-3 architecture, it generates coherent text through autoregressive prediction. Its primary purpose is providing an accessible alternative to proprietary models for studying scaling laws, training dynamics, and model behavior. Core capabilities include text completion, dialogue generation, and few-shot learning. Strengths include transparent training methodology, reproducible design, and research-friendly licensing. Typical use cases encompass academic experiments, model interpretability studies, and educational demonstrations of transformer-based language generation."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "asr": [
      {
        "id": "pyannote/speaker-diarization-3.1",
        "source": "hf",
        "name": "speaker-diarization-3.1",
        "url": "https://huggingface.co/pyannote/speaker-diarization-3.1",
        "tags": [
          "pyannote-audio",
          "pyannote",
          "pyannote-audio-pipeline",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "automatic-speech-recognition",
          "arxiv:2111.14448",
          "arxiv:2012.01477",
          "license:mit",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 16721804,
          "likes_total": 1169,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote speaker-diarization-3.1 是一个音频处理流程，专门用于对录音进行分段并识别不同说话者。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该系统能够区分不同的说话者，并识别多人同时说话的情况。主要优势在于对各种音频质量的鲁棒性表现以及与Hugging Face端点的兼容性。典型应用场景包括会议记录转录、广播内容监控、播客分析和司法音频检查，这些场景都需要精确的说话人识别和时间线标注。该工具基于多项研究成果开发，采用MIT许可证，适用",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "asr",
          "speaker_separation",
          "vector_retrieval",
          "graph_augmented_reco",
          "auto_evaluation_models",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote speaker-diarization-3.1 is an audio processing pipeline designed to segment and identify speakers in audio recordings. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The system can distinguish between different speakers and determine when multiple people speak simultaneously. Strengths include robust performance on various audio qualities and compatibility with Hugging Face endpoints. Typical use cases involve meeting transcription, broadcast monitoring, podcast analysis, and forensic audio examination where speaker identification and timeline annotation are required.",
        "summary_zh": "Pyannote speaker-diarization-3.1 是一个音频处理流程，专门用于对录音进行分段并识别不同说话者。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该系统能够区分不同的说话者，并识别多人同时说话的情况。主要优势在于对各种音频质量的鲁棒性表现以及与Hugging Face端点的兼容性。典型应用场景包括会议记录转录、广播内容监控、播客分析和司法音频检查，这些场景都需要精确的说话人识别和时间线标注。该工具基于多项研究成果开发，采用MIT许可证，适用",
        "summary_es": "Pyannote speaker-diarization-3.1 is an audio processing pipeline designed to segment and identify speakers in audio recordings. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The system can distinguish between different speakers and determine when multiple people speak simultaneously. Strengths include robust performance on various audio qualities and compatibility with Hugging Face endpoints. Typical use cases involve meeting transcription, broadcast monitoring, podcast analysis, and forensic audio examination where speaker identification and timeline annotation are required."
      },
      {
        "id": "pyannote/voice-activity-detection",
        "source": "hf",
        "name": "voice-activity-detection",
        "url": "https://huggingface.co/pyannote/voice-activity-detection",
        "tags": [
          "pyannote-audio",
          "pyannote",
          "pyannote-audio-pipeline",
          "audio",
          "voice",
          "speech",
          "speaker",
          "voice-activity-detection",
          "automatic-speech-recognition",
          "dataset:ami",
          "dataset:dihard",
          "dataset:voxconverse",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4994060,
          "likes_total": 211,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "pyannote/语音活动检测模型是专门用于检测音频录音中语音片段的工具，能够区分人类语音与静默或背景噪声。核心功能包括利用深度学习对语音区域进行精确的时间分段。主要优势在于对AMI、DIHARD和VoxConverse等多种数据集的高准确性，以及对不同声学条件的强鲁棒性。典型应用场景包括语音识别系统的预处理、会议转录、说话人日志生成管道和音频分析工作流。该MIT许可模型作为音频处理应用的基础组件，为后续语音分析任务提供可靠的语音段检测服务。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "asr",
          "speaker_separation",
          "vector_retrieval",
          "graph_augmented_reco",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The pyannote/voice-activity-detection model is a specialized tool for detecting speech segments in audio recordings. It identifies when human speech is present versus silence or background noise. Core capabilities include precise temporal segmentation of speech regions using deep learning. Strengths include high accuracy across diverse datasets like AMI, DIHARD, and VoxConverse, and robustness to various acoustic conditions. Typical use cases involve preprocessing for speech recognition systems, meeting transcription, speaker diarization pipelines, and audio analysis workflows. The MIT-licensed model serves as a fundamental component in audio processing applications.",
        "summary_zh": "pyannote/语音活动检测模型是专门用于检测音频录音中语音片段的工具，能够区分人类语音与静默或背景噪声。核心功能包括利用深度学习对语音区域进行精确的时间分段。主要优势在于对AMI、DIHARD和VoxConverse等多种数据集的高准确性，以及对不同声学条件的强鲁棒性。典型应用场景包括语音识别系统的预处理、会议转录、说话人日志生成管道和音频分析工作流。该MIT许可模型作为音频处理应用的基础组件，为后续语音分析任务提供可靠的语音段检测服务。",
        "summary_es": "The pyannote/voice-activity-detection model is a specialized tool for detecting speech segments in audio recordings. It identifies when human speech is present versus silence or background noise. Core capabilities include precise temporal segmentation of speech regions using deep learning. Strengths include high accuracy across diverse datasets like AMI, DIHARD, and VoxConverse, and robustness to various acoustic conditions. Typical use cases involve preprocessing for speech recognition systems, meeting transcription, speaker diarization pipelines, and audio analysis workflows. The MIT-licensed model serves as a fundamental component in audio processing applications."
      },
      {
        "id": "openai/whisper-large-v3",
        "source": "hf",
        "name": "whisper-large-v3",
        "url": "https://huggingface.co/openai/whisper-large-v3",
        "tags": [
          "transformers",
          "pytorch",
          "jax",
          "safetensors",
          "whisper",
          "automatic-speech-recognition",
          "audio",
          "hf-asr-leaderboard",
          "en",
          "zh",
          "de",
          "es",
          "ru",
          "ko",
          "fr",
          "ja",
          "pt",
          "tr",
          "pl",
          "ca",
          "nl",
          "ar",
          "sv",
          "it",
          "id",
          "hi",
          "fi",
          "vi",
          "he",
          "uk",
          "el",
          "ms",
          "cs",
          "ro",
          "da",
          "hu",
          "ta",
          "no",
          "th",
          "ur",
          "hr",
          "bg",
          "lt",
          "la",
          "mi",
          "ml",
          "cy",
          "sk",
          "te",
          "fa",
          "lv",
          "bn",
          "sr",
          "az",
          "sl",
          "kn",
          "et",
          "mk",
          "br",
          "eu",
          "is",
          "hy",
          "ne",
          "mn",
          "bs",
          "kk",
          "sq",
          "sw",
          "gl",
          "mr",
          "pa",
          "si",
          "km",
          "sn",
          "yo",
          "so",
          "af",
          "oc",
          "ka",
          "be",
          "tg",
          "sd",
          "gu",
          "am",
          "yi",
          "lo",
          "uz",
          "fo",
          "ht",
          "ps",
          "tk",
          "nn",
          "mt",
          "sa",
          "lb",
          "my",
          "bo",
          "tl",
          "mg",
          "as",
          "tt",
          "haw",
          "ln",
          "ha",
          "ba",
          "jw",
          "su",
          "arxiv:2212.04356",
          "license:apache-2.0",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4355478,
          "likes_total": 4946,
          "downloads_7d": 0,
          "likes_7d": 24
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Whisper-large-v3是OpenAI开发的高级自动语音识别模型，专门用于多语言转录和翻译任务。该模型的核心能力包括支持99种语言的语音转文本功能，以及将非英语音频翻译成英语的能力。其优势在于基于大规模互联网音频数据的训练，能够适应不同口音和声学环境，具备较强的鲁棒性，并且作为开源模型易于获取和使用。典型应用场景涵盖专业转录服务、媒体字幕生成、语音控制应用程序以及需要跨语言沟通的工具，特别适用于需要高精度语音转文本而无",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.2820650296011775,
        "task_keys": [
          "asr"
        ],
        "summary_en": "Whisper-large-v3 is OpenAI's advanced automatic speech recognition model designed for multilingual transcription and translation. Its core capabilities include converting speech to text across 99 languages and translating non-English audio into English. The model's strengths lie in its large-scale training on diverse internet audio data, robust performance across various accents and acoustic conditions, and open accessibility. Typical use cases encompass transcription services, media subtitling, voice-controlled applications, and multilingual communication tools where accurate speech-to-text conversion is required without domain-specific fine-tuning.",
        "summary_zh": "Whisper-large-v3是OpenAI开发的高级自动语音识别模型，专门用于多语言转录和翻译任务。该模型的核心能力包括支持99种语言的语音转文本功能，以及将非英语音频翻译成英语的能力。其优势在于基于大规模互联网音频数据的训练，能够适应不同口音和声学环境，具备较强的鲁棒性，并且作为开源模型易于获取和使用。典型应用场景涵盖专业转录服务、媒体字幕生成、语音控制应用程序以及需要跨语言沟通的工具，特别适用于需要高精度语音转文本而无",
        "summary_es": "Whisper-large-v3 is OpenAI's advanced automatic speech recognition model designed for multilingual transcription and translation. Its core capabilities include converting speech to text across 99 languages and translating non-English audio into English. The model's strengths lie in its large-scale training on diverse internet audio data, robust performance across various accents and acoustic conditions, and open accessibility. Typical use cases encompass transcription services, media subtitling, voice-controlled applications, and multilingual communication tools where accurate speech-to-text conversion is required without domain-specific fine-tuning."
      }
    ],
    "tts": [
      {
        "id": "coqui/XTTS-v2",
        "source": "hf",
        "name": "XTTS-v2",
        "url": "https://huggingface.co/coqui/XTTS-v2",
        "tags": [
          "coqui",
          "text-to-speech",
          "license:other",
          "region:us"
        ],
        "stats": {
          "downloads_total": 5511182,
          "likes_total": 3058,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "XTTS-v2是由Coqui开发的多语言文本转语音模型，能够从文本输入生成自然流畅的语音。其核心功能在于通过短音频样本进行语音克隆，并在不同语言间保持说话人特征的一致性。该模型支持多种语言，可产生高质量、富有表现力的语音输出。主要优势包括使用少量参考音频即可高效适配声音，以及跨语言语音特征保持稳定。典型应用场景涵盖有声读物制作、语音助手开发、内容本地化以及为视障用户提供的辅助工具。该模型采用自定义许可证，托",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "tts"
        ],
        "summary_en": "XTTS-v2 is a multilingual text-to-speech model developed by Coqui that generates natural speech from text input. Its core capability lies in voice cloning from short audio samples while maintaining the speaker's characteristics across different languages. The model supports multiple languages and produces high-quality, expressive speech output. Key strengths include efficient voice adaptation with minimal reference audio and cross-lingual voice consistency. Typical use cases encompass audiobook production, voice assistant development, content localization, and accessibility tools for visually impaired users. The model operates under a custom license and is hosted on Hugging Face.",
        "summary_zh": "XTTS-v2是由Coqui开发的多语言文本转语音模型，能够从文本输入生成自然流畅的语音。其核心功能在于通过短音频样本进行语音克隆，并在不同语言间保持说话人特征的一致性。该模型支持多种语言，可产生高质量、富有表现力的语音输出。主要优势包括使用少量参考音频即可高效适配声音，以及跨语言语音特征保持稳定。典型应用场景涵盖有声读物制作、语音助手开发、内容本地化以及为视障用户提供的辅助工具。该模型采用自定义许可证，托",
        "summary_es": "XTTS-v2 is a multilingual text-to-speech model developed by Coqui that generates natural speech from text input. Its core capability lies in voice cloning from short audio samples while maintaining the speaker's characteristics across different languages. The model supports multiple languages and produces high-quality, expressive speech output. Key strengths include efficient voice adaptation with minimal reference audio and cross-lingual voice consistency. Typical use cases encompass audiobook production, voice assistant development, content localization, and accessibility tools for visually impaired users. The model operates under a custom license and is hosted on Hugging Face."
      }
    ],
    "slu": [],
    "speaker_separation": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "pyannote/wespeaker-voxceleb-resnet34-LM",
        "source": "hf",
        "name": "wespeaker-voxceleb-resnet34-LM",
        "url": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "wespeaker",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-recognition",
          "speaker-verification",
          "speaker-identification",
          "speaker-embedding",
          "dataset:voxceleb",
          "license:cc-by-4.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 17865623,
          "likes_total": 76,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "wespeaker-voxceleb-resnet34-LM 是一个用于说话人识别的深度学习模型，主要功能是从音频中提取表征说话人身份的嵌入向量。该模型基于 ResNet-34 架构，在 VoxCeleb 数据集上训练，并融合语言模型评分以提升识别精度。核心能力包括生成固定维度的说话人特征向量，适用于说话人验证和辨认任务。其优势在于对多样语音条件下的说话人区分具有较强鲁棒性。典型应用场景包括说话人日志生成、声纹认证系统、以及多媒体内容中的说话人分离与分析，适用于需要准确识别不同说话",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "graph_augmented_reco",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech segments to generate fixed-dimensional vectors that uniquely represent individual speakers. The model leverages a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved accuracy. Key strengths include robust performance in speaker verification and identification tasks across diverse conditions. Typical use cases encompass speaker diarization, voice authentication systems, and multimedia content analysis where distinguishing between speakers is essential.",
        "summary_zh": "wespeaker-voxceleb-resnet34-LM 是一个用于说话人识别的深度学习模型，主要功能是从音频中提取表征说话人身份的嵌入向量。该模型基于 ResNet-34 架构，在 VoxCeleb 数据集上训练，并融合语言模型评分以提升识别精度。核心能力包括生成固定维度的说话人特征向量，适用于说话人验证和辨认任务。其优势在于对多样语音条件下的说话人区分具有较强鲁棒性。典型应用场景包括说话人日志生成、声纹认证系统、以及多媒体内容中的说话人分离与分析，适用于需要准确识别不同说话",
        "summary_es": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech segments to generate fixed-dimensional vectors that uniquely represent individual speakers. The model leverages a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved accuracy. Key strengths include robust performance in speaker verification and identification tasks across diverse conditions. Typical use cases encompass speaker diarization, voice authentication systems, and multimedia content analysis where distinguishing between speakers is essential."
      },
      {
        "id": "pyannote/segmentation",
        "source": "hf",
        "name": "segmentation",
        "url": "https://huggingface.co/pyannote/segmentation",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "arxiv:2104.04045",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6591894,
          "likes_total": 641,
          "downloads_7d": 0,
          "likes_7d": 3
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.32376268146237963,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition.",
        "summary_zh": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "summary_es": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition."
      }
    ],
    "noise_separation": [],
    "full_duplex_dialogue": [],
    "avsr": [],
    "multimodal_dialogue_system": [
      {
        "id": "Qwen/Qwen2.5-VL-7B-Instruct",
        "source": "hf",
        "name": "Qwen2.5-VL-7B-Instruct",
        "url": "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "qwen2_5_vl",
          "image-to-text",
          "multimodal",
          "image-text-to-text",
          "conversational",
          "en",
          "arxiv:2309.00071",
          "arxiv:2409.12191",
          "arxiv:2308.12966",
          "license:apache-2.0",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 4324866,
          "likes_total": 1266,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态语言模型，专为视觉-语言理解与生成任务设计。其核心能力包括处理图像和文本输入以生成上下文相关的响应，支持对话交互、图像描述和视觉问答。主要优势在于高效的参数规模、与推理端点的兼容性以及Apache 2.0开源许可。典型应用场景涵盖人工智能助手、内容分析和教育工具，适用于需要整合图像与文本处理的领域，基于Transformer架构实现稳定性能。该模型通过多模态数据训练，提升了在复杂视觉语言任务中的实用性。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.0999698378058775,
        "task_keys": [
          "image_text_alignment",
          "multimodal_understanding_generation",
          "vqa",
          "visual_grounding",
          "lightweight_visual_model",
          "multimodal_dialogue_system"
        ],
        "summary_en": "Qwen2.5-VL-7B-Instruct is a 7-billion-parameter multimodal language model designed for visual-language understanding and generation tasks. Its core capabilities include processing both images and text to generate contextual responses, supporting conversational interactions, image captioning, and visual question answering. Key strengths encompass its efficient parameter size, compatibility with inference endpoints, and Apache 2.0 licensing for open use. Typical applications involve AI assistants, content analysis, and educational tools where integrated image-text processing is required, leveraging transformer architecture for robust performance.",
        "summary_zh": "Qwen2.5-VL-7B-Instruct 是一个拥有70亿参数的多模态语言模型，专为视觉-语言理解与生成任务设计。其核心能力包括处理图像和文本输入以生成上下文相关的响应，支持对话交互、图像描述和视觉问答。主要优势在于高效的参数规模、与推理端点的兼容性以及Apache 2.0开源许可。典型应用场景涵盖人工智能助手、内容分析和教育工具，适用于需要整合图像与文本处理的领域，基于Transformer架构实现稳定性能。该模型通过多模态数据训练，提升了在复杂视觉语言任务中的实用性。",
        "summary_es": "Qwen2.5-VL-7B-Instruct is a 7-billion-parameter multimodal language model designed for visual-language understanding and generation tasks. Its core capabilities include processing both images and text to generate contextual responses, supporting conversational interactions, image captioning, and visual question answering. Key strengths encompass its efficient parameter size, compatibility with inference endpoints, and Apache 2.0 licensing for open use. Typical applications involve AI assistants, content analysis, and educational tools where integrated image-text processing is required, leveraging transformer architecture for robust performance."
      }
    ],
    "lightweight_multimodal_model": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "gnn": [],
    "kg_construction": [],
    "kg_reasoning": [],
    "general_recommendation": [],
    "vertical_recommendation": [],
    "vector_retrieval": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "pyannote/wespeaker-voxceleb-resnet34-LM",
        "source": "hf",
        "name": "wespeaker-voxceleb-resnet34-LM",
        "url": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "wespeaker",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-recognition",
          "speaker-verification",
          "speaker-identification",
          "speaker-embedding",
          "dataset:voxceleb",
          "license:cc-by-4.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 17865623,
          "likes_total": 76,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "wespeaker-voxceleb-resnet34-LM 是一个用于说话人识别的深度学习模型，主要功能是从音频中提取表征说话人身份的嵌入向量。该模型基于 ResNet-34 架构，在 VoxCeleb 数据集上训练，并融合语言模型评分以提升识别精度。核心能力包括生成固定维度的说话人特征向量，适用于说话人验证和辨认任务。其优势在于对多样语音条件下的说话人区分具有较强鲁棒性。典型应用场景包括说话人日志生成、声纹认证系统、以及多媒体内容中的说话人分离与分析，适用于需要准确识别不同说话",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "graph_augmented_reco",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech segments to generate fixed-dimensional vectors that uniquely represent individual speakers. The model leverages a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved accuracy. Key strengths include robust performance in speaker verification and identification tasks across diverse conditions. Typical use cases encompass speaker diarization, voice authentication systems, and multimedia content analysis where distinguishing between speakers is essential.",
        "summary_zh": "wespeaker-voxceleb-resnet34-LM 是一个用于说话人识别的深度学习模型，主要功能是从音频中提取表征说话人身份的嵌入向量。该模型基于 ResNet-34 架构，在 VoxCeleb 数据集上训练，并融合语言模型评分以提升识别精度。核心能力包括生成固定维度的说话人特征向量，适用于说话人验证和辨认任务。其优势在于对多样语音条件下的说话人区分具有较强鲁棒性。典型应用场景包括说话人日志生成、声纹认证系统、以及多媒体内容中的说话人分离与分析，适用于需要准确识别不同说话",
        "summary_es": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech segments to generate fixed-dimensional vectors that uniquely represent individual speakers. The model leverages a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved accuracy. Key strengths include robust performance in speaker verification and identification tasks across diverse conditions. Typical use cases encompass speaker diarization, voice authentication systems, and multimedia content analysis where distinguishing between speakers is essential."
      },
      {
        "id": "pyannote/segmentation",
        "source": "hf",
        "name": "segmentation",
        "url": "https://huggingface.co/pyannote/segmentation",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "arxiv:2104.04045",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6591894,
          "likes_total": 641,
          "downloads_7d": 0,
          "likes_7d": 3
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.32376268146237963,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition.",
        "summary_zh": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "summary_es": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition."
      }
    ],
    "vector_db_optimization": [],
    "metric_learning": [],
    "contrastive_learning": [],
    "ltr": [
      {
        "id": "Bingsu/adetailer",
        "source": "hf",
        "name": "adetailer",
        "url": "https://huggingface.co/Bingsu/adetailer",
        "tags": [
          "ultralytics",
          "pytorch",
          "dataset:wider_face",
          "dataset:skytnt/anime-segmentation",
          "doi:10.57967/hf/3633",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15202018,
          "likes_total": 617,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括使用预训练模型进行物体检测、精确区域识别的分割技术，以及自动修复和细化处理。主要优势在于处理低分辨率或模糊输入、改善面部特征，并能有效处理动漫风格内容。典型应用场景包括AI生成流程中的图像预处理、照片修复，以及无需手动干预即可增强数字艺术作品中的特定元素。该工具基于PyTorch开发，整合了动漫分割和面部检测数据",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "ltr",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection using pre-trained models, segmentation for precise area identification, and automated inpainting/refinement. Key strengths are handling low-resolution or blurry inputs, improving facial features, and working with anime-style content. Typical use cases involve image preprocessing for AI generation pipelines, photo restoration, and enhancing specific elements in digital artwork without manual intervention.",
        "summary_zh": "ADetailer 是一款计算机视觉工具，旨在自动检测并增强图像中的细节，特别是面部和其他区域。其核心功能包括使用预训练模型进行物体检测、精确区域识别的分割技术，以及自动修复和细化处理。主要优势在于处理低分辨率或模糊输入、改善面部特征，并能有效处理动漫风格内容。典型应用场景包括AI生成流程中的图像预处理、照片修复，以及无需手动干预即可增强数字艺术作品中的特定元素。该工具基于PyTorch开发，整合了动漫分割和面部检测数据",
        "summary_es": "ADetailer is a computer vision tool designed to automatically detect and enhance details in images, particularly faces and other regions. Its core capabilities include object detection using pre-trained models, segmentation for precise area identification, and automated inpainting/refinement. Key strengths are handling low-resolution or blurry inputs, improving facial features, and working with anime-style content. Typical use cases involve image preprocessing for AI generation pipelines, photo restoration, and enhancing specific elements in digital artwork without manual intervention."
      }
    ],
    "neural_retrieval": [
      {
        "id": "colbert-ir/colbertv2.0",
        "source": "hf",
        "name": "colbertv2.0",
        "url": "https://huggingface.co/colbert-ir/colbertv2.0",
        "tags": [
          "transformers",
          "pytorch",
          "onnx",
          "safetensors",
          "bert",
          "ColBERT",
          "en",
          "arxiv:2004.12832",
          "arxiv:2007.00814",
          "arxiv:2101.00436",
          "arxiv:2112.01488",
          "arxiv:2205.09707",
          "license:mit",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6540699,
          "likes_total": 286,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "ColBERTv2.0是一种基于神经网络的检索模型，通过查询与文档的延迟交互机制提升信息检索效果。该模型利用BERT将文本编码为细粒度上下文嵌入向量，支持高效相似度匹配而无需完整交叉注意力计算。核心能力包括基于精确向量搜索的密集段落检索，兼容批内训练和端到端训练模式。主要优势在于平衡检索效果与系统可扩展性，既优于传统词项匹配方法，又比交叉编码器模型更高效。典型应用场景涵盖网络搜索引擎、智能问答系统以及企业文",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "neural_retrieval"
        ],
        "summary_en": "ColBERTv2.0 is a neural retrieval model that enhances information retrieval through late interaction between queries and documents. It encodes text into fine-grained contextual embeddings using BERT, enabling efficient similarity matching without full cross-attention. Core capabilities include dense passage retrieval with exact vector search, supporting both in-batch and end-to-end training. Strengths lie in balancing effectiveness with scalability, outperforming traditional term-matching while being more efficient than cross-encoder models. Typical use cases include web search, question answering systems, and enterprise document retrieval where precise semantic matching is required.",
        "summary_zh": "ColBERTv2.0是一种基于神经网络的检索模型，通过查询与文档的延迟交互机制提升信息检索效果。该模型利用BERT将文本编码为细粒度上下文嵌入向量，支持高效相似度匹配而无需完整交叉注意力计算。核心能力包括基于精确向量搜索的密集段落检索，兼容批内训练和端到端训练模式。主要优势在于平衡检索效果与系统可扩展性，既优于传统词项匹配方法，又比交叉编码器模型更高效。典型应用场景涵盖网络搜索引擎、智能问答系统以及企业文",
        "summary_es": "ColBERTv2.0 is a neural retrieval model that enhances information retrieval through late interaction between queries and documents. It encodes text into fine-grained contextual embeddings using BERT, enabling efficient similarity matching without full cross-attention. Core capabilities include dense passage retrieval with exact vector search, supporting both in-batch and end-to-end training. Strengths lie in balancing effectiveness with scalability, outperforming traditional term-matching while being more efficient than cross-encoder models. Typical use cases include web search, question answering systems, and enterprise document retrieval where precise semantic matching is required."
      }
    ],
    "graph_augmented_reco": [
      {
        "id": "pyannote/wespeaker-voxceleb-resnet34-LM",
        "source": "hf",
        "name": "wespeaker-voxceleb-resnet34-LM",
        "url": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "wespeaker",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-recognition",
          "speaker-verification",
          "speaker-identification",
          "speaker-embedding",
          "dataset:voxceleb",
          "license:cc-by-4.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 17865623,
          "likes_total": 76,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "wespeaker-voxceleb-resnet34-LM 是一个用于说话人识别的深度学习模型，主要功能是从音频中提取表征说话人身份的嵌入向量。该模型基于 ResNet-34 架构，在 VoxCeleb 数据集上训练，并融合语言模型评分以提升识别精度。核心能力包括生成固定维度的说话人特征向量，适用于说话人验证和辨认任务。其优势在于对多样语音条件下的说话人区分具有较强鲁棒性。典型应用场景包括说话人日志生成、声纹认证系统、以及多媒体内容中的说话人分离与分析，适用于需要准确识别不同说话",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "graph_augmented_reco",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech segments to generate fixed-dimensional vectors that uniquely represent individual speakers. The model leverages a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved accuracy. Key strengths include robust performance in speaker verification and identification tasks across diverse conditions. Typical use cases encompass speaker diarization, voice authentication systems, and multimedia content analysis where distinguishing between speakers is essential.",
        "summary_zh": "wespeaker-voxceleb-resnet34-LM 是一个用于说话人识别的深度学习模型，主要功能是从音频中提取表征说话人身份的嵌入向量。该模型基于 ResNet-34 架构，在 VoxCeleb 数据集上训练，并融合语言模型评分以提升识别精度。核心能力包括生成固定维度的说话人特征向量，适用于说话人验证和辨认任务。其优势在于对多样语音条件下的说话人区分具有较强鲁棒性。典型应用场景包括说话人日志生成、声纹认证系统、以及多媒体内容中的说话人分离与分析，适用于需要准确识别不同说话",
        "summary_es": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech segments to generate fixed-dimensional vectors that uniquely represent individual speakers. The model leverages a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved accuracy. Key strengths include robust performance in speaker verification and identification tasks across diverse conditions. Typical use cases encompass speaker diarization, voice authentication systems, and multimedia content analysis where distinguishing between speakers is essential."
      },
      {
        "id": "pyannote/speaker-diarization-3.1",
        "source": "hf",
        "name": "speaker-diarization-3.1",
        "url": "https://huggingface.co/pyannote/speaker-diarization-3.1",
        "tags": [
          "pyannote-audio",
          "pyannote",
          "pyannote-audio-pipeline",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "automatic-speech-recognition",
          "arxiv:2111.14448",
          "arxiv:2012.01477",
          "license:mit",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 16721804,
          "likes_total": 1169,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote speaker-diarization-3.1 是一个音频处理流程，专门用于对录音进行分段并识别不同说话者。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该系统能够区分不同的说话者，并识别多人同时说话的情况。主要优势在于对各种音频质量的鲁棒性表现以及与Hugging Face端点的兼容性。典型应用场景包括会议记录转录、广播内容监控、播客分析和司法音频检查，这些场景都需要精确的说话人识别和时间线标注。该工具基于多项研究成果开发，采用MIT许可证，适用",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "asr",
          "speaker_separation",
          "vector_retrieval",
          "graph_augmented_reco",
          "auto_evaluation_models",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote speaker-diarization-3.1 is an audio processing pipeline designed to segment and identify speakers in audio recordings. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The system can distinguish between different speakers and determine when multiple people speak simultaneously. Strengths include robust performance on various audio qualities and compatibility with Hugging Face endpoints. Typical use cases involve meeting transcription, broadcast monitoring, podcast analysis, and forensic audio examination where speaker identification and timeline annotation are required.",
        "summary_zh": "Pyannote speaker-diarization-3.1 是一个音频处理流程，专门用于对录音进行分段并识别不同说话者。其核心功能包括语音活动检测、说话人变更检测和重叠语音检测。该系统能够区分不同的说话者，并识别多人同时说话的情况。主要优势在于对各种音频质量的鲁棒性表现以及与Hugging Face端点的兼容性。典型应用场景包括会议记录转录、广播内容监控、播客分析和司法音频检查，这些场景都需要精确的说话人识别和时间线标注。该工具基于多项研究成果开发，采用MIT许可证，适用",
        "summary_es": "Pyannote speaker-diarization-3.1 is an audio processing pipeline designed to segment and identify speakers in audio recordings. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The system can distinguish between different speakers and determine when multiple people speak simultaneously. Strengths include robust performance on various audio qualities and compatibility with Hugging Face endpoints. Typical use cases involve meeting transcription, broadcast monitoring, podcast analysis, and forensic audio examination where speaker identification and timeline annotation are required."
      }
    ],
    "model_compression": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "model_quantization": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "model_distillation": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "compilation_optimization": [],
    "inference_acceleration": [
      {
        "id": "sentence-transformers/all-MiniLM-L6-v2",
        "source": "hf",
        "name": "all-MiniLM-L6-v2",
        "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
        "tags": [
          "sentence-transformers",
          "pytorch",
          "tf",
          "rust",
          "onnx",
          "safetensors",
          "openvino",
          "bert",
          "feature-extraction",
          "sentence-similarity",
          "transformers",
          "en",
          "dataset:s2orc",
          "dataset:flax-sentence-embeddings/stackexchange_xml",
          "dataset:ms_marco",
          "dataset:gooaq",
          "dataset:yahoo_answers_topics",
          "dataset:code_search_net",
          "dataset:search_qa",
          "dataset:eli5",
          "dataset:snli",
          "dataset:multi_nli",
          "dataset:wikihow",
          "dataset:natural_questions",
          "dataset:trivia_qa",
          "dataset:embedding-data/sentence-compression",
          "dataset:embedding-data/flickr30k-captions",
          "dataset:embedding-data/altlex",
          "dataset:embedding-data/simple-wiki",
          "dataset:embedding-data/QQP",
          "dataset:embedding-data/SPECTER",
          "dataset:embedding-data/PAQ_pairs",
          "dataset:embedding-data/WikiAnswers",
          "arxiv:1904.06472",
          "arxiv:2102.07033",
          "arxiv:2104.08727",
          "arxiv:1704.05179",
          "arxiv:1810.09305",
          "license:apache-2.0",
          "autotrain_compatible",
          "text-embeddings-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 90137861,
          "likes_total": 3932,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "all-MiniLM-L6-v2 是一个紧凑型句子转换模型，专门用于高效生成文本嵌入向量。该模型将文本映射到384维向量空间，支持语义相似度计算。核心功能包括句子嵌入生成、语义搜索和文本聚类分析。主要优势在于模型体积小（22MB）、推理速度快且性能均衡。典型应用场景涵盖信息检索系统、重复内容检测、推荐算法和文本分类任务。模型采用知识蒸馏技术训练，使用了QQP、WikiAnswers、MS MARCO等多个数据集进行优化，适用于资源受限环境下的自然语言处理应用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "inference_acceleration"
        ],
        "summary_en": "The all-MiniLM-L6-v2 model is a compact sentence transformer designed for efficient text embedding generation. It maps text to 384-dimensional vectors, enabling semantic similarity comparisons. Core capabilities include sentence embeddings, semantic search, and clustering. Strengths are its small size (22MB), fast inference, and balanced performance. Typical use cases encompass information retrieval, duplicate detection, recommendation systems, and text classification tasks. The model was trained using knowledge distillation techniques on diverse datasets including QQP, WikiAnswers, and MS MARCO.",
        "summary_zh": "all-MiniLM-L6-v2 是一个紧凑型句子转换模型，专门用于高效生成文本嵌入向量。该模型将文本映射到384维向量空间，支持语义相似度计算。核心功能包括句子嵌入生成、语义搜索和文本聚类分析。主要优势在于模型体积小（22MB）、推理速度快且性能均衡。典型应用场景涵盖信息检索系统、重复内容检测、推荐算法和文本分类任务。模型采用知识蒸馏技术训练，使用了QQP、WikiAnswers、MS MARCO等多个数据集进行优化，适用于资源受限环境下的自然语言处理应用。",
        "summary_es": "The all-MiniLM-L6-v2 model is a compact sentence transformer designed for efficient text embedding generation. It maps text to 384-dimensional vectors, enabling semantic similarity comparisons. Core capabilities include sentence embeddings, semantic search, and clustering. Strengths are its small size (22MB), fast inference, and balanced performance. Typical use cases encompass information retrieval, duplicate detection, recommendation systems, and text classification tasks. The model was trained using knowledge distillation techniques on diverse datasets including QQP, WikiAnswers, and MS MARCO."
      },
      {
        "id": "sentence-transformers/all-mpnet-base-v2",
        "source": "hf",
        "name": "all-mpnet-base-v2",
        "url": "https://huggingface.co/sentence-transformers/all-mpnet-base-v2",
        "tags": [
          "sentence-transformers",
          "pytorch",
          "onnx",
          "safetensors",
          "openvino",
          "mpnet",
          "fill-mask",
          "feature-extraction",
          "sentence-similarity",
          "transformers",
          "text-embeddings-inference",
          "en",
          "dataset:s2orc",
          "dataset:flax-sentence-embeddings/stackexchange_xml",
          "dataset:ms_marco",
          "dataset:gooaq",
          "dataset:yahoo_answers_topics",
          "dataset:code_search_net",
          "dataset:search_qa",
          "dataset:eli5",
          "dataset:snli",
          "dataset:multi_nli",
          "dataset:wikihow",
          "dataset:natural_questions",
          "dataset:trivia_qa",
          "dataset:embedding-data/sentence-compression",
          "dataset:embedding-data/flickr30k-captions",
          "dataset:embedding-data/altlex",
          "dataset:embedding-data/simple-wiki",
          "dataset:embedding-data/QQP",
          "dataset:embedding-data/SPECTER",
          "dataset:embedding-data/PAQ_pairs",
          "dataset:embedding-data/WikiAnswers",
          "arxiv:1904.06472",
          "arxiv:2102.07033",
          "arxiv:2104.08727",
          "arxiv:1704.05179",
          "arxiv:1810.09305",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 16922032,
          "likes_total": 1163,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "all-mpnet-base-v2 是一种句子嵌入模型，专门用于将文本转换为高维向量表示以处理语义相似性任务。该模型基于 MPNet 架构，通过对比学习优化生成 768 维嵌入向量。其核心能力在于理解语义含义而非简单关键词匹配，优势包括在不同领域表现稳健且能高效处理句子级任务。典型应用场景涵盖语义搜索、信息检索、相似文档聚类，以及支持下游自然语言处理应用如推荐系统和重复检测。模型训练使用了多种数据集，确保其泛化能力和实用性。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "inference_acceleration"
        ],
        "summary_en": "all-mpnet-base-v2 is a sentence embedding model that converts text into high-dimensional vector representations for semantic similarity tasks. Based on MPNet architecture, it generates 768-dimensional embeddings optimized through contrastive learning. Its core capability lies in understanding semantic meaning rather than exact keyword matching. Strengths include robust performance across diverse domains and efficient handling of sentence-level tasks. Typical use cases encompass semantic search, information retrieval, clustering similar documents, and supporting downstream NLP applications like recommendation systems and duplicate detection.",
        "summary_zh": "all-mpnet-base-v2 是一种句子嵌入模型，专门用于将文本转换为高维向量表示以处理语义相似性任务。该模型基于 MPNet 架构，通过对比学习优化生成 768 维嵌入向量。其核心能力在于理解语义含义而非简单关键词匹配，优势包括在不同领域表现稳健且能高效处理句子级任务。典型应用场景涵盖语义搜索、信息检索、相似文档聚类，以及支持下游自然语言处理应用如推荐系统和重复检测。模型训练使用了多种数据集，确保其泛化能力和实用性。",
        "summary_es": "all-mpnet-base-v2 is a sentence embedding model that converts text into high-dimensional vector representations for semantic similarity tasks. Based on MPNet architecture, it generates 768-dimensional embeddings optimized through contrastive learning. Its core capability lies in understanding semantic meaning rather than exact keyword matching. Strengths include robust performance across diverse domains and efficient handling of sentence-level tasks. Typical use cases encompass semantic search, information retrieval, clustering similar documents, and supporting downstream NLP applications like recommendation systems and duplicate detection."
      },
      {
        "id": "openai/gpt-oss-20b",
        "source": "hf",
        "name": "gpt-oss-20b",
        "url": "https://huggingface.co/openai/gpt-oss-20b",
        "tags": [
          "transformers",
          "safetensors",
          "gpt_oss",
          "text-generation",
          "vllm",
          "conversational",
          "arxiv:2508.10925",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "8-bit",
          "mxfp4",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6823231,
          "likes_total": 3608,
          "downloads_7d": 0,
          "likes_7d": 56
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "GPT-OSS-20B是OpenAI开发的200亿参数开源语言模型，专为高效文本生成设计。该模型采用8位量化和MXFP4精度技术，在保持性能的同时显著降低计算资源需求。核心能力包括对话式AI、内容创作和代码生成，优势在于模型规模与推理效率的良好平衡，使其适合在有限硬件资源上部署。典型应用场景涵盖聊天机器人、写作助手和自动化文档系统。模型基于Apache 2.0许可证开源，兼容Transformers和vLLM等主流框架，支持多种部署方式。其技术细节在相关学术论文中有详细阐述，为研究社区提供了",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 1.2052310655075504,
        "task_keys": [
          "inference_acceleration"
        ],
        "summary_en": "GPT-OSS-20B is a 20-billion-parameter open-source language model developed by OpenAI, designed for efficient text generation. It employs 8-bit quantization and MXFP4 precision to reduce computational requirements while maintaining performance. Core capabilities include conversational AI, content creation, and code generation. Strengths lie in its balance of model size and inference efficiency, making it suitable for deployment on limited hardware. Typical use cases encompass chatbots, writing assistants, and automated documentation systems. The model is licensed under Apache 2.0 and compatible with popular frameworks like Transformers and vLLM.",
        "summary_zh": "GPT-OSS-20B是OpenAI开发的200亿参数开源语言模型，专为高效文本生成设计。该模型采用8位量化和MXFP4精度技术，在保持性能的同时显著降低计算资源需求。核心能力包括对话式AI、内容创作和代码生成，优势在于模型规模与推理效率的良好平衡，使其适合在有限硬件资源上部署。典型应用场景涵盖聊天机器人、写作助手和自动化文档系统。模型基于Apache 2.0许可证开源，兼容Transformers和vLLM等主流框架，支持多种部署方式。其技术细节在相关学术论文中有详细阐述，为研究社区提供了",
        "summary_es": "GPT-OSS-20B is a 20-billion-parameter open-source language model developed by OpenAI, designed for efficient text generation. It employs 8-bit quantization and MXFP4 precision to reduce computational requirements while maintaining performance. Core capabilities include conversational AI, content creation, and code generation. Strengths lie in its balance of model size and inference efficiency, making it suitable for deployment on limited hardware. Typical use cases encompass chatbots, writing assistants, and automated documentation systems. The model is licensed under Apache 2.0 and compatible with popular frameworks like Transformers and vLLM."
      }
    ],
    "federated_learning": [],
    "privacy_computing": [],
    "adversarial_attack": [],
    "adversarial_defense": [],
    "red_teaming": [],
    "content_moderation": [],
    "auto_evaluation_models": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 98988953,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards.",
        "summary_zh": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "edge_hw_sw_co_design": [],
    "xai": [],
    "ai_ethics_risk_assessment": [],
    "training_data_anonymization": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "google-bert/bert-base-uncased",
        "source": "hf",
        "name": "bert-base-uncased",
        "url": "https://huggingface.co/google-bert/bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "coreml",
          "onnx",
          "safetensors",
          "bert",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1810.04805",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 55171860,
          "likes_total": 2417,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "BERT-base-uncased 是基于Transformer架构的英语预训练模型，使用Wikipedia和BookCorpus数据进行训练，采用掩码语言建模和下一句预测目标。核心能力在于双向理解文本上下文，支持多种自然语言处理任务。主要优势包括在文本分类、命名实体识别和问答任务上的优异表现，且无需针对特定任务修改模型架构。典型应用场景包括情感分析、信息抽取和文本理解等下游任务的微调。该模型作为英语语言处理的基础模型，具有广泛的适用性和良好的迁移学习能力。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus using masked language modeling and next sentence prediction. Its core capability is bidirectional context understanding for various NLP tasks. Key strengths include strong performance on text classification, named entity recognition, and question answering without task-specific architecture changes. Typical use cases involve fine-tuning for sentiment analysis, information extraction, and text understanding applications. The model serves as a versatile baseline for English language processing.",
        "summary_zh": "BERT-base-uncased 是基于Transformer架构的英语预训练模型，使用Wikipedia和BookCorpus数据进行训练，采用掩码语言建模和下一句预测目标。核心能力在于双向理解文本上下文，支持多种自然语言处理任务。主要优势包括在文本分类、命名实体识别和问答任务上的优异表现，且无需针对特定任务修改模型架构。典型应用场景包括情感分析、信息抽取和文本理解等下游任务的微调。该模型作为英语语言处理的基础模型，具有广泛的适用性和良好的迁移学习能力。",
        "summary_es": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus using masked language modeling and next sentence prediction. Its core capability is bidirectional context understanding for various NLP tasks. Key strengths include strong performance on text classification, named entity recognition, and question answering without task-specific architecture changes. Typical use cases involve fine-tuning for sentiment analysis, information extraction, and text understanding applications. The model serves as a versatile baseline for English language processing."
      },
      {
        "id": "FacebookAI/roberta-large",
        "source": "hf",
        "name": "roberta-large",
        "url": "https://huggingface.co/FacebookAI/roberta-large",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "onnx",
          "safetensors",
          "roberta",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1907.11692",
          "arxiv:1806.02847",
          "license:mit",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 13086830,
          "likes_total": 247,
          "downloads_7d": 833951,
          "likes_7d": 1
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "RoBERTa-large是由Facebook AI开发的BERT预训练优化模型，移除了BERT的下句预测目标，采用更大批次和更多数据进行训练。该模型包含24层、16个注意力头和3.55亿参数，核心能力包括掩码语言建模、文本分类和自然语言推理。其优势在于GLUE和SQuAD等基准测试中表现优于原始BERT，典型应用涵盖情感分析、问答系统和文本分类等多个领域。模型基于BookCorpus和Wikipedia数据训练，支持PyTorch、TensorFlow和JAX框架，适用于各种自然语言处理任务。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.4257887794444891,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "RoBERTa-large is a robustly optimized BERT pretraining approach developed by Facebook AI. It removes BERT's next-sentence prediction objective and trains with larger batches and more data. The model features 24 layers, 16 attention heads, and 355 million parameters. Its core capabilities include masked language modeling, text classification, and natural language inference. Strengths include superior performance on GLUE and SQuAD benchmarks compared to original BERT. Typical use cases span sentiment analysis, question answering, and text classification tasks across various domains.",
        "summary_zh": "RoBERTa-large是由Facebook AI开发的BERT预训练优化模型，移除了BERT的下句预测目标，采用更大批次和更多数据进行训练。该模型包含24层、16个注意力头和3.55亿参数，核心能力包括掩码语言建模、文本分类和自然语言推理。其优势在于GLUE和SQuAD等基准测试中表现优于原始BERT，典型应用涵盖情感分析、问答系统和文本分类等多个领域。模型基于BookCorpus和Wikipedia数据训练，支持PyTorch、TensorFlow和JAX框架，适用于各种自然语言处理任务。",
        "summary_es": "RoBERTa-large is a robustly optimized BERT pretraining approach developed by Facebook AI. It removes BERT's next-sentence prediction objective and trains with larger batches and more data. The model features 24 layers, 16 attention heads, and 355 million parameters. Its core capabilities include masked language modeling, text classification, and natural language inference. Strengths include superior performance on GLUE and SQuAD benchmarks compared to original BERT. Typical use cases span sentiment analysis, question answering, and text classification tasks across various domains."
      }
    ],
    "training_data_copyright": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "google-bert/bert-base-uncased",
        "source": "hf",
        "name": "bert-base-uncased",
        "url": "https://huggingface.co/google-bert/bert-base-uncased",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "rust",
          "coreml",
          "onnx",
          "safetensors",
          "bert",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1810.04805",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 55171860,
          "likes_total": 2417,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "BERT-base-uncased 是基于Transformer架构的英语预训练模型，使用Wikipedia和BookCorpus数据进行训练，采用掩码语言建模和下一句预测目标。核心能力在于双向理解文本上下文，支持多种自然语言处理任务。主要优势包括在文本分类、命名实体识别和问答任务上的优异表现，且无需针对特定任务修改模型架构。典型应用场景包括情感分析、信息抽取和文本理解等下游任务的微调。该模型作为英语语言处理的基础模型，具有广泛的适用性和良好的迁移学习能力。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus using masked language modeling and next sentence prediction. Its core capability is bidirectional context understanding for various NLP tasks. Key strengths include strong performance on text classification, named entity recognition, and question answering without task-specific architecture changes. Typical use cases involve fine-tuning for sentiment analysis, information extraction, and text understanding applications. The model serves as a versatile baseline for English language processing.",
        "summary_zh": "BERT-base-uncased 是基于Transformer架构的英语预训练模型，使用Wikipedia和BookCorpus数据进行训练，采用掩码语言建模和下一句预测目标。核心能力在于双向理解文本上下文，支持多种自然语言处理任务。主要优势包括在文本分类、命名实体识别和问答任务上的优异表现，且无需针对特定任务修改模型架构。典型应用场景包括情感分析、信息抽取和文本理解等下游任务的微调。该模型作为英语语言处理的基础模型，具有广泛的适用性和良好的迁移学习能力。",
        "summary_es": "BERT-base-uncased is a foundational transformer model pre-trained on English text from Wikipedia and BookCorpus using masked language modeling and next sentence prediction. Its core capability is bidirectional context understanding for various NLP tasks. Key strengths include strong performance on text classification, named entity recognition, and question answering without task-specific architecture changes. Typical use cases involve fine-tuning for sentiment analysis, information extraction, and text understanding applications. The model serves as a versatile baseline for English language processing."
      },
      {
        "id": "FacebookAI/roberta-large",
        "source": "hf",
        "name": "roberta-large",
        "url": "https://huggingface.co/FacebookAI/roberta-large",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "onnx",
          "safetensors",
          "roberta",
          "fill-mask",
          "exbert",
          "en",
          "dataset:bookcorpus",
          "dataset:wikipedia",
          "arxiv:1907.11692",
          "arxiv:1806.02847",
          "license:mit",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 13086830,
          "likes_total": 247,
          "downloads_7d": 833951,
          "likes_7d": 1
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "RoBERTa-large是由Facebook AI开发的BERT预训练优化模型，移除了BERT的下句预测目标，采用更大批次和更多数据进行训练。该模型包含24层、16个注意力头和3.55亿参数，核心能力包括掩码语言建模、文本分类和自然语言推理。其优势在于GLUE和SQuAD等基准测试中表现优于原始BERT，典型应用涵盖情感分析、问答系统和文本分类等多个领域。模型基于BookCorpus和Wikipedia数据训练，支持PyTorch、TensorFlow和JAX框架，适用于各种自然语言处理任务。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.4257887794444891,
        "task_keys": [
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright"
        ],
        "summary_en": "RoBERTa-large is a robustly optimized BERT pretraining approach developed by Facebook AI. It removes BERT's next-sentence prediction objective and trains with larger batches and more data. The model features 24 layers, 16 attention heads, and 355 million parameters. Its core capabilities include masked language modeling, text classification, and natural language inference. Strengths include superior performance on GLUE and SQuAD benchmarks compared to original BERT. Typical use cases span sentiment analysis, question answering, and text classification tasks across various domains.",
        "summary_zh": "RoBERTa-large是由Facebook AI开发的BERT预训练优化模型，移除了BERT的下句预测目标，采用更大批次和更多数据进行训练。该模型包含24层、16个注意力头和3.55亿参数，核心能力包括掩码语言建模、文本分类和自然语言推理。其优势在于GLUE和SQuAD等基准测试中表现优于原始BERT，典型应用涵盖情感分析、问答系统和文本分类等多个领域。模型基于BookCorpus和Wikipedia数据训练，支持PyTorch、TensorFlow和JAX框架，适用于各种自然语言处理任务。",
        "summary_es": "RoBERTa-large is a robustly optimized BERT pretraining approach developed by Facebook AI. It removes BERT's next-sentence prediction objective and trains with larger batches and more data. The model features 24 layers, 16 attention heads, and 355 million parameters. Its core capabilities include masked language modeling, text classification, and natural language inference. Strengths include superior performance on GLUE and SQuAD benchmarks compared to original BERT. Typical use cases span sentiment analysis, question answering, and text classification tasks across various domains."
      }
    ],
    "model_monitoring": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "model_iterative_update": [
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "fluid_simulation": [],
    "material_design": [],
    "drug_molecule_prediction": [],
    "robotic_vision": [
      {
        "id": "openai/clip-vit-base-patch32",
        "source": "hf",
        "name": "clip-vit-base-patch32",
        "url": "https://huggingface.co/openai/clip-vit-base-patch32",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "clip",
          "zero-shot-image-classification",
          "vision",
          "arxiv:2103.00020",
          "arxiv:1908.04913",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 15179264,
          "likes_total": 770,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "CLIP-ViT-Base-Patch32是OpenAI开发的多模态人工智能模型，专门用于连接视觉与语言理解。该模型采用Vision Transformer架构，以32x32像素块处理图像，并与文本编码器配对工作。核心能力为零样本图像分类，能够根据自然语言描述对图像进行分类，无需针对特定任务进行训练。主要优势在于从网络规模的图像-文本对中学习视觉概念，实现跨领域的灵活应用。典型使用场景包括内容审核、视觉搜索、图像描述生成，以及任何需要通过文本提示理解图像内容的场合。模型基于Transformer技术，支持多种",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "robotic_vision"
        ],
        "summary_en": "CLIP-ViT-Base-Patch32 is a multimodal AI model developed by OpenAI that connects vision and language. It uses a Vision Transformer (ViT) architecture with 32x32 pixel patches to process images, paired with a text encoder. The model's core capability is zero-shot image classification, where it can categorize images based on natural language descriptions without task-specific training. Its main strength lies in learning visual concepts from web-scale image-text pairs, enabling flexible application across diverse domains. Typical use cases include content moderation, visual search, image captioning, and any scenario requiring understanding of images through textual prompts.",
        "summary_zh": "CLIP-ViT-Base-Patch32是OpenAI开发的多模态人工智能模型，专门用于连接视觉与语言理解。该模型采用Vision Transformer架构，以32x32像素块处理图像，并与文本编码器配对工作。核心能力为零样本图像分类，能够根据自然语言描述对图像进行分类，无需针对特定任务进行训练。主要优势在于从网络规模的图像-文本对中学习视觉概念，实现跨领域的灵活应用。典型使用场景包括内容审核、视觉搜索、图像描述生成，以及任何需要通过文本提示理解图像内容的场合。模型基于Transformer技术，支持多种",
        "summary_es": "CLIP-ViT-Base-Patch32 is a multimodal AI model developed by OpenAI that connects vision and language. It uses a Vision Transformer (ViT) architecture with 32x32 pixel patches to process images, paired with a text encoder. The model's core capability is zero-shot image classification, where it can categorize images based on natural language descriptions without task-specific training. Its main strength lies in learning visual concepts from web-scale image-text pairs, enabling flexible application across diverse domains. Typical use cases include content moderation, visual search, image captioning, and any scenario requiring understanding of images through textual prompts."
      },
      {
        "id": "openai/clip-vit-large-patch14",
        "source": "hf",
        "name": "clip-vit-large-patch14",
        "url": "https://huggingface.co/openai/clip-vit-large-patch14",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "safetensors",
          "clip",
          "zero-shot-image-classification",
          "vision",
          "arxiv:2103.00020",
          "arxiv:1908.04913",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8008452,
          "likes_total": 1865,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "CLIP-ViT-Large-Patch14是由OpenAI开发的多模态神经网络，旨在连接视觉与语言理解。其核心能力在于无需特定任务训练即可实现零样本图像分类，通过将图像与自然语言描述进行匹配。该模型采用Vision Transformer（ViT-Large）架构，使用14x14图像块处理，并在4亿个图像-文本对上训练。主要优势包括对多样化视觉概念的强大泛化能力以及灵活的下游应用集成。典型应用场景涵盖图像检索、内容审核、视觉问答以及为AI系统增强视觉理解能力，适用于需要跨模态理解的智能应用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "robotic_vision"
        ],
        "summary_en": "CLIP-ViT-Large-Patch14 is a multimodal neural network developed by OpenAI that connects vision and language. Its core capability is zero-shot image classification by matching images with natural language descriptions without task-specific training. The model uses a Vision Transformer (ViT-Large) architecture with 14x14 image patches and is trained on 400 million image-text pairs. Key strengths include strong generalization across diverse visual concepts and flexible integration into downstream applications. Typical use cases encompass image retrieval, content moderation, visual question answering, and enhancing AI systems with visual understanding capabilities.",
        "summary_zh": "CLIP-ViT-Large-Patch14是由OpenAI开发的多模态神经网络，旨在连接视觉与语言理解。其核心能力在于无需特定任务训练即可实现零样本图像分类，通过将图像与自然语言描述进行匹配。该模型采用Vision Transformer（ViT-Large）架构，使用14x14图像块处理，并在4亿个图像-文本对上训练。主要优势包括对多样化视觉概念的强大泛化能力以及灵活的下游应用集成。典型应用场景涵盖图像检索、内容审核、视觉问答以及为AI系统增强视觉理解能力，适用于需要跨模态理解的智能应用。",
        "summary_es": "CLIP-ViT-Large-Patch14 is a multimodal neural network developed by OpenAI that connects vision and language. Its core capability is zero-shot image classification by matching images with natural language descriptions without task-specific training. The model uses a Vision Transformer (ViT-Large) architecture with 14x14 image patches and is trained on 400 million image-text pairs. Key strengths include strong generalization across diverse visual concepts and flexible integration into downstream applications. Typical use cases encompass image retrieval, content moderation, visual question answering, and enhancing AI systems with visual understanding capabilities."
      }
    ],
    "robot_motion_planning": [],
    "robot_environment_interaction": [],
    "molecular_generation": [
      {
        "id": "openai-community/gpt2",
        "source": "hf",
        "name": "gpt2",
        "url": "https://huggingface.co/openai-community/gpt2",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "tflite",
          "rust",
          "onnx",
          "safetensors",
          "gpt2",
          "text-generation",
          "exbert",
          "en",
          "doi:10.57967/hf/0039",
          "license:mit",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 12038015,
          "likes_total": 2955,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本预测和内容创作。其核心功能包括根据输入提示生成连贯的后续文本、完成句子结构以及创作多样化内容。该模型的优势在于无需微调即可实现高质量文本生成、支持多种写作风格且完全开源。典型应用场景涵盖创意写作辅助、聊天机器人对话生成、文本摘要制作以及自然语言处理研究。GPT-2特别擅长保持长文本的上下文一致性，在维持主题连贯性方面表现突出，为开发者和研究者提",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "GPT-2 is a transformer-based language model developed by OpenAI for text generation. Its primary purpose is to predict subsequent text based on input, enabling coherent continuation of prompts. Core capabilities include generating human-like text, completing sentences, and creating content across various domains. Strengths lie in its strong performance without fine-tuning, versatility across writing styles, and open-source availability. Typical use cases encompass creative writing assistance, chatbot responses, content summarization, and language modeling research. The model demonstrates particular effectiveness in maintaining contextual coherence throughout extended text passages.",
        "summary_zh": "GPT-2是由OpenAI开发的基于Transformer架构的语言生成模型，主要用于文本预测和内容创作。其核心功能包括根据输入提示生成连贯的后续文本、完成句子结构以及创作多样化内容。该模型的优势在于无需微调即可实现高质量文本生成、支持多种写作风格且完全开源。典型应用场景涵盖创意写作辅助、聊天机器人对话生成、文本摘要制作以及自然语言处理研究。GPT-2特别擅长保持长文本的上下文一致性，在维持主题连贯性方面表现突出，为开发者和研究者提",
        "summary_es": "GPT-2 is a transformer-based language model developed by OpenAI for text generation. Its primary purpose is to predict subsequent text based on input, enabling coherent continuation of prompts. Core capabilities include generating human-like text, completing sentences, and creating content across various domains. Strengths lie in its strong performance without fine-tuning, versatility across writing styles, and open-source availability. Typical use cases encompass creative writing assistance, chatbot responses, content summarization, and language modeling research. The model demonstrates particular effectiveness in maintaining contextual coherence throughout extended text passages."
      },
      {
        "id": "facebook/opt-125m",
        "source": "hf",
        "name": "opt-125m",
        "url": "https://huggingface.co/facebook/opt-125m",
        "tags": [
          "transformers",
          "pytorch",
          "tf",
          "jax",
          "opt",
          "text-generation",
          "en",
          "arxiv:2205.01068",
          "arxiv:2005.14165",
          "license:other",
          "autotrain_compatible",
          "text-generation-inference",
          "region:us"
        ],
        "stats": {
          "downloads_total": 8845823,
          "likes_total": 218,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "OPT-125M是Meta AI开发的1.25亿参数仅解码器Transformer模型，旨在促进大语言模型的开放研究。基于GPT-3架构，通过自回归预测生成连贯文本。主要目标是为研究缩放定律、训练动态和模型行为提供可替代专有模型的开放方案。核心能力包括文本补全、对话生成和小样本学习。优势在于透明的训练方法、可复现的设计及研究友好许可。典型应用场景涵盖学术实验、模型可解释性研究和Transformer语言生成的教学演示。该模型作为OPT系列最小版本，为计算资源有限的研究者提供了基础",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "inference_acceleration",
          "auto_evaluation_models",
          "molecular_generation"
        ],
        "summary_en": "OPT-125M is a 125 million parameter decoder-only transformer developed by Meta AI for open research in large language models. Based on GPT-3 architecture, it generates coherent text through autoregressive prediction. Its primary purpose is providing an accessible alternative to proprietary models for studying scaling laws, training dynamics, and model behavior. Core capabilities include text completion, dialogue generation, and few-shot learning. Strengths include transparent training methodology, reproducible design, and research-friendly licensing. Typical use cases encompass academic experiments, model interpretability studies, and educational demonstrations of transformer-based language generation.",
        "summary_zh": "OPT-125M是Meta AI开发的1.25亿参数仅解码器Transformer模型，旨在促进大语言模型的开放研究。基于GPT-3架构，通过自回归预测生成连贯文本。主要目标是为研究缩放定律、训练动态和模型行为提供可替代专有模型的开放方案。核心能力包括文本补全、对话生成和小样本学习。优势在于透明的训练方法、可复现的设计及研究友好许可。典型应用场景涵盖学术实验、模型可解释性研究和Transformer语言生成的教学演示。该模型作为OPT系列最小版本，为计算资源有限的研究者提供了基础",
        "summary_es": "OPT-125M is a 125 million parameter decoder-only transformer developed by Meta AI for open research in large language models. Based on GPT-3 architecture, it generates coherent text through autoregressive prediction. Its primary purpose is providing an accessible alternative to proprietary models for studying scaling laws, training dynamics, and model behavior. Core capabilities include text completion, dialogue generation, and few-shot learning. Strengths include transparent training methodology, reproducible design, and research-friendly licensing. Typical use cases encompass academic experiments, model interpretability studies, and educational demonstrations of transformer-based language generation."
      },
      {
        "id": "meta-llama/Llama-3.1-8B-Instruct",
        "source": "hf",
        "name": "Llama-3.1-8B-Instruct",
        "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
        "tags": [
          "transformers",
          "safetensors",
          "llama",
          "text-generation",
          "facebook",
          "meta",
          "pytorch",
          "llama-3",
          "conversational",
          "en",
          "de",
          "fr",
          "it",
          "pt",
          "hi",
          "es",
          "th",
          "arxiv:2204.05149",
          "base_model:meta-llama/Llama-3.1-8B",
          "base_model:finetune:meta-llama/Llama-3.1-8B",
          "license:llama3.1",
          "autotrain_compatible",
          "text-generation-inference",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 7222308,
          "likes_total": 4675,
          "downloads_7d": 394514,
          "likes_7d": 34
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.952436763095692,
        "task_keys": [
          "text_to_image",
          "text_to_video",
          "lightweight_visual_model",
          "code_generation",
          "image_text_alignment",
          "multimodal_understanding_generation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "inference_acceleration",
          "auto_evaluation_models",
          "model_monitoring",
          "model_iterative_update",
          "molecular_generation"
        ],
        "summary_en": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses.",
        "summary_zh": "Llama-3.1-8B-Instruct是由Meta开发的专门用于指令跟随任务的80亿参数语言模型。它基于Llama-3.1-8B基础模型构建，核心目的是准确理解和执行用户提供的指令。主要能力包括文本生成、问答以及跨多种语言（如英语、西班牙语、法语、德语、意大利语、葡萄牙语和印地语）的对话交互。其优势在于模型尺寸高效，适用于计算资源有限的环境，并且与AutoTrain等微调框架兼容。典型应用场景涵盖聊天机器人、虚拟助手、内容创作以及需要可靠指令响应的教育工具，强调实用性和多语言支持。",
        "summary_es": "Llama-3.1-8B-Instruct is a specialized 8-billion parameter language model developed by Meta for instruction-following tasks. Built upon the Llama-3.1-8B base model, its core purpose is to understand and execute user-provided instructions accurately. Key capabilities include text generation, question answering, and conversational interaction across multiple languages such as English, Spanish, French, German, Italian, Portuguese, and Hindi. Strengths lie in its efficient size, making it suitable for deployment where computational resources are limited, and its compatibility with fine-tuning frameworks like AutoTrain. Typical use cases encompass chatbots, virtual assistants, content creation, and educational tools that require reliable, instruction-based responses."
      }
    ],
    "bioinformatics_analysis": [],
    "time_series_forecasting": [
      {
        "id": "Datadog/Toto-Open-Base-1.0",
        "source": "hf",
        "name": "Toto-Open-Base-1.0",
        "url": "https://huggingface.co/Datadog/Toto-Open-Base-1.0",
        "tags": [
          "transformers",
          "safetensors",
          "time-series-forecasting",
          "foundation models",
          "pretrained models",
          "time series foundation models",
          "time series",
          "time-series",
          "timeseries",
          "forecasting",
          "observability",
          "dataset:Salesforce/GiftEvalPretrain",
          "dataset:autogluon/chronos_datasets",
          "arxiv:2505.14766",
          "license:apache-2.0",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6369903,
          "likes_total": 112,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Toto-Open-Base-1.0是由Datadog开发的时间序列基础模型，主要用于预测应用。其核心目的是为多样化时间数据模式提供准确预测。核心能力包括多变量时间序列预测、异常检测和模式识别。主要优势在于基于Salesforce/GiftEvalPretrain和AutoGluon/Chronos等数据集的大规模预训练、与Transformers架构的兼容性以及Apache 2.0开源许可。典型应用场景涵盖IT可观测性、业务指标预测、运营监控和工业物联网应用，充分利用其处理复杂时间依赖关系的能力。该模型特别适用于需要高精度时间序列分析的监控和预测任务。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_forecasting",
          "time_series_anomaly_detection"
        ],
        "summary_en": "Toto-Open-Base-1.0 is a time series foundation model developed by Datadog for forecasting applications. Its primary purpose is to provide accurate predictions across diverse temporal data patterns. Core capabilities include multivariate time series forecasting, anomaly detection, and pattern recognition. Key strengths are its large-scale pretraining on datasets like Salesforce/GiftEvalPretrain and AutoGluon/Chronos, compatibility with Transformers architecture, and Apache 2.0 open-source licensing. Typical use cases span IT observability, business metrics forecasting, operational monitoring, and industrial IoT applications, leveraging its ability to handle complex temporal dependencies.",
        "summary_zh": "Toto-Open-Base-1.0是由Datadog开发的时间序列基础模型，主要用于预测应用。其核心目的是为多样化时间数据模式提供准确预测。核心能力包括多变量时间序列预测、异常检测和模式识别。主要优势在于基于Salesforce/GiftEvalPretrain和AutoGluon/Chronos等数据集的大规模预训练、与Transformers架构的兼容性以及Apache 2.0开源许可。典型应用场景涵盖IT可观测性、业务指标预测、运营监控和工业物联网应用，充分利用其处理复杂时间依赖关系的能力。该模型特别适用于需要高精度时间序列分析的监控和预测任务。",
        "summary_es": "Toto-Open-Base-1.0 is a time series foundation model developed by Datadog for forecasting applications. Its primary purpose is to provide accurate predictions across diverse temporal data patterns. Core capabilities include multivariate time series forecasting, anomaly detection, and pattern recognition. Key strengths are its large-scale pretraining on datasets like Salesforce/GiftEvalPretrain and AutoGluon/Chronos, compatibility with Transformers architecture, and Apache 2.0 open-source licensing. Typical use cases span IT observability, business metrics forecasting, operational monitoring, and industrial IoT applications, leveraging its ability to handle complex temporal dependencies."
      },
      {
        "id": "thuml/sundial-base-128m",
        "source": "hf",
        "name": "sundial-base-128m",
        "url": "https://huggingface.co/thuml/sundial-base-128m",
        "tags": [
          "safetensors",
          "sundial",
          "time series",
          "time-series",
          "forecasting",
          "foundation models",
          "pretrained models",
          "generative models",
          "time series foundation models",
          "time-series-forecasting",
          "custom_code",
          "dataset:thuml/UTSD",
          "dataset:Salesforce/lotsa_data",
          "dataset:autogluon/chronos_datasets",
          "arxiv:2502.00816",
          "arxiv:2403.07815",
          "license:apache-2.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6093403,
          "likes_total": 47,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Sundial-base-128m是由THUML开发的1.28亿参数时间序列基础模型。该模型作为预训练的生成式模型，专门用于时间序列预测应用。其核心能力包括从多样化数据集学习时间模式，实现跨领域的高精度未来预测。主要优势在于基础模型架构支持向特定预测任务的迁移学习，参数规模在性能和计算需求间取得平衡，并支持多种时间序列格式。典型应用场景涵盖需求预测、金融市场预测、能源负荷预测和工业监控系统，这些场景均需要基于历史时间模式来推断未来",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "lightweight_visual_model",
          "code_generation",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_forecasting",
          "time_series_anomaly_detection"
        ],
        "summary_en": "Sundial-base-128m is a 128-million parameter time series foundation model developed by THUML. It serves as a pretrained generative model for time series forecasting applications. The model's core capabilities include learning temporal patterns from diverse datasets to enable accurate future predictions across various domains. Key strengths include its foundation model architecture that allows transfer learning to specific forecasting tasks, efficient parameter size balancing performance and computational requirements, and support for multiple time series formats. Typical use cases span demand forecasting, financial market prediction, energy load forecasting, and industrial monitoring systems where historical temporal patterns inform future trends.",
        "summary_zh": "Sundial-base-128m是由THUML开发的1.28亿参数时间序列基础模型。该模型作为预训练的生成式模型，专门用于时间序列预测应用。其核心能力包括从多样化数据集学习时间模式，实现跨领域的高精度未来预测。主要优势在于基础模型架构支持向特定预测任务的迁移学习，参数规模在性能和计算需求间取得平衡，并支持多种时间序列格式。典型应用场景涵盖需求预测、金融市场预测、能源负荷预测和工业监控系统，这些场景均需要基于历史时间模式来推断未来",
        "summary_es": "Sundial-base-128m is a 128-million parameter time series foundation model developed by THUML. It serves as a pretrained generative model for time series forecasting applications. The model's core capabilities include learning temporal patterns from diverse datasets to enable accurate future predictions across various domains. Key strengths include its foundation model architecture that allows transfer learning to specific forecasting tasks, efficient parameter size balancing performance and computational requirements, and support for multiple time series formats. Typical use cases span demand forecasting, financial market prediction, energy load forecasting, and industrial monitoring systems where historical temporal patterns inform future trends."
      },
      {
        "id": "Salesforce/moirai-2.0-R-small",
        "source": "hf",
        "name": "moirai-2.0-R-small",
        "url": "https://huggingface.co/Salesforce/moirai-2.0-R-small",
        "tags": [
          "safetensors",
          "time series",
          "forecasting",
          "pretrained models",
          "foundation models",
          "time series foundation models",
          "time-series",
          "time-series-forecasting",
          "arxiv:2403.07815",
          "arxiv:2402.02592",
          "license:cc-by-nc-4.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 5405450,
          "likes_total": 22,
          "downloads_7d": 1133274,
          "likes_7d": 2
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Moirai-2.0-R-small是Salesforce开发的时间序列基础模型，专为预测应用设计。该模型作为一个统一框架，能够处理跨多个领域的多样化时间序列数据。其核心能力包括零样本预测、少样本学习以及跨不同时间序列数据集的迁移学习。主要优势在于处理异构频率、支持多种预测范围并提供概率性预测。典型应用场景涵盖零售需求预测、金融市场预测、能源负荷预测和医疗健康分析。该模型代表了时间序列分析领域的重大进展，提供了一个通用解决方案，减少了对领",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.744376915745296,
        "task_keys": [
          "time_series_forecasting"
        ],
        "summary_en": "Moirai-2.0-R-small is a time series foundation model developed by Salesforce for forecasting applications. It serves as a unified framework capable of handling diverse time series data across multiple domains. The model's core capabilities include zero-shot forecasting, few-shot learning, and transfer learning across different time series datasets. Its strengths lie in handling heterogeneous frequencies, supporting various prediction horizons, and providing probabilistic forecasts. Typical use cases span retail demand forecasting, financial market prediction, energy load forecasting, and healthcare analytics. The model represents a significant advancement in time series analysis by offering a general-purpose solution that reduces the need for domain-specific model development.",
        "summary_zh": "Moirai-2.0-R-small是Salesforce开发的时间序列基础模型，专为预测应用设计。该模型作为一个统一框架，能够处理跨多个领域的多样化时间序列数据。其核心能力包括零样本预测、少样本学习以及跨不同时间序列数据集的迁移学习。主要优势在于处理异构频率、支持多种预测范围并提供概率性预测。典型应用场景涵盖零售需求预测、金融市场预测、能源负荷预测和医疗健康分析。该模型代表了时间序列分析领域的重大进展，提供了一个通用解决方案，减少了对领",
        "summary_es": "Moirai-2.0-R-small is a time series foundation model developed by Salesforce for forecasting applications. It serves as a unified framework capable of handling diverse time series data across multiple domains. The model's core capabilities include zero-shot forecasting, few-shot learning, and transfer learning across different time series datasets. Its strengths lie in handling heterogeneous frequencies, supporting various prediction horizons, and providing probabilistic forecasts. Typical use cases span retail demand forecasting, financial market prediction, energy load forecasting, and healthcare analytics. The model represents a significant advancement in time series analysis by offering a general-purpose solution that reduces the need for domain-specific model development."
      }
    ],
    "time_series_anomaly_detection": [
      {
        "id": "Falconsai/nsfw_image_detection",
        "source": "hf",
        "name": "nsfw_image_detection",
        "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
        "tags": [
          "transformers",
          "pytorch",
          "safetensors",
          "vit",
          "image-classification",
          "arxiv:2010.11929",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 98988953,
          "likes_total": 826,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "image_text_alignment",
          "auto_evaluation_models",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards.",
        "summary_zh": "该项目专门设计用于检测NSFW（不适宜工作场所）内容的图像分类模型。基于Vision Transformer架构，能够分析图像并识别不当材料，涵盖色情、暴力或冒犯性内容等类别。核心功能包括高精度分类露骨图像，支持自动化内容审核。其优势在于大规模训练数据和高效处理能力。典型应用场景包括社交媒体平台、内容过滤系统以及需要自动化安全措施的在线社区，用于维护适当的内容标准。该模型特别适用于需要自动识别和过滤不当视觉内容的数字环境，帮助平",
        "summary_es": "This project provides an image classification model specifically designed to detect NSFW (Not Safe For Work) content. Built on Vision Transformer architecture, it analyzes images to identify inappropriate material across categories like pornography, violence, or offensive content. Core capabilities include high-accuracy classification of explicit imagery, supporting automated content moderation. Strengths lie in its large-scale training data and efficient processing. Typical use cases include social media platforms, content filtering systems, and online communities requiring automated safety measures to maintain appropriate content standards."
      },
      {
        "id": "dima806/fairface_age_image_detection",
        "source": "hf",
        "name": "fairface_age_image_detection",
        "url": "https://huggingface.co/dima806/fairface_age_image_detection",
        "tags": [
          "transformers",
          "safetensors",
          "vit",
          "image-classification",
          "dataset:nateraw/fairface",
          "base_model:google/vit-base-patch16-224-in21k",
          "base_model:finetune:google/vit-base-patch16-224-in21k",
          "license:apache-2.0",
          "autotrain_compatible",
          "endpoints_compatible",
          "region:us"
        ],
        "stats": {
          "downloads_total": 60653441,
          "likes_total": 41,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "image_classification",
          "object_detection",
          "text_to_image",
          "medical_image_processing",
          "remote_sensing_image_processing",
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "image_text_alignment",
          "lightweight_multimodal_model",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "auto_evaluation_models",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection"
        ],
        "summary_en": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion.",
        "summary_zh": "该项目提供了一个基于谷歌ViT-base架构微调的视觉Transformer模型，专门用于从面部图像进行年龄分类。模型在FairFace数据集上训练，擅长预测不同人口群体的年龄范围。核心能力包括准确的多类别年龄分类和增强的公平性指标。主要优势在于对各种面部特征的稳健识别性能，以及通过平衡训练数据实现的偏见缓解。典型应用场景涵盖人口统计分析、适龄内容过滤，以及需要公平面部属性识别的研究领域。该工具注重技术实用性和公平性，适用于学术研究",
        "summary_es": "This project provides a fine-tuned Vision Transformer model for age classification from facial images. Based on Google's ViT-base architecture and trained on the FairFace dataset, it specializes in predicting age ranges across diverse demographic groups. Core capabilities include accurate multi-class age categorization with enhanced fairness metrics. Key strengths are robust performance on varied facial characteristics and bias mitigation through balanced training data. Typical use cases span demographic analysis, age-appropriate content filtering, and research applications requiring equitable facial attribute recognition without commercial promotion."
      },
      {
        "id": "pyannote/segmentation",
        "source": "hf",
        "name": "segmentation",
        "url": "https://huggingface.co/pyannote/segmentation",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "arxiv:2104.04045",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6591894,
          "likes_total": 641,
          "downloads_7d": 0,
          "likes_7d": 3
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.32376268146237963,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition.",
        "summary_zh": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "summary_es": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition."
      }
    ],
    "radar_understanding": [],
    "lidar_understanding": [],
    "low_resource_medical_ai": [],
    "low_resource_voice_assistant": [
      {
        "id": "pyannote/segmentation-3.0",
        "source": "hf",
        "name": "segmentation-3.0",
        "url": "https://huggingface.co/pyannote/segmentation-3.0",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-diarization",
          "speaker-change-detection",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 18518237,
          "likes_total": 605,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines.",
        "summary_zh": "Pyannote segmentation-3.0 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化任务，即将音频流分割为说话人同质区域。其核心功能包括语音活动检测、说话人变换检测、重叠语音检测和说话人分割。该模型在处理多说话人场景和重叠语音方面表现出色，特别适用于会议转录、广播监控和对话分析等典型应用场景。作为 MIT 许可的开源模型，它支持研究和商业用途，可集成到语音处理流程中。模型专注于精确的音频分割能力，不包含营销内容。",
        "summary_es": "Pyannote segmentation-3.0 is a PyTorch-based neural network for speaker diarization, designed to segment audio streams into speaker-homogeneous regions. Its core capabilities include voice activity detection, speaker change detection, overlapped speech detection, and speaker segmentation. The model excels at handling challenging scenarios with multiple speakers and overlapping speech. Typical use cases include meeting transcription, broadcast monitoring, and conversational analysis. As an MIT-licensed model, it supports research and commercial applications in speech processing pipelines."
      },
      {
        "id": "pyannote/wespeaker-voxceleb-resnet34-LM",
        "source": "hf",
        "name": "wespeaker-voxceleb-resnet34-LM",
        "url": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "wespeaker",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-recognition",
          "speaker-verification",
          "speaker-identification",
          "speaker-embedding",
          "dataset:voxceleb",
          "license:cc-by-4.0",
          "region:us"
        ],
        "stats": {
          "downloads_total": 17865623,
          "likes_total": 76,
          "downloads_7d": 0,
          "likes_7d": 0
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "wespeaker-voxceleb-resnet34-LM 是一个用于说话人识别的深度学习模型，主要功能是从音频中提取表征说话人身份的嵌入向量。该模型基于 ResNet-34 架构，在 VoxCeleb 数据集上训练，并融合语言模型评分以提升识别精度。核心能力包括生成固定维度的说话人特征向量，适用于说话人验证和辨认任务。其优势在于对多样语音条件下的说话人区分具有较强鲁棒性。典型应用场景包括说话人日志生成、声纹认证系统、以及多媒体内容中的说话人分离与分析，适用于需要准确识别不同说话",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": 0.09996983784444612,
        "task_keys": [
          "lightweight_visual_model",
          "nlp_data_synthesis",
          "nlp_data_distillation",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "graph_augmented_reco",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "training_data_anonymization",
          "training_data_copyright",
          "model_monitoring",
          "model_iterative_update",
          "low_resource_voice_assistant"
        ],
        "summary_en": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech segments to generate fixed-dimensional vectors that uniquely represent individual speakers. The model leverages a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved accuracy. Key strengths include robust performance in speaker verification and identification tasks across diverse conditions. Typical use cases encompass speaker diarization, voice authentication systems, and multimedia content analysis where distinguishing between speakers is essential.",
        "summary_zh": "wespeaker-voxceleb-resnet34-LM 是一个用于说话人识别的深度学习模型，主要功能是从音频中提取表征说话人身份的嵌入向量。该模型基于 ResNet-34 架构，在 VoxCeleb 数据集上训练，并融合语言模型评分以提升识别精度。核心能力包括生成固定维度的说话人特征向量，适用于说话人验证和辨认任务。其优势在于对多样语音条件下的说话人区分具有较强鲁棒性。典型应用场景包括说话人日志生成、声纹认证系统、以及多媒体内容中的说话人分离与分析，适用于需要准确识别不同说话",
        "summary_es": "The wespeaker-voxceleb-resnet34-LM model is a speaker recognition system designed to extract distinctive speaker embeddings from audio. Its core capability involves processing speech segments to generate fixed-dimensional vectors that uniquely represent individual speakers. The model leverages a ResNet-34 architecture trained on the VoxCeleb dataset, enhanced with language model scoring for improved accuracy. Key strengths include robust performance in speaker verification and identification tasks across diverse conditions. Typical use cases encompass speaker diarization, voice authentication systems, and multimedia content analysis where distinguishing between speakers is essential."
      },
      {
        "id": "pyannote/segmentation",
        "source": "hf",
        "name": "segmentation",
        "url": "https://huggingface.co/pyannote/segmentation",
        "tags": [
          "pyannote-audio",
          "pytorch",
          "pyannote",
          "pyannote-audio-model",
          "audio",
          "voice",
          "speech",
          "speaker",
          "speaker-segmentation",
          "voice-activity-detection",
          "overlapped-speech-detection",
          "resegmentation",
          "arxiv:2104.04045",
          "license:mit",
          "region:us"
        ],
        "stats": {
          "downloads_total": 6591894,
          "likes_total": 641,
          "downloads_7d": 0,
          "likes_7d": 3
        },
        "updated_at": "2025-09-27T06:54:22.087Z",
        "added_at": "2025-09-27",
        "summary": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "flags": {
          "pinned": false,
          "hidden": false
        },
        "score_model": -0.32376268146237963,
        "task_keys": [
          "object_detection",
          "semantic_segmentation",
          "instance_segmentation",
          "panoptic_segmentation",
          "lightweight_visual_model",
          "speaker_separation",
          "lightweight_multimodal_model",
          "vector_retrieval",
          "model_compression",
          "model_quantization",
          "model_distillation",
          "model_monitoring",
          "model_iterative_update",
          "time_series_anomaly_detection",
          "low_resource_voice_assistant"
        ],
        "summary_en": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition.",
        "summary_zh": "Pyannote/segmentation 是一个基于 PyTorch 的神经网络模型，专门用于说话人日志化和音频分割任务。其核心功能包括语音活动检测、说话人变换检测和重叠语音检测。该模型擅长将音频流划分为同质片段，并以高时间精度识别说话人边界。典型应用场景涉及会议转录、广播监控和对话分析，其中确定“谁在何时说话”至关重要。它作为语音处理流程中的基础组件，在说话人聚类或语音识别之前提供精确的时间分割。该模型基于 MIT 许可发布，在音频处理社区中广泛使用。",
        "summary_es": "Pyannote/segmentation is a PyTorch-based neural network for speaker diarization and audio segmentation tasks. Its core capabilities include voice activity detection, speaker change detection, and overlapped speech detection. The model excels at partitioning audio streams into homogeneous segments and identifying speaker boundaries with high temporal precision. Typical use cases involve meeting transcription, broadcast monitoring, and conversational analysis where identifying 'who spoke when' is essential. It serves as a fundamental component in speech processing pipelines, providing accurate temporal segmentation before speaker clustering or speech recognition."
      }
    ],
    "ar_vr_interaction": [],
    "multimodal_temporal_fusion": [],
    "robot_dialogue_logic": []
  }
}