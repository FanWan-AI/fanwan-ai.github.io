{
  "version": 1,
  "updated_at": "2025-09-20T22:06:12.525Z",
  "items": [
    {
      "id": "dphn/dolphin-2.9.1-yi-1.5-34b",
      "source": "hf",
      "name": "dolphin-2.9.1-yi-1.5-34b",
      "url": "https://huggingface.co/dphn/dolphin-2.9.1-yi-1.5-34b",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "generated_from_trainer",
        "axolotl",
        "conversational",
        "dataset:cognitivecomputations/Dolphin-2.9",
        "dataset:teknium/OpenHermes-2.5",
        "dataset:m-a-p/CodeFeedback-Filtered-Instruction",
        "dataset:cognitivecomputations/dolphin-coder",
        "dataset:cognitivecomputations/samantha-data",
        "dataset:microsoft/orca-math-word-problems-200k",
        "dataset:Locutusque/function-calling-chatml",
        "dataset:internlm/Agent-FLAN",
        "base_model:01-ai/Yi-1.5-34B",
        "base_model:finetune:01-ai/Yi-1.5-34B",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 4698276,
        "likes_total": 39
      },
      "score": 9416.052,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "dolphin-2.9.1-yi-1.5-34b 是一个基于 Yi-34B 架构的大规模语言模型，专为对话生成和代码辅助任务优化。该模型融合了 Dolphin-2.9、OpenHermes-2.5 和 CodeFeedback 等多个高质量数据集，具备更强的指令遵循能力和代码理解能力。其亮点在于支持自然语言交互和代码生成，适用于聊天机器人、编程助手以及需要复杂推理的对话场景。模型以 Apache 2.0 协议开源，可通过 Transformers 框架直接调用。",
      "updated_at": "2025-09-20T17:52:25.763Z",
      "task_keys": [
        "code_generation"
      ],
      "summary_en": "Dolphin-2.9.1-Yi-1.5-34B is a 34-billion-parameter language model fine-tuned on datasets including Dolphin-2.9, OpenHermes-2.5, and CodeFeedback. It excels in conversational AI, code generation, and instruction-following tasks. Built on the Yi architecture, it supports transformers and safetensors for efficient deployment. Ideal for developers and researchers seeking a robust, open-source model for natural language processing and coding assistance.",
      "summary_zh": "dolphin-2.9.1-yi-1.5-34b 是一个基于 Yi-34B 架构的大规模语言模型，专为对话生成和代码辅助任务优化。该模型融合了 Dolphin-2.9、OpenHermes-2.5 和 CodeFeedback 等多个高质量数据集，具备更强的指令遵循能力和代码理解能力。其亮点在于支持自然语言交互和代码生成，适用于聊天机器人、编程助手以及需要复杂推理的对话场景。模型以 Apache 2.0 协议开源，可通过 Transformers 框架直接调用。",
      "summary_es": "Modelo de lenguaje Dolphin 2.9.1, basado en Yi-34B, optimizado para conversación y generación de texto. Destaca por su entrenamiento con datasets especializados como OpenHermes y CodeFeedback, mejorando interacciones naturales y asistencia en código. Ideal para chatbots, soporte técnico y herramientas de desarrollo.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：dolphin-2.9.1-yi-1.5-34b"
    },
    {
      "id": "colbert-ir/colbertv2.0",
      "source": "hf",
      "name": "colbertv2.0",
      "url": "https://huggingface.co/colbert-ir/colbertv2.0",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "bert",
        "ColBERT",
        "en",
        "arxiv:2004.12832",
        "arxiv:2007.00814",
        "arxiv:2101.00436",
        "arxiv:2112.01488",
        "arxiv:2205.09707",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 5861802,
        "likes_total": 285
      },
      "score": 11866.104,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "ColBERTv2.0 是一种基于 BERT 的检索模型，通过引入“上下文化晚期交互”（Contextualized Late Interaction）机制，显著提升了密集检索的效率与准确性。该模型将查询和文档分别编码为细粒度嵌入，并通过最大相似度计算实现高效匹配，适用于大规模文档检索和问答任务。相比传统检索方法，ColBERTv2.0 在保持高精度的同时大幅降低了计算开销，尤其适合需要快速响应的信息检索场景。其预训练版本支持多种下游任务，包括开放域问答和文档排序。",
      "updated_at": "2025-09-20T17:52:25.763Z",
      "summary_en": "ColBERTv2.0 is a neural retrieval model that enhances dense passage retrieval with late interaction, combining BERT-based contextual embeddings with efficient approximate nearest neighbor search. It excels in information retrieval tasks, offering strong performance on benchmarks like MS MARCO and Natural Questions. The model is suitable for building scalable search systems, supporting both English and multilingual contexts. Its design balances accuracy and computational efficiency, making it practical for real-world applications.",
      "summary_zh": "ColBERTv2.0 是一种基于 BERT 的检索模型，通过引入“上下文化晚期交互”（Contextualized Late Interaction）机制，显著提升了密集检索的效率与准确性。该模型将查询和文档分别编码为细粒度嵌入，并通过最大相似度计算实现高效匹配，适用于大规模文档检索和问答任务。相比传统检索方法，ColBERTv2.0 在保持高精度的同时大幅降低了计算开销，尤其适合需要快速响应的信息检索场景。其预训练版本支持多种下游任务，包括开放域问答和文档排序。",
      "summary_es": "ColBERTv2.0 es un modelo de recuperación de información basado en BERT que indexa y busca documentos mediante representaciones densas de tokens. Destaca por su eficiencia en búsquedas semánticas y su escalabilidad para grandes colecciones de texto. Es ideal para sistemas de preguntas-respuestas, motores de búsqueda mejorados y aplicaciones que requieren recuperación precisa de documentos.",
      "reason_label": "new_release",
      "reason_text": "新版本发布：colbertv2.0",
      "task_keys": [
        "neural_retrieval"
      ]
    },
    {
      "id": "coqui/XTTS-v2",
      "source": "hf",
      "name": "XTTS-v2",
      "url": "https://huggingface.co/coqui/XTTS-v2",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "coqui",
        "text-to-speech",
        "license:other",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 4909924,
        "likes_total": 3043
      },
      "score": 11341.348,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "XTTS-v2 是由 Coqui 开发的开源文本转语音模型，支持多语言语音合成与语音克隆功能。该模型能够基于少量参考音频生成自然且富有表现力的语音，同时保持说话人的音色特征。其亮点在于支持跨语言语音合成，即使用一种语言的参考音频生成另一种语言的语音输出。适用于配音、有声内容制作、语音助手开发等场景，尤其适合需要个性化语音合成的技术应用。",
      "updated_at": "2025-09-20T17:52:25.763Z",
      "task_keys": [
        "tts"
      ],
      "summary_en": "XTTS-v2 is a multilingual text-to-speech model by Coqui, supporting voice cloning and cross-lingual synthesis. It excels in generating natural, expressive speech from minimal reference audio, making it suitable for audiobook narration, voiceovers, and accessibility tools. The model is robust across diverse languages and accents, offering high-quality output with efficient inference. Ideal for developers and creators needing customizable, scalable TTS solutions.",
      "summary_zh": "XTTS-v2 是由 Coqui 开发的开源文本转语音模型，支持多语言语音合成与语音克隆功能。该模型能够基于少量参考音频生成自然且富有表现力的语音，同时保持说话人的音色特征。其亮点在于支持跨语言语音合成，即使用一种语言的参考音频生成另一种语言的语音输出。适用于配音、有声内容制作、语音助手开发等场景，尤其适合需要个性化语音合成的技术应用。",
      "summary_es": "XTTS-v2 es un modelo de síntesis de voz que genera habla natural multilingüe a partir de texto. Destaca por su capacidad de clonación de voz con pocos datos de referencia. Es ideal para aplicaciones de accesibilidad, narración automatizada y generación de contenido multimedia. Su arquitectura permite alta calidad y adaptabilidad en diversos contextos.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：XTTS-v2"
    },
    {
      "id": "nlpaueb/legal-bert-base-uncased",
      "source": "hf",
      "name": "legal-bert-base-uncased",
      "url": "https://huggingface.co/nlpaueb/legal-bert-base-uncased",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "bert",
        "pretraining",
        "legal",
        "fill-mask",
        "en",
        "license:cc-by-sa-4.0",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 5594395,
        "likes_total": 271
      },
      "score": 11324.29,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "legal-bert-base-uncased 是一个基于 BERT 架构的预训练语言模型，专门针对法律文本进行了优化。该模型在大量法律文档上进行了预训练，能够更好地理解和处理法律术语及复杂句式。其核心功能包括文本分类、信息抽取和语义匹配，适用于法律文档分析、合同审查和法规检索等场景。该模型支持多种深度学习框架，并遵循 CC BY-SA 4.0 开源协议。",
      "updated_at": "2025-09-20T17:52:25.763Z",
      "summary_en": "Legal-BERT-Base-Uncased is a BERT model pretrained on English legal text. It is optimized for tasks like legal document classification, named entity recognition, and information extraction. Its strengths include domain-specific understanding of legal language and terminology. It is applicable for legal tech, compliance automation, and academic research in law and NLP.",
      "summary_zh": "legal-bert-base-uncased 是一个基于 BERT 架构的预训练语言模型，专门针对法律文本进行了优化。该模型在大量法律文档上进行了预训练，能够更好地理解和处理法律术语及复杂句式。其核心功能包括文本分类、信息抽取和语义匹配，适用于法律文档分析、合同审查和法规检索等场景。该模型支持多种深度学习框架，并遵循 CC BY-SA 4.0 开源协议。",
      "summary_es": "Legal-BERT-base-uncased es un modelo de lenguaje preentrenado especializado en textos legales en inglés. Basado en BERT, destaca en tareas como análisis de documentos jurídicos, clasificación de sentencias y extracción de cláusulas. Su entrenamiento con corpus legal mejora precisión en terminología especializada. Ideal para automatización legal, investigación jurídica y procesamiento de contratos.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：legal-bert-base-uncased"
    },
    {
      "id": "autogluon/chronos-bolt-base",
      "source": "hf",
      "name": "chronos-bolt-base",
      "url": "https://huggingface.co/autogluon/chronos-bolt-base",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "safetensors",
        "t5",
        "time series",
        "forecasting",
        "pretrained models",
        "foundation models",
        "time series foundation models",
        "time-series",
        "time-series-forecasting",
        "arxiv:1910.10683",
        "arxiv:2403.07815",
        "license:apache-2.0",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 5275394,
        "likes_total": 26
      },
      "score": 10563.788,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Chronos-Bolt-Base是一个基于T5架构的时间序列预测基础模型，采用预训练方式构建。该模型通过将时间序列数据转换为token序列，利用Transformer的自回归机制进行预测，适用于多种时间序列任务。其亮点在于无需领域特定特征工程，能够直接处理原始时间序列数据，并支持零样本或少样本预测。该模型适用于能源需求、销售预测、经济指标分析等场景，为时间序列分析提供了灵活且高效的解决方案。",
      "updated_at": "2025-09-20T17:52:25.763Z",
      "task_keys": [
        "time_series_forecasting"
      ],
      "summary_en": "Chronos-Bolt-Base is a pretrained T5-based foundation model for time series forecasting. It excels in zero-shot and few-shot scenarios, requiring minimal fine-tuning for accurate predictions across diverse domains. Its strengths include strong generalization and ease of integration into forecasting pipelines. Ideal for applications in finance, energy, and retail where rapid, adaptable forecasting is needed.",
      "summary_zh": "Chronos-Bolt-Base是一个基于T5架构的时间序列预测基础模型，采用预训练方式构建。该模型通过将时间序列数据转换为token序列，利用Transformer的自回归机制进行预测，适用于多种时间序列任务。其亮点在于无需领域特定特征工程，能够直接处理原始时间序列数据，并支持零样本或少样本预测。该模型适用于能源需求、销售预测、经济指标分析等场景，为时间序列分析提供了灵活且高效的解决方案。",
      "summary_es": "Chronos-Bolt-Base es un modelo de base para previsión de series temporales, basado en T5 y preentrenado con datos sintéticos. Su principal fortaleza es la capacidad de adaptarse a múltiples dominios sin necesidad de ajuste específico. Es ideal para predicciones univariantes con datos escasos o sin historial extenso. Incluye soporte para SafeTensors y está diseñado para aplicaciones que requieren inferencia rápida y eficiente.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：chronos-bolt-base"
    },
    {
      "id": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
      "source": "hf",
      "name": "Wan_2.2_ComfyUI_Repackaged",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "diffusion-single-file",
        "comfyui",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 5097262,
        "likes_total": 326
      },
      "score": 10357.524,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Wan_2.2_ComfyUI_Repackaged 是一个基于 ComfyUI 框架重新封装的扩散模型工具包，专为图像生成任务设计。它集成了 Wan 2.2 模型，支持单文件加载，简化了工作流程并提升了易用性。该工具适用于需要高效、灵活图像生成的专业用户和开发者，尤其适合在本地或云端部署的 AI 创作场景。其亮点在于优化了模型与 ComfyUI 的兼容性，同时保持了生成质量与性能的平衡。",
      "updated_at": "2025-09-20T17:52:25.763Z",
      "task_keys": [
        "text_to_image"
      ],
      "summary_en": "Wan_2.2_ComfyUI_Repackaged is a single-file diffusion model optimized for use with ComfyUI. It enables efficient image generation and manipulation, particularly suited for users seeking streamlined workflows without complex dependencies. Its repackaged format simplifies deployment and integration, making it accessible for creative applications and prototyping. Ideal for artists and developers focused on rapid experimentation and stable diffusion-based projects.",
      "summary_zh": "Wan_2.2_ComfyUI_Repackaged 是一个基于 ComfyUI 框架重新封装的扩散模型工具包，专为图像生成任务设计。它集成了 Wan 2.2 模型，支持单文件加载，简化了工作流程并提升了易用性。该工具适用于需要高效、灵活图像生成的专业用户和开发者，尤其适合在本地或云端部署的 AI 创作场景。其亮点在于优化了模型与 ComfyUI 的兼容性，同时保持了生成质量与性能的平衡。",
      "summary_es": "Wan 2.2 ComfyUI Repackaged es un modelo de difusión empaquetado para ComfyUI, optimizado para generación de imágenes. Destaca por su facilidad de uso, integración directa y compatibilidad con flujos de trabajo de IA generativa. Ideal para artistas digitales e investigadores que buscan implementación rápida y estable en entornos de producción.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：Wan_2.2_ComfyUI_Repackaged"
    },
    {
      "id": "google-t5/t5-small",
      "source": "hf",
      "name": "t5-small",
      "url": "https://huggingface.co/google-t5/t5-small",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "onnx",
        "safetensors",
        "t5",
        "text2text-generation",
        "summarization",
        "translation",
        "en",
        "fr",
        "ro",
        "de",
        "multilingual",
        "dataset:c4",
        "arxiv:1805.12471",
        "arxiv:1708.00055",
        "arxiv:1704.05426",
        "arxiv:1606.05250",
        "arxiv:1808.09121",
        "arxiv:1810.12885",
        "arxiv:1905.10044",
        "arxiv:1910.09700",
        "license:apache-2.0",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 4535380,
        "likes_total": 488
      },
      "score": 9314.76,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "t5-small是谷歌T5（Text-to-Text Transfer Transformer）系列中的轻量级模型，适用于多种文本生成任务。它采用统一的“文本到文本”框架，能够处理摘要生成、翻译、问答和文本分类等任务。该模型参数量较小，适合资源受限的环境或需要快速推理的场景。支持多种深度学习框架，包括PyTorch、TensorFlow和JAX，便于集成到不同技术栈中。适用于自然语言处理研究、原型验证和小规模应用部署。",
      "updated_at": "2025-09-20T17:52:25.763Z",
      "summary_en": "t5-small is a compact text-to-text transformer model by Google, suitable for tasks like summarization, translation, and question answering. It balances efficiency and performance, making it ideal for resource-constrained environments. The model supports multiple frameworks including PyTorch, TensorFlow, and JAX, ensuring broad applicability. Its open-source nature and versatility make it a practical choice for prototyping and lightweight NLP applications.",
      "summary_zh": "t5-small是谷歌T5（Text-to-Text Transfer Transformer）系列中的轻量级模型，适用于多种文本生成任务。它采用统一的“文本到文本”框架，能够处理摘要生成、翻译、问答和文本分类等任务。该模型参数量较小，适合资源受限的环境或需要快速推理的场景。支持多种深度学习框架，包括PyTorch、TensorFlow和JAX，便于集成到不同技术栈中。适用于自然语言处理研究、原型验证和小规模应用部署。",
      "summary_es": "El modelo T5-small es un modelo de texto a texto basado en la arquitectura Transformer. Es eficiente y versátil, ideal para tareas como resumen, traducción y generación de texto. Su tamaño compacto permite un despliegue ágil en entornos con recursos limitados. Es ampliamente utilizado en procesamiento de lenguaje natural e investigación.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：t5-small"
    },
    {
      "id": "trl-internal-testing/tiny-Qwen2ForCausalLM-2.5",
      "source": "hf",
      "name": "tiny-Qwen2ForCausalLM-2.5",
      "url": "https://huggingface.co/trl-internal-testing/tiny-Qwen2ForCausalLM-2.5",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "trl",
        "conversational",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 4322548,
        "likes_total": 1
      },
      "score": 8645.596,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "tiny-Qwen2ForCausalLM-2.5 是一个基于 Qwen2 架构的小型因果语言模型，专为文本生成任务设计。该模型支持对话式交互，适用于聊天机器人、内容生成和问答系统等场景。其亮点包括兼容 Transformers 和 Safetensors 框架，并支持文本生成推理和自动训练功能。该模型部署友好，适合资源受限环境下的轻量级应用。",
      "updated_at": "2025-09-20T17:52:25.763Z",
      "summary_en": "A compact, open-source language model optimized for efficient text generation and conversational tasks. Built on the Qwen2 architecture, it supports seamless integration with Transformers and TRL for fine-tuning and inference. Ideal for lightweight applications requiring fast, scalable AI responses, such as chatbots or automated content creation. Compatible with AutoTrain and text-generation-inference tools for easy deployment.",
      "summary_zh": "tiny-Qwen2ForCausalLM-2.5 是一个基于 Qwen2 架构的小型因果语言模型，专为文本生成任务设计。该模型支持对话式交互，适用于聊天机器人、内容生成和问答系统等场景。其亮点包括兼容 Transformers 和 Safetensors 框架，并支持文本生成推理和自动训练功能。该模型部署友好，适合资源受限环境下的轻量级应用。",
      "summary_es": "Modelo causal de lenguaje pequeño basado en Qwen2, optimizado para generación de texto conversacional. Destaca por su compatibilidad con Transformers, SafeTensors y herramientas como TRL y AutoTrain. Ideal para pruebas de inferencia, prototipado rápido y despliegue en endpoints. Su tamaño reducido permite uso eficiente en entornos con recursos limitados.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：tiny-Qwen2ForCausalLM-2.5"
    }
  ]
}