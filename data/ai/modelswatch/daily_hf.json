{
  "version": 1,
  "updated_at": "2025-09-25T22:06:36.404Z",
  "items": [
    {
      "id": "facebook/opt-125m",
      "source": "hf",
      "name": "opt-125m",
      "url": "https://huggingface.co/facebook/opt-125m",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "opt",
        "text-generation",
        "en",
        "arxiv:2205.01068",
        "arxiv:2005.14165",
        "license:other",
        "autotrain_compatible",
        "text-generation-inference",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 9875029,
        "likes_total": 217
      },
      "score": 19858.558,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "OPT-125M是Meta开源的1.25亿参数语言模型，属于OPT系列的基础版本。该模型采用标准的Transformer解码器架构，支持英文文本生成任务。作为研究友好型模型，其权重完全开放，便于学术界开展语言模型机理、性能分析等实验。适用于轻量级文本补全、语言理解研究及教育场景，为资源受限环境提供可复现的基线模型。",
      "updated_at": "2025-09-21T12:06:53.894Z",
      "summary_en": "Meta's OPT-125M is a 125-million-parameter decoder-only transformer model for English text generation. It is designed as an open-source alternative to GPT-3, supporting tasks like content creation, summarization, and dialogue. The model is trained on diverse public text data and is accessible via multiple frameworks (PyTorch, TensorFlow, JAX). It is suitable for research and lightweight applications requiring efficient, transparent language generation.",
      "summary_zh": "OPT-125M是Meta开源的1.25亿参数语言模型，属于OPT系列的基础版本。该模型采用标准的Transformer解码器架构，支持英文文本生成任务。作为研究友好型模型，其权重完全开放，便于学术界开展语言模型机理、性能分析等实验。适用于轻量级文本补全、语言理解研究及教育场景，为资源受限环境提供可复现的基线模型。",
      "summary_es": "OPT-125m es un modelo de lenguaje pequeño basado en el transformer de decoder, desarrollado por Meta. Especializado en generación de texto en inglés, es ideal para tareas básicas de NLP como completar frases o experimentos educativos. Su arquitectura eficiente permite un despliegue rápido en entornos con recursos limitados.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：opt-125m"
    },
    {
      "id": "Qwen/Qwen2.5-3B-Instruct",
      "source": "hf",
      "name": "Qwen2.5-3B-Instruct",
      "url": "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "chat",
        "conversational",
        "en",
        "arxiv:2407.10671",
        "base_model:Qwen/Qwen2.5-3B",
        "base_model:finetune:Qwen/Qwen2.5-3B",
        "license:other",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 7220665,
        "likes_total": 311
      },
      "score": 14596.83,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Qwen2.5-3B-Instruct 是基于 Qwen2.5-3B 微调优化的对话专用模型，参数量为 30 亿。该模型专为自然语言交互场景设计，支持多轮对话与指令跟随，适用于聊天助手、任务型对话系统等应用。其亮点在于在保持轻量级架构的同时，实现了较高的响应质量与对话连贯性，并能够处理中英文混合输入。模型以 SafeTensors 格式发布，便于通过 Transformers 库快速集成与部署，适合资源受限环境或需要快速原型验证的对话场景。",
      "updated_at": "2025-09-21T12:06:53.894Z",
      "summary_en": "Qwen2.5-3B-Instruct is a 3-billion-parameter instruction-tuned language model optimized for conversational and text-generation tasks. It excels in chat applications, providing coherent and context-aware responses. Built on the Qwen2.5-3B base model, it supports English and is suitable for lightweight, efficient AI deployments. The model is ideal for developers needing a compact yet capable solution for dialogue systems and instruction-following applications.",
      "summary_zh": "Qwen2.5-3B-Instruct 是基于 Qwen2.5-3B 微调优化的对话专用模型，参数量为 30 亿。该模型专为自然语言交互场景设计，支持多轮对话与指令跟随，适用于聊天助手、任务型对话系统等应用。其亮点在于在保持轻量级架构的同时，实现了较高的响应质量与对话连贯性，并能够处理中英文混合输入。模型以 SafeTensors 格式发布，便于通过 Transformers 库快速集成与部署，适合资源受限环境或需要快速原型验证的对话场景。",
      "summary_es": "Qwen2.5-3B-Instruct es un modelo de lenguaje pequeño (3B) optimizado para diálogo y tareas de instrucción. Basado en Qwen2.5, destaca por su eficiencia en generación de texto conversacional y su bajo consumo de recursos. Es ideal para aplicaciones que requieren respuestas rápidas y precisas en chats o asistentes, así como para fine-tuning en entornos con limitaciones computacionales.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：Qwen2.5-3B-Instruct"
    },
    {
      "id": "tech4humans/yolov8s-signature-detector",
      "source": "hf",
      "name": "yolov8s-signature-detector",
      "url": "https://huggingface.co/tech4humans/yolov8s-signature-detector",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "ultralytics",
        "tensorboard",
        "onnx",
        "object-detection",
        "signature-detection",
        "yolo",
        "yolov8",
        "pytorch",
        "dataset:tech4humans/signature-detection",
        "base_model:Ultralytics/YOLOv8",
        "base_model:quantized:Ultralytics/YOLOv8",
        "license:agpl-3.0",
        "model-index",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 43141280,
        "likes_total": 39
      },
      "score": 86302.06,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "这是一个基于YOLOv8s的专用签名检测模型，专为文档处理场景设计。该模型在tech4humans团队构建的签名检测数据集上训练，能够精准识别各类文档中的手写签名区域。支持ONNX格式导出和TensorBoard训练监控，便于实际部署与性能分析。适用于合同管理、档案数字化等需要自动化签名识别的业务场景，为文档处理流程提供可靠的计算机视觉解决方案。",
      "updated_at": "2025-09-21T12:06:53.893Z",
      "task_keys": [
        "object_detection"
      ],
      "summary_en": "YOLOv8s-signature-detector is an object detection model fine-tuned from Ultralytics' YOLOv8s to identify signatures in documents. It is optimized for signature detection tasks, leveraging a specialized dataset for training. The model supports export to ONNX for deployment and integrates with TensorBoard for monitoring. It is suitable for automating signature verification in digital document processing workflows.",
      "summary_zh": "这是一个基于YOLOv8s的专用签名检测模型，专为文档处理场景设计。该模型在tech4humans团队构建的签名检测数据集上训练，能够精准识别各类文档中的手写签名区域。支持ONNX格式导出和TensorBoard训练监控，便于实际部署与性能分析。适用于合同管理、档案数字化等需要自动化签名识别的业务场景，为文档处理流程提供可靠的计算机视觉解决方案。",
      "summary_es": "Este modelo YOLOv8 especializado detecta firmas en documentos usando computer vision. Basado en YOLOv8s, ofrece alta precisión para identificar firmas manuscritas o digitales en diversos formatos. Es útil para automatizar procesos de verificación documental, procesamiento de archivos y extracción de información. Optimizado para integración con ONNX y seguimiento mediante TensorBoard.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：yolov8s-signature-detector"
    },
    {
      "id": "pyannote/segmentation-3.0",
      "source": "hf",
      "name": "segmentation-3.0",
      "url": "https://huggingface.co/pyannote/segmentation-3.0",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "pyannote-audio",
        "pytorch",
        "pyannote",
        "pyannote-audio-model",
        "audio",
        "voice",
        "speech",
        "speaker",
        "speaker-diarization",
        "speaker-change-detection",
        "speaker-segmentation",
        "voice-activity-detection",
        "overlapped-speech-detection",
        "resegmentation",
        "license:mit",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 18789102,
        "likes_total": 591
      },
      "score": 37873.704,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "segmentation-3.0是基于PyTorch框架的音频处理模型，专注于语音信号中的说话人分割任务。该模型能够精确检测音频流中的说话人转换点，为说话人日志化系统提供关键的时间边界信息。其核心优势在于采用先进的深度学习架构，在说话人变化检测任务上表现出较高的时间精度。该模型适用于会议记录、访谈转录等需要区分不同说话人的场景，可作为语音处理流程中的基础组件使用。",
      "updated_at": "2025-09-21T12:06:53.893Z",
      "summary_en": "segmentation-3.0 is a PyTorch-based model for speaker diarization and change detection in audio. It identifies speaker segments and detects when speakers change in recordings. The model is useful for transcribing meetings, analyzing interviews, and processing podcasts. It excels in handling diverse audio conditions and supports integration via the pyannote-audio library.",
      "summary_zh": "segmentation-3.0是基于PyTorch框架的音频处理模型，专注于语音信号中的说话人分割任务。该模型能够精确检测音频流中的说话人转换点，为说话人日志化系统提供关键的时间边界信息。其核心优势在于采用先进的深度学习架构，在说话人变化检测任务上表现出较高的时间精度。该模型适用于会议记录、访谈转录等需要区分不同说话人的场景，可作为语音处理流程中的基础组件使用。",
      "summary_es": "Modelo de segmentación de audio basado en PyTorch para detección de cambios de hablante y diarización. Identifica segmentos de voz y transiciones entre locutores en grabaciones. Útil para análisis de conversaciones, transcripciones automáticas y procesamiento de audio. Funciona con señales crudas sin requerir metadatos adicionales.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：segmentation-3.0"
    },
    {
      "id": "Bingsu/adetailer",
      "source": "hf",
      "name": "adetailer",
      "url": "https://huggingface.co/Bingsu/adetailer",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "ultralytics",
        "pytorch",
        "dataset:wider_face",
        "dataset:skytnt/anime-segmentation",
        "doi:10.57967/hf/3633",
        "license:apache-2.0",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 15383187,
        "likes_total": 616
      },
      "score": 31074.374,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "adetailer是一个基于Ultralytics和PyTorch的自动面部检测与修复工具，适用于图像生成后的细节增强。该模型融合了WIDER FACE人脸数据集和动漫分割数据集，能够精准识别并优化生成图像中的人脸区域，包括修复模糊五官、补充细节特征。其亮点在于支持真人照片与动漫风格图像的跨域处理，可无缝集成到AI绘画流程中提升输出质量。适用于数字艺术创作、图像后期处理等需要自动化人脸增强的场景。",
      "updated_at": "2025-09-21T12:06:53.894Z",
      "summary_en": "ADetailer is an open-source tool for automatic face detection and enhancement in images. It leverages models trained on datasets like WIDER FACE and anime segmentation to accurately identify and refine facial features. The tool is particularly useful for improving portrait quality, generating consistent character faces in media, and automating image editing workflows. Built with PyTorch and Ultralytics, it is well-suited for developers and creators working on computer vision applications.",
      "summary_zh": "adetailer是一个基于Ultralytics和PyTorch的自动面部检测与修复工具，适用于图像生成后的细节增强。该模型融合了WIDER FACE人脸数据集和动漫分割数据集，能够精准识别并优化生成图像中的人脸区域，包括修复模糊五官、补充细节特征。其亮点在于支持真人照片与动漫风格图像的跨域处理，可无缝集成到AI绘画流程中提升输出质量。适用于数字艺术创作、图像后期处理等需要自动化人脸增强的场景。",
      "summary_es": "adetailer es un modelo de detección de rostros basado en Ultralytics y PyTorch. Utiliza datasets como WIDER FACE y anime-segmentation para mejorar la precisión en entornos diversos. Es ideal para aplicaciones de análisis facial en imágenes reales y animadas. Su licencia Apache 2.0 facilita su uso en proyectos comerciales y de investigación.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：adetailer"
    },
    {
      "id": "openai/clip-vit-base-patch32",
      "source": "hf",
      "name": "clip-vit-base-patch32",
      "url": "https://huggingface.co/openai/clip-vit-base-patch32",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "clip",
        "zero-shot-image-classification",
        "vision",
        "arxiv:2103.00020",
        "arxiv:1908.04913",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 15096939,
        "likes_total": 767
      },
      "score": 30577.378,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "CLIP-ViT-Base-Patch32是OpenAI发布的多模态预训练模型，通过对比学习将图像与文本映射到统一语义空间。该模型采用ViT-Base架构，以32×32像素块处理图像，配合文本编码器实现跨模态理解。其核心优势在于支持零样本图像分类，无需针对特定任务微调即可根据自然语言描述完成图像识别。典型应用包括图文匹配、内容检索和跨模态分析，适用于需要快速适配新分类场景的视觉任务。模型基于Transformer架构，提供PyTorch、TensorFlow和JAX多框架支持。",
      "updated_at": "2025-09-21T12:06:53.894Z",
      "summary_en": "CLIP-ViT-Base-Patch32 is a vision-language model that connects images and text for zero-shot image classification. It excels at matching images with natural language descriptions without task-specific training. The model is versatile for applications like content moderation, visual search, and multimodal understanding. It supports multiple frameworks (PyTorch, TensorFlow, JAX) and is based on the Vision Transformer architecture with a 32x32 patch size.",
      "summary_zh": "CLIP-ViT-Base-Patch32是OpenAI发布的多模态预训练模型，通过对比学习将图像与文本映射到统一语义空间。该模型采用ViT-Base架构，以32×32像素块处理图像，配合文本编码器实现跨模态理解。其核心优势在于支持零样本图像分类，无需针对特定任务微调即可根据自然语言描述完成图像识别。典型应用包括图文匹配、内容检索和跨模态分析，适用于需要快速适配新分类场景的视觉任务。模型基于Transformer架构，提供PyTorch、TensorFlow和JAX多框架支持。",
      "summary_es": "CLIP-ViT-Base-Patch32 es un modelo multimodal que conecta imágenes y texto mediante aprendizaje contrastivo. Combina un codificador ViT-Base para imágenes con un transformador para texto, permitiendo clasificación de imágenes zero-shot y búsqueda multimodal. Sus puntos fuertes incluyen versatilidad para múltiples tareas sin fine-tuning y compatibilidad con varios frameworks. Es útil para aplicaciones como filtrado de contenido, recuperación de imágenes y sistemas de recomendación visual.",
      "reason_label": "notable",
      "reason_text": "值得关注的项目：clip-vit-base-patch32",
      "task_keys": [
        "image_classification",
        "contrastive_learning",
        "content_moderation"
      ]
    },
    {
      "id": "context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16",
      "source": "hf",
      "name": "meta-llama-Llama-3.2-3B-Instruct-FP16",
      "url": "https://huggingface.co/context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "facebook",
        "meta",
        "pytorch",
        "llama-3",
        "conversational",
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th",
        "arxiv:2204.05149",
        "arxiv:2405.16406",
        "license:llama3.2",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "downloads_total": 9025392,
        "likes_total": 7
      },
      "score": 18054.284,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "这是一个基于Meta Llama 3.2架构的30亿参数指令调优模型，专为对话交互场景优化。模型采用FP16精度格式，在保持生成质量的同时提升推理效率。该版本支持英文文本生成任务，适用于聊天助手、内容创作等需要自然语言理解的场景。作为Llama-3系列的轻量化版本，它在单卡环境下即可流畅运行，为开发者提供了更易部署的AI对话解决方案。",
      "updated_at": "2025-09-21T12:06:53.894Z",
      "summary_en": "Meta-llama-Llama-3.2-3B-Instruct-FP16 is a 3-billion-parameter instruction-tuned language model based on Meta's Llama 3.2 architecture. It is optimized for conversational tasks and text generation in English. The model is distributed in FP16 precision for efficient inference, making it suitable for deployment on consumer-grade hardware. Its primary use cases include chatbots, virtual assistants, and other interactive AI applications requiring responsive dialogue.",
      "summary_zh": "这是一个基于Meta Llama 3.2架构的30亿参数指令调优模型，专为对话交互场景优化。模型采用FP16精度格式，在保持生成质量的同时提升推理效率。该版本支持英文文本生成任务，适用于聊天助手、内容创作等需要自然语言理解的场景。作为Llama-3系列的轻量化版本，它在单卡环境下即可流畅运行，为开发者提供了更易部署的AI对话解决方案。",
      "summary_es": "Meta-llama-Llama-3.2-3B-Instruct-FP16 es un modelo de lenguaje de 3.000 millones de parámetros optimizado para instrucciones, basado en Llama 3. Utiliza precisión FP16 para mayor eficiencia en memoria y velocidad. Especializado en tareas conversacionales y generación de texto en inglés, es ideal para chatbots, asistentes virtuales y aplicaciones de procesamiento de lenguaje natural. Su arquitectura permite un buen equilibrio entre rendimiento y recursos computacionales.",
      "reason_label": "model_optimization",
      "reason_text": "模型优化/量化相关实践：meta-llama-Llama-3.2-3B-Instruct-FP16"
    }
  ]
}