{
  "updated_at": "2025-09-17T18:42:44.950Z",
  "items": [
    {
      "id": "omni-research/Tarsier2-Recap-7b",
      "source": "hf",
      "name": "Tarsier2-Recap-7b",
      "url": "https://huggingface.co/omni-research/Tarsier2-Recap-7b",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "safetensors",
        "video LLM",
        "arxiv:2501.07888",
        "license:apache-2.0",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 8263301,
        "hf_likes": 17
      },
      "score": 16535.102,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Tarsier2-Recap-7b 是一个基于 Transformer 架构的视频语言模型，专注于视频内容理解与摘要生成。该模型能够处理视频输入并生成结构化的文本摘要，适用于视频内容分析、自动字幕生成和多媒体信息检索等场景。其亮点在于结合了视觉与语言模态，支持对视频时序信息的建模，提升了长视频内容的理解能力。该模型适用于视频平台、智能监控和多媒体数据分析等领域，为自动化视频处理提供了高效工具。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_en": "Tarsier2-Recap-7b is a 7-billion-parameter video language model designed for summarizing and analyzing video content. It excels at generating concise recaps and extracting key information from videos, making it suitable for applications in media analysis, content indexing, and accessibility. Its Apache 2.0 license and strong performance metrics indicate broad usability for research and practical deployment. The model is particularly effective for tasks requiring efficient video understanding without extensive computational resources.",
      "summary_zh": "Tarsier2-Recap-7b 是一个基于 Transformer 架构的视频语言模型，专注于视频内容理解与摘要生成。该模型能够处理视频输入并生成结构化的文本摘要，适用于视频内容分析、自动字幕生成和多媒体信息检索等场景。其亮点在于结合了视觉与语言模态，支持对视频时序信息的建模，提升了长视频内容的理解能力。该模型适用于视频平台、智能监控和多媒体数据分析等领域，为自动化视频处理提供了高效工具。",
      "summary_es": "Tarsier2-Recap-7b es un modelo de lenguaje multimodal especializado en el análisis y resumen de contenido de video. Basado en la arquitectura Llama-3.2-7B, destaca por su capacidad para procesar marcos de video y generar descripciones textuales precisas. Sus principales aplicaciones incluyen la creación automática de resúmenes de vídeos, extracción de información clave y generación de metadatos descriptivos. El modelo utiliza el formato de tensores seguros (safetensors) y está disponible bajo licencia Apache 2.0."
    },
    {
      "id": "meta-llama/Llama-3.1-8B-Instruct",
      "source": "hf",
      "name": "Llama-3.1-8B-Instruct",
      "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "facebook",
        "meta",
        "pytorch",
        "llama-3",
        "conversational",
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th",
        "arxiv:2204.05149",
        "base_model:meta-llama/Llama-3.1-8B",
        "base_model:finetune:meta-llama/Llama-3.1-8B",
        "license:llama3.1",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 7230352,
        "hf_likes": 4626
      },
      "score": 16773.703999999998,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Llama-3.1-8B-Instruct 是 Meta 推出的开源对话优化语言模型，基于 Llama-3 架构，参数量为 80 亿。该模型专为指令遵循和对话交互设计，适用于文本生成、问答、代码辅助等多种任务。其亮点在于高效的推理性能与较强的上下文理解能力，尤其适合资源受限环境下的部署。模型支持 Transformers 和 PyTorch 框架，主要面向英语场景，可用于聊天机器人、内容创作或自动化任务处理等应用。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_en": "Llama-3.1-8B-Instruct is an 8-billion-parameter language model optimized for instruction-following tasks. It excels in conversational applications, text generation, and structured reasoning. Built on the robust Llama-3 architecture, it offers strong performance in English while maintaining efficiency for deployment. Ideal for chatbots, content creation, and research prototyping.",
      "summary_zh": "Llama-3.1-8B-Instruct 是 Meta 推出的开源对话优化语言模型，基于 Llama-3 架构，参数量为 80 亿。该模型专为指令遵循和对话交互设计，适用于文本生成、问答、代码辅助等多种任务。其亮点在于高效的推理性能与较强的上下文理解能力，尤其适合资源受限环境下的部署。模型支持 Transformers 和 PyTorch 框架，主要面向英语场景，可用于聊天机器人、内容创作或自动化任务处理等应用。",
      "summary_es": "Llama-3.1-8B-Instruct es un modelo de lenguaje de 8 mil millones de parámetros optimizado para tareas de instrucción y diálogo. Basado en la arquitectura Transformer, destaca por su eficiencia computacional y capacidad para generar respuestas coherentes en inglés. Es ideal para chatbots, asistentes virtuales y aplicaciones de procesamiento de lenguaje natural. Su licencia permite uso comercial e investigación."
    },
    {
      "id": "amazon/chronos-t5-small",
      "source": "hf",
      "name": "chronos-t5-small",
      "url": "https://huggingface.co/amazon/chronos-t5-small",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "safetensors",
        "t5",
        "text2text-generation",
        "time series",
        "forecasting",
        "pretrained models",
        "foundation models",
        "time series foundation models",
        "time-series",
        "time-series-forecasting",
        "arxiv:2403.07815",
        "arxiv:1910.10683",
        "license:apache-2.0",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 13199198,
        "hf_likes": 131
      },
      "score": 26463.896,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "Chronos-T5-small是亚马逊基于T5架构开发的时间序列预测基础模型。该模型采用文本到文本的生成范式，将数值时间序列转换为标记序列进行训练和推理。其核心优势在于通过大规模预训练捕捉通用时间模式，无需领域特异性训练即可完成多种预测任务。模型适用于商业指标预测、传感器数据分析等场景，为时间序列分析提供了零样本和少样本的解决方案。开源版本为small规模，平衡了性能与计算效率。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "Chronos-T5-small is a compact, pretrained time series forecasting model based on T5 architecture. It excels at converting time series data into text-like sequences for accurate predictions across various domains like finance, energy, and IoT. Its strengths include efficient handling of univariate series, zero-shot generalization, and integration with Hugging Face Transformers. Ideal for researchers and practitioners needing lightweight, scalable forecasting without extensive retraining.",
      "summary_zh": "Chronos-T5-small是亚马逊基于T5架构开发的时间序列预测基础模型。该模型采用文本到文本的生成范式，将数值时间序列转换为标记序列进行训练和推理。其核心优势在于通过大规模预训练捕捉通用时间模式，无需领域特异性训练即可完成多种预测任务。模型适用于商业指标预测、传感器数据分析等场景，为时间序列分析提供了零样本和少样本的解决方案。开源版本为small规模，平衡了性能与计算效率。",
      "summary_es": "Chronos-T5-small es un modelo de series temporales basado en T5, preentrenado para predecir secuencias numéricas. Destaca por su capacidad de generalización en múltiples dominios sin ajuste específico. Usa tokenización numérica y genera pronósticos en formato de texto. Ideal para aplicaciones de forecasting en finanzas, energía o demanda."
    },
    {
      "id": "distilbert/distilbert-base-uncased",
      "source": "hf",
      "name": "distilbert-base-uncased",
      "url": "https://huggingface.co/distilbert/distilbert-base-uncased",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "safetensors",
        "distilbert",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1910.01108",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 12227309,
        "hf_likes": 755
      },
      "score": 24832.118000000002,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "DistilBERT-base-uncased 是一个轻量化的 BERT 模型，通过知识蒸馏技术将 BERT 参数量压缩 40%，同时保留 97% 的语言理解能力。它适用于掩码语言建模任务，支持多种框架，包括 PyTorch、TensorFlow 和 JAX。该模型在英文文本处理中表现高效，特别适合资源受限或需要快速推理的场景，如搜索、分类和实体识别。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "DistilBERT is a distilled version of BERT, designed for efficient natural language understanding tasks. It retains 97% of BERT's performance while being 40% smaller and 60% faster, making it ideal for resource-constrained environments. Common use cases include text classification, named entity recognition, and masked language modeling. It supports multiple frameworks and is widely used in production for its balance of speed and accuracy.",
      "summary_zh": "DistilBERT-base-uncased 是一个轻量化的 BERT 模型，通过知识蒸馏技术将 BERT 参数量压缩 40%，同时保留 97% 的语言理解能力。它适用于掩码语言建模任务，支持多种框架，包括 PyTorch、TensorFlow 和 JAX。该模型在英文文本处理中表现高效，特别适合资源受限或需要快速推理的场景，如搜索、分类和实体识别。",
      "summary_es": "DistilBERT es un modelo de lenguaje basado en BERT, optimizado para eficiencia y velocidad. Conserva el 97% del rendimiento de BERT original con un 40% menos de parámetros. Ideal para tareas de procesamiento de lenguaje natural como clasificación de texto, análisis de sentimiento y relleno de máscaras. Ampliamente utilizado en entornos con recursos limitados."
    },
    {
      "id": "FacebookAI/roberta-large",
      "source": "hf",
      "name": "roberta-large",
      "url": "https://huggingface.co/FacebookAI/roberta-large",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "onnx",
        "safetensors",
        "roberta",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1907.11692",
        "arxiv:1806.02847",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 12085823,
        "hf_likes": 245
      },
      "score": 24294.146,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "RoBERTa-large是基于Transformer架构的大规模预训练语言模型，由Facebook AI开发。它在BERT的基础上通过优化训练策略（如移除下一句预测任务、使用更大批次和更长的序列）提升了性能。该模型适用于多种自然语言处理任务，包括文本分类、命名实体识别和掩码语言建模。主要面向研究人员和开发者，用于构建高效的NLP应用或作为下游任务的基准模型。支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "RoBERTa-large is a robust transformer-based language model optimized for masked language modeling. It excels in a wide range of natural language understanding tasks, including text classification, named entity recognition, and sentiment analysis. With strong performance across benchmarks, it is well-suited for research and production applications requiring high accuracy. Its compatibility with multiple frameworks (PyTorch, TensorFlow, JAX, ONNX) ensures flexibility in deployment.",
      "summary_zh": "RoBERTa-large是基于Transformer架构的大规模预训练语言模型，由Facebook AI开发。它在BERT的基础上通过优化训练策略（如移除下一句预测任务、使用更大批次和更长的序列）提升了性能。该模型适用于多种自然语言处理任务，包括文本分类、命名实体识别和掩码语言建模。主要面向研究人员和开发者，用于构建高效的NLP应用或作为下游任务的基准模型。支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。",
      "summary_es": "RoBERTa-large es un modelo de lenguaje basado en transformers optimizado para tareas de comprensión y generación de texto. Destaca por su entrenamiento robusto sin tareas de siguiente frase, mejorando rendimiento en clasificación, entidad nombrada y relleno de máscaras. Usos comunes incluyen análisis de sentimientos, preguntas-respuestas y procesamiento de lenguaje natural en inglés."
    },
    {
      "id": "google/electra-base-discriminator",
      "source": "hf",
      "name": "electra-base-discriminator",
      "url": "https://huggingface.co/google/electra-base-discriminator",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "electra",
        "pretraining",
        "en",
        "arxiv:1406.2661",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 11547887,
        "hf_likes": 64
      },
      "score": 23127.774,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "ELECTRA-base-discriminator是由Google开发的一种基于ELECTRA架构的预训练语言模型判别器。该模型通过替换token检测任务进行预训练，相比传统MLM方法具有更高的训练效率。模型支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。适用于文本分类、情感分析等下游NLP任务，特别适合需要高效文本表征的场景。模型在英文语料上训练，采用Apache 2.0开源协议。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "ELECTRA-base-discriminator is a pretrained transformer model designed for efficient discriminative tasks. It excels in natural language understanding, including text classification, named entity recognition, and sentiment analysis. Its key strength lies in its pretraining efficiency, outperforming many models of similar size. It is widely applicable in research and production environments for English-language NLP tasks.",
      "summary_zh": "ELECTRA-base-discriminator是由Google开发的一种基于ELECTRA架构的预训练语言模型判别器。该模型通过替换token检测任务进行预训练，相比传统MLM方法具有更高的训练效率。模型支持多种深度学习框架，包括PyTorch、TensorFlow和JAX。适用于文本分类、情感分析等下游NLP任务，特别适合需要高效文本表征的场景。模型在英文语料上训练，采用Apache 2.0开源协议。",
      "summary_es": "ELECTRA-base-discriminator es un modelo de lenguaje preentrenado que utiliza un enfoque de discriminación para el aprendizaje de representaciones. Destaca por su eficiencia computacional y rendimiento en tareas de comprensión del lenguaje natural. Es ideal para clasificación de texto, análisis de sentimientos y extracción de información. Soporta múltiples frameworks como PyTorch, TensorFlow y JAX."
    },
    {
      "id": "prajjwal1/bert-tiny",
      "source": "hf",
      "name": "bert-tiny",
      "url": "https://huggingface.co/prajjwal1/bert-tiny",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "BERT",
        "MNLI",
        "NLI",
        "transformer",
        "pre-training",
        "en",
        "arxiv:1908.08962",
        "arxiv:2110.01518",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 10176180,
        "hf_likes": 127
      },
      "score": 20415.86,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "bert-tiny是一个轻量级BERT模型，专为资源受限环境设计。它基于BERT架构，但显著减少了参数量，在保持合理性能的同时大幅提升推理速度。该模型适用于文本分类、自然语言推理等任务，特别适合移动设备或边缘计算场景。支持PyTorch框架，预训练权重可直接用于下游任务的微调。",
      "updated_at": "2025-09-17T17:53:14.778Z",
      "summary_en": "BERT-Tiny is a compact, efficient variant of the BERT model, designed for resource-constrained environments. It is pre-trained on English text and fine-tuned for natural language inference tasks like MNLI. Ideal for applications requiring fast inference and low memory usage, such as mobile or edge devices. Its small size makes it suitable for prototyping and lightweight NLP pipelines.",
      "summary_zh": "bert-tiny是一个轻量级BERT模型，专为资源受限环境设计。它基于BERT架构，但显著减少了参数量，在保持合理性能的同时大幅提升推理速度。该模型适用于文本分类、自然语言推理等任务，特别适合移动设备或边缘计算场景。支持PyTorch框架，预训练权重可直接用于下游任务的微调。",
      "summary_es": "BERT-Tiny es una versión ultrapequeña del modelo BERT, optimizada para eficiencia computacional. Ideal para dispositivos con recursos limitados, mantiene capacidades sólidas en tareas de comprensión del lenguaje natural como inferencia (NLI/MNLI). Su diseño compacto permite despliegues rápidos en entornos restringidos, manteniendo un buen rendimiento en clasificación de texto y otras aplicaciones de NLP ligero."
    },
    {
      "id": "trpakov/vit-face-expression",
      "source": "hf",
      "name": "vit-face-expression",
      "url": "https://huggingface.co/trpakov/vit-face-expression",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "vit",
        "image-classification",
        "doi:10.57967/hf/2289",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 8025363,
        "hf_likes": 78
      },
      "score": 16089.726,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "vit-face-expression是基于Vision Transformer架构的面部表情分类模型，能够识别图像中的七种基本表情（如高兴、悲伤、愤怒等）。该模型使用PyTorch和ONNX格式提供，支持快速推理和部署，适用于实时表情分析场景。其亮点在于结合了ViT的全局建模能力与轻量化设计，在保证精度的同时具备较高的计算效率。适用于人机交互、情感计算或心理学研究等需要自动化表情识别的领域。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_en": "A Vision Transformer (ViT) model fine-tuned for facial expression classification. It supports PyTorch, ONNX, and SafeTensors formats, enabling efficient deployment across platforms. Ideal for emotion recognition in images, with applications in human-computer interaction and behavioral analysis. High download count reflects its reliability and broad usability.",
      "summary_zh": "vit-face-expression是基于Vision Transformer架构的面部表情分类模型，能够识别图像中的七种基本表情（如高兴、悲伤、愤怒等）。该模型使用PyTorch和ONNX格式提供，支持快速推理和部署，适用于实时表情分析场景。其亮点在于结合了ViT的全局建模能力与轻量化设计，在保证精度的同时具备较高的计算效率。适用于人机交互、情感计算或心理学研究等需要自动化表情识别的领域。",
      "summary_es": "Modelo de clasificación de expresiones faciales basado en Vision Transformer (ViT). Detecta emociones como alegría, tristeza o enfado en imágenes. Destaca por su precisión y eficiencia usando arquitecturas transformer. Útil para análisis de comportamiento humano o interacciones persona-computadora."
    },
    {
      "id": "BAAI/bge-base-en-v1.5",
      "source": "hf",
      "name": "bge-base-en-v1.5",
      "url": "https://huggingface.co/BAAI/bge-base-en-v1.5",
      "license": "N/A",
      "lang": "N/A",
      "tags": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "mteb",
        "en",
        "arxiv:2401.03462",
        "arxiv:2312.15503",
        "arxiv:2311.13534",
        "arxiv:2310.07554",
        "arxiv:2309.07597",
        "license:mit",
        "model-index",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "categories": {
        "capabilities": [],
        "scenes": [],
        "lifecycle": []
      },
      "stats": {
        "hf_downloads_7d": 7340919,
        "hf_likes": 346
      },
      "score": 14854.838,
      "timeline": {
        "t": [],
        "stars": [],
        "downloads": []
      },
      "summary": "bge-base-en-v1.5 是一个基于 BERT 架构的英文文本嵌入模型，由北京智源人工智能研究院（BAAI）开发。该模型主要用于生成高质量的句子或段落级向量表示，适用于语义相似度计算、信息检索和文本聚类等任务。其亮点在于通过大规模对比学习优化，在多项基准测试（如 MTEB）中表现优异，尤其擅长处理英文文本的语义理解。适用于需要高效且精准的文本表示嵌入的场景，如搜索引擎、推荐系统或自然语言处理应用。",
      "updated_at": "2025-09-17T17:53:14.779Z",
      "summary_en": "BGE-base-en-v1.5 is a BERT-based English sentence embedding model optimized for semantic similarity and retrieval tasks. It excels in generating dense vector representations for text, making it suitable for applications like search, clustering, and recommendation systems. With strong performance on benchmarks like MTEB, it is widely used in both research and production environments. The model supports multiple deployment formats, including PyTorch, ONNX, and SafeTensors, ensuring flexibility and ease of integration.",
      "summary_zh": "bge-base-en-v1.5 是一个基于 BERT 架构的英文文本嵌入模型，由北京智源人工智能研究院（BAAI）开发。该模型主要用于生成高质量的句子或段落级向量表示，适用于语义相似度计算、信息检索和文本聚类等任务。其亮点在于通过大规模对比学习优化，在多项基准测试（如 MTEB）中表现优异，尤其擅长处理英文文本的语义理解。适用于需要高效且精准的文本表示嵌入的场景，如搜索引擎、推荐系统或自然语言处理应用。",
      "summary_es": "Modelo de incrustación de texto BGE base en inglés, versión 1.5. Basado en BERT, genera representaciones vectoriales densas para textos. Destaca en similitud semántica y recuperación de información. Usos: búsqueda, clustering y aplicaciones de NLP que requieran comparación de similitudes entre oraciones."
    }
  ]
}