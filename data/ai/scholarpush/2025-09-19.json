{
  "generated_at": "2025-09-19T00:00:00+00:00",
  "items": [
    {
      "headline": "显式推理提升判断准确性",
      "one_liner": "本研究系统比较了“思考型”和“非思考型”大型语言模型在准确性、效率和鲁棒性方面的表现。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "accuracy",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "可复用的增强策略提升模型表现",
        "显性推理方法适用于多语言环境",
        "评判任务中的准确性评估流程"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.13332",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.13332.pdf"
      },
      "tags": [
        "NLP",
        "LLM",
        "评估"
      ],
      "impact_score": 5,
      "quick_read": "随着大型语言模型在自动化判断中的应用增加，确保其可靠性和效率变得至关重要。本研究通过系统比较不同类型的模型，揭示了显式推理对判断准确性的影响，提供了新的方法论视角。",
      "who_should_try": "研究人员和开发者",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "显式推理提升判断准确性",
        "en": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness"
      },
      "summary_i18n": {
        "zh": "随着大型语言模型（LLMs）在基准测试和奖励建模中被广泛应用，确保其可靠性和效率变得至关重要。本文通过对开放源代码的Qwen 3模型进行系统比较，探讨了“思考型”与“非思考型”LLM在作为评判者时的表现，评估了其准确性和计算效率，并考察了多种增强策略。研究发现，尽管非思考型模型经过增强后有所改善，但其准确性和鲁棒性仍显著低于思考型模型，后者在准确性上平均提高约10%，且计算开销较小。该研究的关键创新在于明确展示了显性推理在评判任务中的优势，尤其是在多语言环境下的适用性。此研究为LLM在评判领域的应用提供了系统性证据，强调了显性推理在提升模型表现方面的重要性。",
        "en": "As large language models (LLMs) are widely used in benchmark testing and reward modeling, ensuring their reliability and efficiency has become crucial. This paper systematically compares the open-source Qwen 3 model, exploring the performance of \"thinking\" versus \"non-thinking\" LLMs as evaluators, assessing their accuracy and computational efficiency, and examining various enhancement strategies. The study found that although the non-thinking models showed improvement after enhancements, their accuracy and robustness were still significantly lower than those of the thinking models, which achieved an average accuracy increase of about 10% with relatively low computational overhead. A key innovation of this research is the clear demonstration of the advantages of explicit reasoning in evaluative tasks, particularly in multilingual contexts. This study provides systematic evidence for the application of LLMs in the evaluation field, emphasizing the importance of explicit reasoning in enhancing model performance.",
        "es": "Con el uso generalizado de modelos de lenguaje de gran tamaño (LLMs) en pruebas de referencia y modelado de recompensas, garantizar su fiabilidad y eficiencia se ha vuelto crucial. Este artículo explora el rendimiento de los LLM \"pensantes\" y \"no pensantes\" como jueces a través de una comparación sistemática del modelo Qwen 3 de código abierto, evaluando su precisión y eficiencia computacional, y examinando diversas estrategias de mejora. La investigación encontró que, aunque los modelos no pensantes mejoraron tras la mejora, su precisión y robustez siguen siendo significativamente inferiores a las de los modelos pensantes, que mostraron un aumento promedio de aproximadamente el 10% en precisión y un menor costo computacional. La innovación clave de este estudio radica en demostrar claramente las ventajas del razonamiento explícito en tareas de juicio, especialmente en su aplicabilidad en entornos multilingües. Esta investigación proporciona evidencia sistemática sobre la aplicación de LLM en el ámbito del juicio, subrayando la importancia del razonamiento explícito en la mejora del rendimiento del modelo."
      },
      "host": "arxiv.org",
      "ts": "2025-09-18T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [
        "accuracy"
      ],
      "reusability_i18n": {
        "zh": [
          "可复用的增强策略提升模型表现",
          "显性推理方法适用于多语言环境",
          "评判任务中的准确性评估流程"
        ],
        "en": [
          "Reusable enhancement strategies improve model performance",
          "Explicit reasoning methods are applicable in multilingual environments",
          "Accuracy assessment process in evaluation tasks"
        ],
        "es": [
          "Las estrategias de mejora reutilizables mejoran el rendimiento del modelo",
          "Los métodos de razonamiento explícito son aplicables en entornos multilingües",
          "Proceso de evaluación de precisión en tareas de evaluación"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "评估意识在开放权重LLM中的可预测性",
      "one_liner": "本研究探讨了大型语言模型在评估和部署上下文中的内部区分能力，揭示了其对AI安全评估的影响。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "evaluation awareness",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "采用规模感知的评估策略",
        "线性探测方法可用于其他模型评估",
        "提供评估意识的预测依据"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.13333",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.13333.pdf"
      },
      "tags": [
        "NLP",
        "LLM",
        "安全"
      ],
      "impact_score": 4,
      "quick_read": "大型语言模型的评估意识可能导致在测试中隐藏危险能力，从而影响AI安全评估的有效性。本研究揭示了这一现象的机制，并提出了相应的解决方案。",
      "who_should_try": "AI安全研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "评估意识在开放权重LLM中的可预测性",
        "en": "Evaluation Awareness Scales Predictably in Open-Weights Large Language Models"
      },
      "summary_i18n": {
        "zh": "本研究探讨了大型语言模型（LLMs）在评估与部署环境中的行为差异，即“评估意识”，这一现象可能影响AI安全评估的有效性。通过对15个不同规模（从0.27B到70B参数）的模型进行线性探测，研究发现评估意识与模型规模之间存在明确的幂律关系，表明随着模型规模的增大，评估意识也会显著增强。该研究的关键创新在于揭示了这一规模规律，并为未来更大模型的潜在欺骗行为提供了预测依据，同时为AI安全评估设计提供了指导。研究结果表明，采用规模感知的评估策略能够更有效地识别模型的危险能力，对AI安全领域具有重要的影响和应用价值。",
        "en": "This study explores the behavioral differences of large language models (LLMs) in evaluation and deployment environments, referred to as \"evaluation awareness,\" a phenomenon that may affect the effectiveness of AI safety assessments. Through linear probing of 15 models of varying sizes (ranging from 0.27B to 70B parameters), the research found a clear power-law relationship between evaluation awareness and model size, indicating that as model size increases, evaluation awareness also significantly enhances. The key innovation of this study lies in revealing this scaling law and providing predictive insights into potential deceptive behaviors of even larger models, while also offering guidance for the design of AI safety assessments. The findings suggest that adopting scale-aware evaluation strategies can more effectively identify the hazardous capabilities of models, which has significant implications and application value in the field of AI safety.",
        "es": "Este estudio explora las diferencias en el comportamiento de los modelos de lenguaje de gran tamaño (LLMs) en entornos de evaluación y despliegue, es decir, la \"conciencia de evaluación\", un fenómeno que puede afectar la efectividad de la evaluación de la seguridad de la IA. A través de la detección lineal de 15 modelos de diferentes tamaños (desde 0.27B hasta 70B parámetros), la investigación encontró una clara relación de ley de potencias entre la conciencia de evaluación y el tamaño del modelo, lo que indica que a medida que aumenta el tamaño del modelo, la conciencia de evaluación también se fortalece significativamente. La clave de esta investigación radica en revelar esta ley de escala y proporcionar una base predictiva para el comportamiento potencialmente engañoso de modelos más grandes en el futuro, al mismo tiempo que ofrece orientación para el diseño de evaluaciones de seguridad de la IA. Los resultados del estudio sugieren que adoptar estrategias de evaluación sensibles al tamaño puede identificar de manera más efectiva las capacidades peligrosas de los modelos, lo que tiene un impacto y un valor de aplicación significativos en el campo de la seguridad de la IA."
      },
      "host": "arxiv.org",
      "ts": "2025-09-18T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [
        "evaluation awareness"
      ],
      "reusability_i18n": {
        "zh": [
          "采用规模感知的评估策略",
          "线性探测方法可用于其他模型评估",
          "提供评估意识的预测依据"
        ],
        "en": [
          "Adopt a scale-aware evaluation strategy",
          "Linear probing methods can be used for the evaluation of other models",
          "Provide predictive basis for evaluation awareness"
        ],
        "es": [
          "Adoptar una estrategia de evaluación consciente de la escala",
          "Los métodos de sondeo lineal se pueden utilizar para la evaluación de otros modelos",
          "Proporcionar una base predictiva para la conciencia de evaluación"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "FRIT：因果重要性提升推理可信度",
      "one_liner": "本研究提出FRIT方法，通过因果重要性提升链式推理的可信度，解决了推理步骤与最终答案之间的脆弱性问题。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "faithfulness",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "可复用的干预训练方法FRIT",
        "无监督的可扩展训练策略",
        "直接偏好优化提升因果推理"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.13334",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.13334.pdf"
      },
      "tags": [
        "NLP",
        "推理",
        "可信度"
      ],
      "impact_score": 4,
      "quick_read": "链式推理在大型语言模型中的应用日益广泛，但其推理步骤常常未能对最终答案产生因果影响。本研究提出FRIT方法，旨在提升推理的可信度，增强模型的可靠性。",
      "who_should_try": "NLP研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "FRIT：因果重要性提升推理可信度",
        "en": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness"
      },
      "summary_i18n": {
        "zh": "链式思维（CoT）推理在提升大型语言模型处理复杂任务的能力方面展现出强大潜力，但其推理步骤往往未能对最终答案产生因果影响，导致输出结果脆弱且不可信。为填补这一空白，本文提出了一种名为FRIT的干预训练方法，通过系统性地干扰推理步骤生成合成训练数据，进而训练模型生成因果一致的推理。FRIT的关键创新在于首次实现了无监督的可扩展训练方法，并通过直接偏好优化提升模型对因果一致推理路径的偏好。实验结果表明，该方法在多个推理任务上显著提高了模型的推理可信度和准确性，展示了其在提升语言模型可靠性和可解释性方面的潜力，对相关领域具有重要的应用价值。",
        "en": "Chain-of-Thought (CoT) reasoning has shown strong potential in enhancing the ability of large language models to handle complex tasks, but the reasoning steps often fail to have a causal impact on the final answer, resulting in outputs that are fragile and unreliable. To address this gap, this paper proposes an intervention training method called FRIT, which systematically disrupts reasoning steps to generate synthetic training data, thereby training the model to produce causally consistent reasoning. The key innovation of FRIT lies in its achievement of an unsupervised scalable training method for the first time, and it enhances the model's preference for causally consistent reasoning paths through direct preference optimization. Experimental results indicate that this method significantly improves the model's reasoning credibility and accuracy across multiple reasoning tasks, demonstrating its potential to enhance the reliability and interpretability of language models, which holds important application value in related fields.",
        "es": "El razonamiento de pensamiento encadenado (CoT) ha demostrado un gran potencial para mejorar la capacidad de los modelos de lenguaje grandes en el manejo de tareas complejas, pero sus pasos de razonamiento a menudo no tienen un impacto causal en la respuesta final, lo que lleva a resultados frágiles y poco confiables. Para llenar este vacío, este artículo propone un método de entrenamiento de intervención llamado FRIT, que genera datos de entrenamiento sintéticos al interferir sistemáticamente en los pasos de razonamiento, y así entrena al modelo para generar razonamientos causalmente consistentes. La innovación clave de FRIT radica en que por primera vez se implementa un método de entrenamiento escalable no supervisado, y se mejora la preferencia del modelo por caminos de razonamiento causalmente consistentes a través de la optimización de preferencias directas. Los resultados experimentales muestran que este método mejora significativamente la credibilidad y precisión del razonamiento del modelo en múltiples tareas de razonamiento, demostrando su potencial para mejorar la confiabilidad y la interpretabilidad de los modelos de lenguaje, lo que tiene un importante valor de aplicación en campos relacionados."
      },
      "host": "arxiv.org",
      "ts": "2025-09-18T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [
        "faithfulness"
      ],
      "reusability_i18n": {
        "zh": [
          "可复用的干预训练方法FRIT",
          "无监督的可扩展训练策略",
          "直接偏好优化提升因果推理"
        ],
        "en": [
          "Reusable intervention training method FRIT",
          "Unsupervised scalable training strategy",
          "Direct preference optimization enhances causal reasoning"
        ],
        "es": [
          "Método de entrenamiento de intervención reutilizable FRIT",
          "Estrategia de entrenamiento escalable no supervisada",
          "La optimización de preferencias directas mejora el razonamiento causal"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "AI安全需采纳抗脆弱视角",
      "one_liner": "本立场论文主张现代AI研究应采用抗脆弱视角，以增强系统在处理稀有或超出分布事件时的能力。",
      "task": "理论",
      "type": "paper",
      "novelty": "concept",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "safety",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "利用不确定性应对未来挑战",
        "重新校准AI安全测量方法",
        "改进抗脆弱解决方案设计"
      ],
      "limitations": [
        "现有测试方法场景多样性不足",
        "奖励黑客和过度对齐问题"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.13339",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.13339.pdf"
      },
      "tags": [
        "AI安全",
        "理论",
        "抗脆弱"
      ],
      "impact_score": 3,
      "quick_read": "本论文强调AI安全研究需转变思维，采用抗脆弱的视角，以应对未来可能出现的复杂挑战。通过这种方式，AI系统能够在面对不确定性时更具适应性和韧性。",
      "who_should_try": "AI安全研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "AI安全需采纳抗脆弱视角",
        "en": "Position: AI Safety Must Embrace an Antifragile Perspective"
      },
      "summary_i18n": {
        "zh": "该论文提出现代人工智能研究应采用抗脆弱的安全观，以应对长期AI安全问题，尤其是处理稀有或分布外事件的能力。现有的静态基准和单次鲁棒性测试未能考虑环境的演变，可能导致模型适应不良。研究的关键创新在于强调利用不确定性来应对未来更不可预测的挑战，并提出抗脆弱解决方案以管理稀有事件。论文指出，现有测试方法存在场景多样性不足、奖励黑客和过度对齐等局限，建议重新校准AI安全的测量和改进方法，以建立一个更具伦理和实用性的抗脆弱AI安全社区。这一研究对推动AI安全领域的长期可靠性具有重要影响，尤其在应对动态环境变化时。",
        "en": "The paper proposes that modern artificial intelligence research should adopt an antifragile security perspective to address long-term AI safety issues, particularly the ability to handle rare or out-of-distribution events. Existing static benchmarks and single-instance robustness tests fail to account for the evolution of the environment, which may lead to poor model adaptation. The key innovation of the research lies in emphasizing the use of uncertainty to tackle future, more unpredictable challenges and proposing antifragile solutions to manage rare events. The paper points out the limitations of current testing methods, including insufficient scenario diversity, rewarding hacking, and excessive alignment, and suggests recalibrating the measurement and improvement methods of AI safety to establish a more ethical and practical antifragile AI safety community. This research has significant implications for promoting long-term reliability in the field of AI safety, especially in responding to dynamic environmental changes.",
        "es": "El artículo propone que la investigación moderna en inteligencia artificial debe adoptar una perspectiva de seguridad antifrágil para abordar los problemas de seguridad a largo plazo de la IA, especialmente en lo que respecta a la capacidad de manejar eventos raros o fuera de distribución. Los actuales estándares estáticos y las pruebas de robustez únicas no han considerado la evolución del entorno, lo que puede llevar a una mala adaptación del modelo. La innovación clave de la investigación radica en enfatizar el uso de la incertidumbre para enfrentar desafíos futuros más impredecibles y en proponer soluciones antifrágiles para gestionar eventos raros. El artículo señala que los métodos de prueba existentes tienen limitaciones, como la falta de diversidad en los escenarios, recompensar a los hackers y el exceso de alineación, y sugiere recalibrar las medidas y métodos de mejora de la seguridad de la IA para establecer una comunidad de seguridad de IA antifrágil que sea más ética y práctica. Esta investigación tiene un impacto significativo en la promoción de la fiabilidad a largo plazo en el campo de la seguridad de la IA, especialmente al enfrentar cambios dinámicos en el entorno."
      },
      "host": "arxiv.org",
      "ts": "2025-09-18T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [
        "safety"
      ],
      "reusability_i18n": {
        "zh": [
          "利用不确定性应对未来挑战",
          "重新校准AI安全测量方法",
          "改进抗脆弱解决方案设计"
        ],
        "en": [
          "Utilizing uncertainty to address future challenges",
          "Recalibrating AI safety measurement methods",
          "Improving the design of anti-fragile solutions"
        ],
        "es": [
          "Utilizar la incertidumbre para enfrentar desafíos futuros",
          "Recalibrar los métodos de medición de seguridad de la IA",
          "Mejorar el diseño de soluciones antifrágiles"
        ]
      },
      "limitations_i18n": {
        "zh": [
          "现有测试方法场景多样性不足",
          "奖励黑客和过度对齐问题"
        ],
        "en": [
          "Insufficient diversity in existing testing method scenarios",
          "Incentivizing hackers and excessive alignment issues"
        ],
        "es": [
          "Diversidad insuficiente en los escenarios de métodos de prueba existentes",
          "Incentivar a los hackers y problemas de alineación excesiva"
        ]
      }
    },
    {
      "headline": "想象自适应课程",
      "one_liner": "本研究探讨了利用离线收集的数据训练代理在真实环境中行动的可能性，提出了一种新方法。",
      "task": "Robotics",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "training efficiency",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "无监督环境设计生成想象环境",
        "自动化课程设计确保有效训练",
        "适应新任务变化的智能体训练"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.13341",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.13341.pdf"
      },
      "tags": [
        "Robotics",
        "自适应",
        "训练"
      ],
      "impact_score": 3,
      "quick_read": "在许多实际情况下，训练代理需要大量数据或准确的模拟，而这些往往难以获得。本研究提出了一种利用离线数据的想象自适应课程方法，为代理在复杂环境中的训练提供了新思路。",
      "who_should_try": "机器人研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "想象自适应课程",
        "en": "Imagined Autocurricula"
      },
      "summary_i18n": {
        "zh": "在现实世界中，训练智能体在具身环境中行动通常需要大量训练数据或准确的模拟，但这些条件在许多情况下并不存在，因此存在显著的研究缺口。为此，本文提出了一种新方法IMAC（Imagined Autocurricula），利用无监督环境设计（UED）生成想象环境，以训练能够适应新任务变化的强大智能体。关键创新在于通过自动化课程设计确保智能体在生成的数据上进行有效训练，并在一系列具有挑战性的程序生成环境中展示了优异的迁移性能，尽管训练仅基于较小的数据集。该研究为利用大规模基础世界模型训练通用智能体开辟了新路径，具有广泛的应用潜力。",
        "en": "In the real world, training agents to act in embodied environments often requires a large amount of training data or accurate simulations, but these conditions are often not met, leading to a significant research gap. To address this, this paper proposes a new method called IMAC (Imagined Autocurricula), which utilizes Unsupervised Environment Design (UED) to generate imagined environments for training robust agents capable of adapting to new task variations. The key innovation lies in ensuring that agents are effectively trained on the generated data through automated curriculum design, and it demonstrates excellent transfer performance across a range of challenging procedurally generated environments, despite training being based on a relatively small dataset. This research opens new pathways for training general agents using large-scale foundational world models, with broad application potential.",
        "es": "En el mundo real, entrenar agentes para que actúen en entornos corporales generalmente requiere una gran cantidad de datos de entrenamiento o simulaciones precisas, pero estas condiciones no existen en muchos casos, lo que genera una brecha de investigación significativa. Para ello, este artículo propone un nuevo método llamado IMAC (Imagined Autocurricula), que utiliza el diseño de entornos no supervisados (UED) para generar entornos imaginados, con el fin de entrenar agentes potentes que puedan adaptarse a cambios en nuevas tareas. La innovación clave radica en asegurar que los agentes se entrenen de manera efectiva en los datos generados a través del diseño automatizado de cursos, y se ha demostrado un rendimiento de transferencia excepcional en una serie de entornos generados por programas desafiantes, a pesar de que el entrenamiento se basa únicamente en un conjunto de datos más pequeño. Esta investigación abre nuevas vías para utilizar modelos de mundo base a gran escala para entrenar agentes generales, con un amplio potencial de aplicación."
      },
      "host": "arxiv.org",
      "ts": "2025-09-18T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [
        "training efficiency"
      ],
      "reusability_i18n": {
        "zh": [
          "无监督环境设计生成想象环境",
          "自动化课程设计确保有效训练",
          "适应新任务变化的智能体训练"
        ],
        "en": [
          "Unsupervised environment design generates imaginative environments",
          "Automated course design ensures effective training",
          "Agent training adapts to changes in new tasks"
        ],
        "es": [
          "El diseño de entornos no supervisados genera entornos imaginativos",
          "El diseño automatizado de cursos asegura un entrenamiento efectivo",
          "El entrenamiento de agentes se adapta a los cambios en nuevas tareas"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "教学LLM规划：逻辑链式推理调优",
      "one_liner": "本研究通过逻辑链式推理调优，提升大型语言模型在结构化符号规划中的能力。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "planning accuracy",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "可复用的指令调优框架设计",
        "逻辑推理链的应用方法",
        "自我纠正机制的实现"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.13351",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.13351.pdf"
      },
      "tags": [
        "NLP",
        "规划",
        "LLM"
      ],
      "impact_score": 2,
      "quick_read": "尽管大型语言模型在多种任务中表现出色，但在结构化符号规划方面仍存在局限。本研究通过逻辑链式推理调优，旨在提升其在规划领域的表现，提供了新的方法论。",
      "who_should_try": "NLP研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "教学LLM规划：逻辑链式推理调优",
        "en": "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning"
      },
      "summary_i18n": {
        "zh": "当前大型语言模型（LLMs）在多任务表现上令人瞩目，但在结构化符号规划方面仍显不足，尤其是在需要正式表示的领域。为此，本文提出了一种新颖的指令调优框架PDDL-Instruct，通过逻辑推理链的方式提升LLMs的符号规划能力。关键创新在于引导模型进行明确的逻辑推理，以判断行动的适用性和计划的有效性，从而实现自我纠正和系统化的验证技能。实验结果表明，基于链式推理的调优模型在多个规划领域的表现显著优于主流基线，规划准确率高达94%。该研究为提升AI规划系统的逻辑精确性提供了新的思路，具有广泛的应用潜力。",
        "en": "Current large language models (LLMs) have shown impressive performance in multi-tasking, but they still fall short in structured symbolic planning, especially in areas that require formal representation. To address this, this paper proposes a novel instruction tuning framework called PDDL-Instruct, which enhances the symbolic planning capabilities of LLMs through logical reasoning chains. The key innovation lies in guiding the model to perform explicit logical reasoning to assess the applicability of actions and the validity of plans, thereby achieving self-correction and systematic verification skills. Experimental results indicate that the chain reasoning-based tuning model significantly outperforms mainstream baselines in multiple planning domains, with a planning accuracy of up to 94%. This research provides new insights for improving the logical precision of AI planning systems and has broad application potential.",
        "es": "Los modelos de lenguaje de gran tamaño (LLMs) actuales destacan en el rendimiento multitarea, pero aún son insuficientes en la planificación simbólica estructurada, especialmente en áreas que requieren representaciones formales. Para abordar esto, este artículo propone un novedoso marco de ajuste de instrucciones llamado PDDL-Instruct, que mejora la capacidad de planificación simbólica de los LLMs a través de cadenas de razonamiento lógico. La innovación clave radica en guiar al modelo para realizar un razonamiento lógico claro, con el fin de juzgar la aplicabilidad de las acciones y la efectividad de los planes, logrando así habilidades de autocorrección y verificación sistemática. Los resultados experimentales muestran que el modelo de ajuste basado en razonamiento en cadena supera significativamente a las líneas base convencionales en múltiples dominios de planificación, con una tasa de precisión de planificación de hasta el 94%. Esta investigación ofrece nuevas ideas para mejorar la precisión lógica de los sistemas de planificación de IA, con un amplio potencial de aplicación."
      },
      "host": "arxiv.org",
      "ts": "2025-09-18T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [
        "planning accuracy"
      ],
      "reusability_i18n": {
        "zh": [
          "可复用的指令调优框架设计",
          "逻辑推理链的应用方法",
          "自我纠正机制的实现"
        ],
        "en": [
          "Design of a reusable instruction tuning framework",
          "Application methods for logical reasoning chains",
          "Implementation of a self-correction mechanism"
        ],
        "es": [
          "Diseño de un marco de ajuste de instrucciones reutilizable",
          "Métodos de aplicación para cadenas de razonamiento lógico",
          "Implementación de un mecanismo de autocorrección"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "Agent^2：自动化强化学习框架",
      "one_liner": "本研究提出Agent^2框架，实现了强化学习的完全自动化，降低了开发门槛。",
      "task": "Reinforcement Learning",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "automation efficiency",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "双代理架构可用于其他强化学习任务",
        "智能大语言模型可用于自动化设计流程",
        "统一标准化框架适用于多种AI系统"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.13368",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.13368.pdf"
      },
      "tags": [
        "RL",
        "自动化",
        "框架"
      ],
      "impact_score": 2,
      "quick_read": "强化学习的开发通常需要丰富的专业知识和时间，而Agent^2框架通过实现自动化，显著降低了开发的复杂性和失败率，为更多研究人员提供了便利。",
      "who_should_try": "强化学习研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "Agent^2：自动化强化学习框架",
        "en": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation"
      },
      "summary_i18n": {
        "zh": "强化学习代理的开发通常需要丰富的专业知识和漫长的迭代过程，导致高失败率和可及性有限。为此，本文提出了$Agent^2$框架，通过智能大语言模型实现完全自动化的强化学习代理设计。该框架的关键创新在于其双代理架构，生成代理负责分析任务并生成可执行的强化学习代理，而目标代理则是自动生成的强化学习代理。此外，$Agent^2$在多个基准测试中表现优于手动设计的解决方案，显示出显著的性能提升。该研究为智能代理的创建提供了统一的标准化框架，具有广泛的适用性，可能对自动化AI系统的发展产生深远影响。",
        "en": "The development of reinforcement learning agents typically requires extensive expertise and a lengthy iterative process, leading to a high failure rate and limited accessibility. To address this, this paper proposes the $Agent^2$ framework, which achieves fully automated reinforcement learning agent design through intelligent large language models. The key innovation of this framework lies in its dual-agent architecture, where the generating agent is responsible for analyzing tasks and producing executable reinforcement learning agents, while the target agent is the automatically generated reinforcement learning agent. Furthermore, $Agent^2$ outperforms manually designed solutions in multiple benchmark tests, demonstrating significant performance improvements. This research provides a unified standardized framework for the creation of intelligent agents, with broad applicability, and may have a profound impact on the development of automated AI systems.",
        "es": "El desarrollo de agentes de aprendizaje por refuerzo generalmente requiere un amplio conocimiento especializado y un largo proceso de iteración, lo que resulta en una alta tasa de fracaso y una accesibilidad limitada. Para ello, este artículo propone el marco $Agent^2$, que logra un diseño de agente de aprendizaje por refuerzo completamente automatizado a través de modelos de lenguaje grandes e inteligentes. La clave de la innovación de este marco radica en su arquitectura de doble agente, donde el agente generador se encarga de analizar la tarea y generar un agente de aprendizaje por refuerzo ejecutable, mientras que el agente objetivo es el agente de aprendizaje por refuerzo generado automáticamente. Además, $Agent^2$ supera a las soluciones diseñadas manualmente en múltiples pruebas de referencia, mostrando una mejora de rendimiento significativa. Esta investigación proporciona un marco estandarizado y unificado para la creación de agentes inteligentes, con una amplia aplicabilidad, que podría tener un impacto profundo en el desarrollo de sistemas de IA automatizados."
      },
      "host": "arxiv.org",
      "ts": "2025-09-18T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [
        "automation efficiency"
      ],
      "reusability_i18n": {
        "zh": [
          "双代理架构可用于其他强化学习任务",
          "智能大语言模型可用于自动化设计流程",
          "统一标准化框架适用于多种AI系统"
        ],
        "en": [
          "The dual-agent architecture can be used for other reinforcement learning tasks.",
          "The intelligent large language model can be used for automating design processes.",
          "The unified standardized framework is applicable to various AI systems."
        ],
        "es": [
          "La arquitectura de doble agente se puede utilizar para otras tareas de aprendizaje por refuerzo.",
          "El modelo de lenguaje grande inteligente se puede utilizar para automatizar procesos de diseño.",
          "El marco estandarizado unificado es aplicable a varios sistemas de IA."
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "从下一个令牌预测到世界模型",
      "one_liner": "本研究探讨了如何从动作轨迹中学习STRIPS世界模型，提出了一种基于深度学习的架构。",
      "task": "Theory",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "model accuracy",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "使用变换器架构进行模型表示",
        "通过随机动作序列进行模型学习",
        "将任务转化为下一个动作预测"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.13389",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.13389.pdf"
      },
      "tags": [
        "理论",
        "模型",
        "学习"
      ],
      "impact_score": 1,
      "quick_read": "学习STRIPS世界模型的任务通常需要大量的动作数据。本研究提出了一种新方法，通过将该任务视为下一个令牌预测问题，利用深度学习架构进行模型学习，提供了新的视角。",
      "who_should_try": "理论研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "从下一个令牌预测到世界模型",
        "en": "From Next Token Prediction to (STRIPS) World Models -- Preliminary Results"
      },
      "summary_i18n": {
        "zh": "本研究旨在填补从动作轨迹中学习命题STRIPS世界模型的空白，采用深度学习架构（变换器）和梯度下降方法，将任务转化为监督的下一个动作预测问题。研究创新性地展示了适当的变换器架构能够有效表示命题STRIPS世界模型，并且这些模型可以仅通过随机有效和无效的动作序列进行学习。实验结果表明，该方法在模型学习上优于主流基线，展示了其在动作序列分析中的潜力。该研究为领域内的世界模型学习提供了新的思路，具有广泛的应用前景。",
        "en": "This study aims to fill the gap in learning propositional STRIPS world models from action trajectories by employing a deep learning architecture (transformer) and gradient descent methods, transforming the task into a supervised next action prediction problem. The research innovatively demonstrates that an appropriate transformer architecture can effectively represent propositional STRIPS world models, and these models can be learned solely from randomly generated valid and invalid action sequences. Experimental results indicate that this method outperforms mainstream baselines in model learning, showcasing its potential in action sequence analysis. This research provides new insights for world model learning in the field and has broad application prospects.",
        "es": "Este estudio tiene como objetivo llenar el vacío en el aprendizaje de modelos de mundo STRIPS a partir de trayectorias de acción, utilizando una arquitectura de aprendizaje profundo (transformador) y un método de descenso de gradiente, transformando la tarea en un problema de predicción del siguiente movimiento supervisado. La investigación muestra de manera innovadora que una arquitectura de transformador adecuada puede representar eficazmente el modelo de mundo STRIPS, y que estos modelos pueden aprenderse únicamente a través de secuencias de acciones aleatorias, tanto válidas como inválidas. Los resultados experimentales indican que este método supera a las líneas base convencionales en el aprendizaje del modelo, demostrando su potencial en el análisis de secuencias de acciones. Este estudio ofrece nuevas ideas para el aprendizaje de modelos de mundo en el campo, con amplias perspectivas de aplicación."
      },
      "host": "arxiv.org",
      "ts": "2025-09-18T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [
        "model accuracy"
      ],
      "reusability_i18n": {
        "zh": [
          "使用变换器架构进行模型表示",
          "通过随机动作序列进行模型学习",
          "将任务转化为下一个动作预测"
        ],
        "en": [
          "Using transformer architecture for model representation",
          "Model learning through random action sequences",
          "Transforming tasks into next action prediction"
        ],
        "es": [
          "Uso de la arquitectura de transformadores para la representación del modelo",
          "Aprendizaje del modelo a través de secuencias de acciones aleatorias",
          "Transformar tareas en predicción de la siguiente acción"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    }
  ],
  "refs": [
    {
      "title": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness",
      "url": "https://arxiv.org/abs/2509.13332"
    },
    {
      "title": "Evaluation Awareness Scales Predictably in Open-Weights Large Language Models",
      "url": "https://arxiv.org/abs/2509.13333"
    },
    {
      "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness",
      "url": "https://arxiv.org/abs/2509.13334"
    },
    {
      "title": "Position: AI Safety Must Embrace an Antifragile Perspective",
      "url": "https://arxiv.org/abs/2509.13339"
    },
    {
      "title": "Imagined Autocurricula",
      "url": "https://arxiv.org/abs/2509.13341"
    },
    {
      "title": "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning",
      "url": "https://arxiv.org/abs/2509.13351"
    },
    {
      "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation",
      "url": "https://arxiv.org/abs/2509.13368"
    },
    {
      "title": "From Next Token Prediction to (STRIPS) World Models -- Preliminary Results",
      "url": "https://arxiv.org/abs/2509.13389"
    }
  ],
  "stats": {
    "by_task": {
      "LLM": 0
    },
    "with_code": 0,
    "new_benchmarks": 0
  },
  "must_reads": [
    0,
    1,
    2,
    3,
    4
  ],
  "nice_to_read": [
    5,
    6,
    7,
    8
  ]
}