{
  "generated_at": "2025-09-26T00:00:00+00:00",
  "items": [
    {
      "headline": "用户模拟在AGI追求中的重要性",
      "one_liner": "本研究探讨用户模拟在评估复杂交互系统和训练自适应代理中的关键作用。",
      "task": "Other",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "用户模拟可用于生成交互数据",
        "模拟器设计可复用于评估系统",
        "智能任务代理与用户模拟协同"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.19456",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.19456.pdf"
      },
      "tags": [
        "AGI",
        "用户模拟",
        "交互系统"
      ],
      "impact_score": 1,
      "quick_read": "本研究强调用户模拟在AGI发展中的重要性，尤其是在复杂交互系统的评估和自适应代理的训练中。通过创建用户模拟，研究者可以更有效地收集交互数据，从而推动AGI的进步。",
      "who_should_try": "研究人员和开发者",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "用户模拟在AGI追求中的重要性",
        "en": "The Indispensable Role of User Simulation in the Pursuit of AGI"
      },
      "summary_i18n": {
        "zh": "在追求通用人工智能（AGI）的过程中，评估复杂交互系统和获取丰富的交互数据面临显著瓶颈，现有研究对此关注不足。本文提出用户模拟，即创建模仿人类与AI系统交互的计算代理，是克服这些瓶颈、加速AGI发展的关键。研究的创新点在于强调用户模拟在可扩展评估和数据生成中的重要性，以及其与智能任务代理的协同发展。结果表明，现实的模拟器能够有效促进AGI的适应能力培养，优于主流基线方法。该研究为未来的用户模拟技术和智能代理的研究提供了新的视角，可能对AGI领域的进展产生深远影响。",
        "en": "In the pursuit of Artificial General Intelligence (AGI), evaluating complex interactive systems and acquiring rich interaction data face significant bottlenecks, and existing research has paid insufficient attention to this issue. This paper proposes user simulation, which involves creating computational agents that mimic human interactions with AI systems, as a key to overcoming these bottlenecks and accelerating AGI development. The innovation of this research lies in emphasizing the importance of user simulation in scalable evaluation and data generation, as well as its synergistic development with intelligent task agents. The results indicate that realistic simulators can effectively enhance the adaptability training of AGI, outperforming mainstream baseline methods. This study provides a new perspective for future research on user simulation technology and intelligent agents, which may have a profound impact on the progress in the field of AGI.",
        "es": "En el proceso de búsqueda de la inteligencia artificial general (AGI), la evaluación de sistemas de interacción complejos y la obtención de datos de interacción ricos enfrentan un notable cuello de botella, y la investigación existente presta poca atención a esto. Este artículo propone la simulación de usuarios, es decir, la creación de agentes computacionales que imitan la interacción entre humanos y sistemas de IA, como clave para superar estos cuellos de botella y acelerar el desarrollo de la AGI. El punto innovador de la investigación radica en enfatizar la importancia de la simulación de usuarios en la evaluación escalable y la generación de datos, así como su desarrollo colaborativo con agentes de tareas inteligentes. Los resultados indican que simuladores realistas pueden promover de manera efectiva el desarrollo de la adaptabilidad de la AGI, superando a los métodos de referencia convencionales. Este estudio ofrece una nueva perspectiva para la futura investigación en tecnologías de simulación de usuarios y agentes inteligentes, lo que podría tener un impacto profundo en el avance del campo de la AGI."
      },
      "host": "arxiv.org",
      "ts": "2025-09-25T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [],
      "reusability_i18n": {
        "zh": [
          "用户模拟可用于生成交互数据",
          "模拟器设计可复用于评估系统",
          "智能任务代理与用户模拟协同"
        ],
        "en": [
          "User simulation can be used to generate interactive data",
          "Simulator design can be reused to evaluate systems",
          "Intelligent task agents collaborate with user simulations"
        ],
        "es": [
          "La simulación de usuarios se puede utilizar para generar datos interactivos",
          "El diseño del simulador se puede reutilizar para evaluar sistemas",
          "Los agentes de tareas inteligentes colaboran con simulaciones de usuarios"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "评估意识强化学习的挑战",
      "one_liner": "本论文分析了现有评估方法在数据有限和长时间任务中的高方差和高偏差问题。",
      "task": "Reinforcement Learning",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "引入评估条件的状态值预测器",
        "最大化预期回报与最小化评估误差",
        "适用于多种离散和连续动作领域"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.19464",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.19464.pdf"
      },
      "tags": [
        "强化学习",
        "评估",
        "高方差"
      ],
      "impact_score": 1,
      "quick_read": "本论文探讨了强化学习中的评估方法，指出现有方法在数据有限和长时间任务中存在的高方差和高偏差问题。提出了改进评估策略的必要性，以提高系统的安全性和性能。",
      "who_should_try": "研究人员和工程师",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "评估意识强化学习的挑战",
        "en": "Evaluation-Aware Reinforcement Learning"
      },
      "summary_i18n": {
        "zh": "本研究针对现有强化学习（RL）方法在安全和性能关键系统中评估阶段面临的高方差和高偏差问题，提出了一种新的评估意识强化学习（EvA-RL）框架。该方法通过在训练策略时同时最大化预期回报和最小化预期评估误差，旨在提高策略的可评估性。关键创新在于引入了评估条件的状态值预测器，以解决评估准确性与策略性能之间的权衡。实验证明，EvA-RL在多种离散和连续动作领域中显著降低了评估误差，同时保持了竞争性的回报。这项研究为将可靠评估作为强化学习训练中的核心原则奠定了基础，具有广泛的应用潜力。",
        "en": "This study addresses the high variance and high bias issues faced by existing reinforcement learning (RL) methods during the evaluation phase in safety and performance-critical systems, proposing a new evaluation-aware reinforcement learning (EvA-RL) framework. This method aims to enhance the assessability of policies by simultaneously maximizing expected returns and minimizing expected evaluation errors during policy training. A key innovation is the introduction of a state value predictor conditioned on evaluation to tackle the trade-off between evaluation accuracy and policy performance. Experiments demonstrate that EvA-RL significantly reduces evaluation errors across various discrete and continuous action domains while maintaining competitive returns. This research lays the groundwork for establishing reliable evaluation as a core principle in reinforcement learning training, with broad application potential.",
        "es": "Este estudio aborda los problemas de alta varianza y alto sesgo que enfrentan los métodos de aprendizaje por refuerzo (RL) existentes en la fase de evaluación en sistemas críticos de seguridad y rendimiento, y propone un nuevo marco de aprendizaje por refuerzo consciente de evaluación (EvA-RL). Este método tiene como objetivo mejorar la evaluabilidad de la política al maximizar simultáneamente el retorno esperado y minimizar el error de evaluación esperado durante el entrenamiento de la política. La innovación clave radica en la introducción de un predictor de valor de estado condicionado a la evaluación, para abordar el compromiso entre la precisión de la evaluación y el rendimiento de la política. Los experimentos demuestran que EvA-RL reduce significativamente el error de evaluación en una variedad de dominios de acciones discretas y continuas, manteniendo al mismo tiempo un retorno competitivo. Esta investigación sienta las bases para establecer la evaluación confiable como un principio central en el entrenamiento de aprendizaje por refuerzo, con un amplio potencial de aplicación."
      },
      "host": "arxiv.org",
      "ts": "2025-09-25T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [],
      "reusability_i18n": {
        "zh": [
          "引入评估条件的状态值预测器",
          "最大化预期回报与最小化评估误差",
          "适用于多种离散和连续动作领域"
        ],
        "en": [
          "Introducing a state value predictor for evaluation conditions",
          "Maximizing expected returns while minimizing evaluation errors",
          "Applicable to various discrete and continuous action domains"
        ],
        "es": [
          "Introducción de un predictor de valor de estado para condiciones de evaluación",
          "Maximizar los retornos esperados mientras se minimizan los errores de evaluación",
          "Aplicable a varios dominios de acciones discretas y continuas"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "大语言模型自一致性估计",
      "one_liner": "本文分析了大语言模型在相同提示下的自一致性及其在固定计算预算下的权衡。",
      "task": "NLP",
      "type": "paper",
      "novelty": "metric",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "自一致性评估的新方法可用于其他模型",
        "提示数量与调用次数的分配策略可复用",
        "优化大语言模型的使用思路可借鉴"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.19489",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.19489.pdf"
      },
      "tags": [
        "自一致性",
        "大语言模型",
        "估计"
      ],
      "impact_score": 1,
      "quick_read": "本文探讨了大语言模型在重复提示下的自一致性，分析了在固定计算预算下的权衡。研究结果为提高模型的可靠性提供了新的视角。",
      "who_should_try": "NLP研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "大语言模型自一致性估计",
        "en": "Estimating the Self-Consistency of LLMs"
      },
      "summary_i18n": {
        "zh": "本研究旨在解决大语言模型（LLMs）在重复提示时响应一致性的问题，填补了现有方法在计算预算下的自一致性评估缺口。通过分析在固定计算预算条件下的自一致性估计器，提出了在任务分布中采样提示数量与每个提示重复调用次数的最佳分配策略，即$m,n\\propto\\sqrt{B}$。关键创新在于提供了自一致性评估的新方法，并揭示了计算预算对模型性能的影响。研究结果表明，该方法在提高响应可靠性方面优于主流基线，具有较高的实用价值。此研究为优化大语言模型的使用提供了新的思路，可能对相关领域的模型设计和应用产生深远影响。",
        "en": "This study aims to address the issue of response consistency in large language models (LLMs) when prompts are repeated, filling the gap in existing methods for self-consistency evaluation under computational budget constraints. By analyzing self-consistency estimators under fixed computational budget conditions, an optimal allocation strategy is proposed for the number of sampled prompts and the number of repetitions for each prompt, specifically $m,n\\propto\\sqrt{B}$. The key innovation lies in providing a new method for self-consistency evaluation and revealing the impact of computational budget on model performance. The results indicate that this method outperforms mainstream baselines in enhancing response reliability, demonstrating high practical value. This research offers new insights for optimizing the use of large language models and may have a profound impact on model design and application in related fields.",
        "es": "Este estudio tiene como objetivo abordar el problema de la consistencia de las respuestas de los grandes modelos de lenguaje (LLMs) al repetir indicaciones, llenando el vacío en la evaluación de la auto-consistencia de los métodos existentes bajo un presupuesto computacional. A través del análisis de los estimadores de auto-consistencia bajo condiciones de presupuesto computacional fijo, se propone una estrategia óptima de asignación entre la cantidad de indicaciones muestreadas en la distribución de tareas y el número de veces que se repite cada indicación, es decir, $m,n\\propto\\sqrt{B}$. La innovación clave radica en proporcionar un nuevo método de evaluación de la auto-consistencia y en revelar el impacto del presupuesto computacional en el rendimiento del modelo. Los resultados de la investigación indican que este método supera a las líneas base predominantes en la mejora de la fiabilidad de las respuestas, mostrando un alto valor práctico. Este estudio ofrece nuevas ideas para optimizar el uso de grandes modelos de lenguaje y podría tener un impacto profundo en el diseño y la aplicación de modelos en campos relacionados."
      },
      "host": "arxiv.org",
      "ts": "2025-09-25T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [],
      "reusability_i18n": {
        "zh": [
          "自一致性评估的新方法可用于其他模型",
          "提示数量与调用次数的分配策略可复用",
          "优化大语言模型的使用思路可借鉴"
        ],
        "en": [
          "The new method for self-consistency evaluation can be used for other models.",
          "The distribution strategy of the number of prompts and the number of calls is reusable.",
          "The approach to optimizing the use of large language models can be referenced."
        ],
        "es": [
          "El nuevo método para la evaluación de auto-consistencia se puede utilizar para otros modelos.",
          "La estrategia de distribución del número de indicaciones y el número de llamadas es reutilizable.",
          "El enfoque para optimizar el uso de modelos de lenguaje grandes se puede referenciar."
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "大语言模型的认知负荷限制",
      "one_liner": "本研究基准测试了大语言模型在多跳推理中的表现，揭示了其在动态环境中的脆弱性。",
      "task": "NLP",
      "type": "paper",
      "novelty": "benchmark",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "设计交错认知评估基准以评估模型表现",
        "利用认知负荷理论优化多步推理任务",
        "系统性操控负荷因素以提高模型韧性"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.19517",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.19517.pdf"
      },
      "tags": [
        "认知负荷",
        "多跳推理",
        "大语言模型"
      ],
      "impact_score": 1,
      "quick_read": "本研究基准测试了大语言模型在多跳推理中的表现，揭示了其在动态信息丰富环境中的脆弱性。研究结果强调了模型在复杂任务中的局限性。",
      "who_should_try": "NLP研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "大语言模型的认知负荷限制",
        "en": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning"
      },
      "summary_i18n": {
        "zh": "大型语言模型（LLMs）的快速发展暴露了其在动态信息丰富环境中的脆弱性，尤其是在多步推理任务中的认知负荷限制尚未得到充分理解。为此，研究者提出了一种形式化的计算认知负荷理论，强调了无关信息的干扰和任务切换对模型性能的影响，并设计了“交错认知评估”（ICE）基准，以系统性地操控这些负荷因素。关键创新在于通过ICE基准揭示了不同模型在认知负荷下的表现差异，尤其是小型开源架构在高负荷任务中表现出明显的脆弱性，而某些模型则在控制条件下展现出一定的韧性。这些结果初步证明了认知负荷是推理失败的重要因素，支持了在不确定性下的猜测理论。研究强调了动态认知压力测试的重要性，为评估先进AI系统的韧性和安全性提供了新的视角，具有广泛的应用潜力。",
        "en": "The rapid development of large language models (LLMs) has exposed their vulnerabilities in dynamically information-rich environments, particularly in multi-step reasoning tasks where the limitations of cognitive load have not been fully understood. To address this, researchers proposed a formalized computational cognitive load theory, highlighting the impact of irrelevant information interference and task switching on model performance, and designed the \"Interleaved Cognitive Evaluation\" (ICE) benchmark to systematically manipulate these load factors. A key innovation is that the ICE benchmark reveals performance differences among various models under cognitive load, particularly showing that small open-source architectures exhibit significant vulnerabilities in high-load tasks, while certain models demonstrate some resilience under controlled conditions. These results preliminarily demonstrate that cognitive load is an important factor in reasoning failures, supporting the theory of guessing under uncertainty. The research emphasizes the importance of dynamic cognitive stress testing, providing a new perspective for assessing the resilience and safety of advanced AI systems, with broad application potential.",
        "es": "El rápido desarrollo de los modelos de lenguaje de gran tamaño (LLMs) ha expuesto su vulnerabilidad en entornos dinámicos ricos en información, especialmente en tareas de razonamiento de múltiples pasos, donde las limitaciones de carga cognitiva aún no se comprenden completamente. Para abordar esto, los investigadores han propuesto una teoría formalizada de carga cognitiva computacional, que enfatiza la interferencia de información irrelevante y el impacto del cambio de tareas en el rendimiento del modelo, y han diseñado el \"Evaluación Cognitiva Intercalada\" (ICE) como un estándar para manipular sistemáticamente estos factores de carga. La innovación clave radica en que el estándar ICE revela las diferencias en el rendimiento de diferentes modelos bajo carga cognitiva, destacando que las arquitecturas de código abierto más pequeñas muestran una vulnerabilidad notable en tareas de alta carga, mientras que ciertos modelos exhiben cierta resiliencia en condiciones controladas. Estos resultados demuestran preliminarmente que la carga cognitiva es un factor importante en el fracaso del razonamiento, apoyando la teoría de conjeturas bajo incertidumbre. La investigación subraya la importancia de las pruebas de presión cognitiva dinámica, proporcionando una nueva perspectiva para evaluar la resiliencia y seguridad de los sistemas de IA avanzados, con un amplio potencial de aplicación."
      },
      "host": "arxiv.org",
      "ts": "2025-09-25T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [],
      "reusability_i18n": {
        "zh": [
          "设计交错认知评估基准以评估模型表现",
          "利用认知负荷理论优化多步推理任务",
          "系统性操控负荷因素以提高模型韧性"
        ],
        "en": [
          "Design interleaved cognitive assessment benchmarks to evaluate model performance",
          "Utilize cognitive load theory to optimize multi-step reasoning tasks",
          "Systematically manipulate load factors to enhance model resilience"
        ],
        "es": [
          "Diseñar benchmarks de evaluación cognitiva entrelazados para evaluar el rendimiento del modelo",
          "Utilizar la teoría de la carga cognitiva para optimizar tareas de razonamiento de múltiples pasos",
          "Manipular sistemáticamente los factores de carga para mejorar la resiliencia del modelo"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "基于VLM的机器人操作子目标评估",
      "one_liner": "本文提出了一种新的评估方法，通过子目标级别报告来提高机器人操作的透明度。",
      "task": "Robotics",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "开源项目蓝图促进社区参与",
        "基于子目标成功率的评估方法",
        "视觉语言模型用于自动判断"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.19524",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.19524.pdf"
      },
      "tags": [
        "机器人",
        "操作评估",
        "子目标"
      ],
      "impact_score": 1,
      "quick_read": "本文提出了一种新的评估方法，通过子目标级别报告来提高机器人操作的透明度。研究表明，这种方法能够更清晰地识别政策在多步骤任务中的成功与失败。",
      "who_should_try": "机器人研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "基于VLM的机器人操作子目标评估",
        "en": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation"
      },
      "summary_i18n": {
        "zh": "本研究旨在填补机器人操作任务中对多步骤成功率评估的不足，强调在评估中应关注子目标的表现。提出了一种名为StepEval的评估框架，该框架利用视觉语言模型（VLM）自动判断子目标的成功与否，旨在提供更细致的评估指标，而非仅依赖于最终成功率。关键创新在于设计了一种可扩展的开源项目蓝图，鼓励社区参与，并提出了以子目标成功率向量为核心的评估方法。研究表明，该框架在评估效率和准确性方面优于传统方法，具有广泛的适用性和轻量化特征，能够促进机器人学习领域的标准化和可重复性。此方法为未来的机器人操作评估提供了新的方向，推动了多步骤任务评估的常规化。",
        "en": "This study aims to address the shortcomings in the evaluation of multi-step success rates in robotic operation tasks, emphasizing the importance of focusing on the performance of sub-goals during assessment. A framework called StepEval is proposed, which utilizes visual language models (VLM) to automatically determine the success or failure of sub-goals, aiming to provide more detailed evaluation metrics rather than relying solely on the final success rate. A key innovation is the design of a scalable open-source project blueprint that encourages community participation, along with an evaluation method centered on sub-goal success rate vectors. The research demonstrates that this framework outperforms traditional methods in terms of evaluation efficiency and accuracy, featuring broad applicability and lightweight characteristics, which can promote standardization and reproducibility in the field of robotic learning. This method offers a new direction for future robotic operation assessments and advances the normalization of multi-step task evaluations.",
        "es": "Este estudio tiene como objetivo llenar el vacío en la evaluación de la tasa de éxito de múltiples pasos en tareas de operación de robots, enfatizando la importancia de centrarse en el rendimiento de los subobjetivos durante la evaluación. Se propone un marco de evaluación llamado StepEval, que utiliza modelos de lenguaje visual (VLM) para juzgar automáticamente el éxito o fracaso de los subobjetivos, con el fin de proporcionar métricas de evaluación más detalladas, en lugar de depender únicamente de la tasa de éxito final. La innovación clave radica en el diseño de un plano de proyecto de código abierto escalable, que fomenta la participación de la comunidad, y se presenta un método de evaluación centrado en el vector de tasa de éxito de los subobjetivos. La investigación muestra que este marco supera a los métodos tradicionales en términos de eficiencia y precisión en la evaluación, con una amplia aplicabilidad y características de ligereza, lo que puede promover la estandarización y la repetibilidad en el campo del aprendizaje de robots. Este método ofrece una nueva dirección para la evaluación de operaciones robóticas en el futuro, impulsando la normalización de la evaluación de tareas de múltiples pasos."
      },
      "host": "arxiv.org",
      "ts": "2025-09-25T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [],
      "reusability_i18n": {
        "zh": [
          "开源项目蓝图促进社区参与",
          "基于子目标成功率的评估方法",
          "视觉语言模型用于自动判断"
        ],
        "en": [
          "The open-source project blueprint promotes community participation",
          "Evaluation method based on sub-goal success rate",
          "Visual language models used for automatic judgment"
        ],
        "es": [
          "El plano del proyecto de código abierto promueve la participación de la comunidad",
          "Método de evaluación basado en la tasa de éxito de subobjetivos",
          "Modelos de lenguaje visual utilizados para el juicio automático"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "小语言模型代理在基因组学中的应用",
      "one_liner": "本研究探讨了小语言模型在基因组学问答中的应用，旨在解决幻觉问题和计算成本挑战。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "任务分解方法可用于其他领域的模型优化",
        "API接入设计可推广至其他系统集成",
        "小型模型的高效性可应用于多种问答场景"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.19566",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.19566.pdf"
      },
      "tags": [
        "基因组学",
        "小语言模型",
        "问答"
      ],
      "impact_score": 1,
      "quick_read": "本研究探讨了小语言模型在基因组学问答中的应用，旨在通过代理框架解决幻觉问题和计算成本挑战。研究结果为基因组学领域的AI应用提供了新思路。",
      "who_should_try": "基因组学研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "小语言模型代理在基因组学中的应用",
        "en": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics"
      },
      "summary_i18n": {
        "zh": "本研究旨在解决小型语言模型在基因组学问答中的应用问题，尤其是幻觉现象和计算成本的挑战。研究团队提出了Nano Bio-Agent（NBA）框架，该框架通过任务分解、工具协同和API接入等方式，整合了NCBI和AlphaGenome等成熟系统。关键创新在于小型语言模型与代理框架的结合，展现出在性能上优于传统大型模型的潜力，尤其在GeneTuring基准测试中取得了98%的准确率。研究结果表明，3-10亿参数的小型模型在保持高准确率的同时，显著降低了计算资源需求，展示了在基因组学工具的高效性、成本节约和普及化方面的前景。这一方法为相关领域提供了新的思路，可能推动机器学习在基因组学中的广泛应用。",
        "en": "This study aims to address the application issues of small language models in genomics question answering, particularly the challenges of hallucination phenomena and computational costs. The research team proposed the Nano Bio-Agent (NBA) framework, which integrates established systems like NCBI and AlphaGenome through task decomposition, tool collaboration, and API access. The key innovation lies in the combination of small language models with the agent framework, demonstrating the potential to outperform traditional large models in terms of performance, especially achieving 98% accuracy in the GeneTuring benchmark test. The results indicate that small models with 300 million to 1 billion parameters significantly reduce computational resource requirements while maintaining high accuracy, showcasing prospects for efficiency, cost savings, and popularization in genomics tools. This approach offers new insights for related fields and may promote the widespread application of machine learning in genomics.",
        "es": "Este estudio tiene como objetivo abordar los problemas de aplicación de modelos de lenguaje pequeños en preguntas y respuestas de genómica, especialmente los desafíos de los fenómenos de alucinación y los costos computacionales. El equipo de investigación propuso el marco Nano Bio-Agent (NBA), que integra sistemas maduros como NCBI y AlphaGenome a través de la descomposición de tareas, la colaboración de herramientas y la conexión a API. La clave de la innovación radica en la combinación de modelos de lenguaje pequeños con el marco de agentes, mostrando un potencial de rendimiento superior al de los modelos grandes tradicionales, especialmente al alcanzar una precisión del 98% en la prueba de referencia GeneTuring. Los resultados de la investigación indican que los modelos pequeños con entre 300 y 1,000 millones de parámetros mantienen una alta precisión mientras reducen significativamente la demanda de recursos computacionales, demostrando perspectivas de eficiencia, ahorro de costos y democratización en las herramientas de genómica. Este enfoque ofrece nuevas ideas para campos relacionados y podría impulsar la amplia aplicación del aprendizaje automático en la genómica."
      },
      "host": "arxiv.org",
      "ts": "2025-09-25T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [],
      "reusability_i18n": {
        "zh": [
          "任务分解方法可用于其他领域的模型优化",
          "API接入设计可推广至其他系统集成",
          "小型模型的高效性可应用于多种问答场景"
        ],
        "en": [
          "The task decomposition method can be used for model optimization in other fields.",
          "The API integration design can be extended to other system integrations.",
          "The efficiency of small models can be applied to various question-and-answer scenarios."
        ],
        "es": [
          "El método de descomposición de tareas se puede utilizar para la optimización de modelos en otros campos.",
          "El diseño de integración de API se puede extender a otras integraciones de sistemas.",
          "La eficiencia de los modelos pequeños se puede aplicar a varios escenarios de preguntas y respuestas."
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "AI能力的稳健推断框架",
      "one_liner": "本论文提出了一种框架，用于评估生成模型在基准数据上的能力，解决了可靠性问题。",
      "task": "Other",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "基于能力理论的评估框架",
        "考虑不确定性的能力推断方法",
        "自适应算法降低样本复杂度"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.19590",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.19590.pdf"
      },
      "tags": [
        "生成模型",
        "基准评估",
        "可靠性"
      ],
      "impact_score": 1,
      "quick_read": "本论文提出了一种框架，用于评估生成模型在基准数据上的能力，解决了当前评估方法的可靠性问题。研究结果为AI能力的评估提供了新的视角。",
      "who_should_try": "AI研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "AI能力的稳健推断框架",
        "en": "What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities"
      },
      "summary_i18n": {
        "zh": "随着生成模型在基准数据上的评估日益普遍，公众和科学界对AI能力的期望受到影响，但对评估结果可靠性的质疑也在增加。现有的评估往往将基准分数视为能力的简单测量，缺乏对能力本质的深入理解。为此，本文提出了一种基于能力理论的评估框架，强调将评估视为推断过程，并引入了考虑不确定性和样本有限性的能力推断方法，包括一种显著降低样本复杂度的自适应算法。该框架的创新之处在于明确了能力评估的理论基础，并提供了更可靠的能力估计方法，具有潜在的广泛应用价值，能够提升AI评估的可信度和科学性。",
        "en": "As the evaluation of generative models on benchmark data becomes increasingly common, public and scientific expectations regarding AI capabilities are influenced, but doubts about the reliability of evaluation results are also growing. Existing evaluations often treat benchmark scores as a simple measure of capability, lacking a deep understanding of the nature of capability. To address this, this paper proposes a capability theory-based evaluation framework that emphasizes viewing evaluation as an inferential process and introduces capability inference methods that consider uncertainty and limited samples, including an adaptive algorithm that significantly reduces sample complexity. The innovation of this framework lies in clarifying the theoretical foundation of capability assessment and providing more reliable capability estimation methods, which have potential broad application value and can enhance the credibility and scientific rigor of AI evaluations.",
        "es": "A medida que la evaluación de modelos generativos en datos de referencia se vuelve cada vez más común, las expectativas del público y de la comunidad científica sobre las capacidades de la IA se ven afectadas, pero también aumentan las dudas sobre la fiabilidad de los resultados de la evaluación. Las evaluaciones existentes a menudo consideran las puntuaciones de referencia como una medida simple de capacidad, careciendo de una comprensión profunda de la naturaleza de la capacidad. Para abordar esto, este artículo propone un marco de evaluación basado en la teoría de la capacidad, que enfatiza la consideración de la evaluación como un proceso de inferencia e introduce un método de inferencia de capacidad que tiene en cuenta la incertidumbre y la limitación de muestras, incluyendo un algoritmo adaptativo que reduce significativamente la complejidad de la muestra. La innovación de este marco radica en clarificar la base teórica de la evaluación de capacidades y proporcionar métodos de estimación de capacidades más fiables, con un potencial valor de aplicación amplio, que puede mejorar la credibilidad y la cientificidad de la evaluación de la IA."
      },
      "host": "arxiv.org",
      "ts": "2025-09-25T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [],
      "reusability_i18n": {
        "zh": [
          "基于能力理论的评估框架",
          "考虑不确定性的能力推断方法",
          "自适应算法降低样本复杂度"
        ],
        "en": [
          "Assessment framework based on capability theory",
          "Capability inference method considering uncertainty",
          "Adaptive algorithms reduce sample complexity"
        ],
        "es": [
          "Marco de evaluación basado en la teoría de capacidades",
          "Método de inferencia de capacidades considerando la incertidumbre",
          "Los algoritmos adaptativos reducen la complejidad de la muestra"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    },
    {
      "headline": "SteinerSQL: 图引导的文本到SQL生成",
      "one_liner": "本研究提出了一种新的方法，解决了复杂文本到SQL查询中的数学推理和模式导航问题。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "采用数学分解优化推理过程",
        "构建Steiner树提升结构设计",
        "多层验证确保结果准确性"
      ],
      "limitations": [],
      "links": {
        "paper": "https://arxiv.org/abs/2509.19623",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.19623.pdf"
      },
      "tags": [
        "文本到SQL",
        "图引导",
        "数学推理"
      ],
      "impact_score": 1,
      "quick_read": "本研究提出了一种新的方法SteinerSQL，解决了复杂文本到SQL查询中的数学推理和模式导航问题。研究结果为提高文本到SQL生成的准确性提供了新的思路。",
      "who_should_try": "NLP研究人员",
      "reproducibility_score": 50,
      "title_i18n": {
        "zh": "SteinerSQL: 图引导的文本到SQL生成",
        "en": "SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation"
      },
      "summary_i18n": {
        "zh": "在复杂的文本到SQL查询中，大型语言模型面临着数学推理和复杂模式导航的挑战，现有方法往往孤立解决这些问题，导致推理过程的逻辑和结构性缺陷。为此，研究者提出了SteinerSQL框架，将这两大挑战整合为一个图中心的优化问题，采用数学分解、Steiner树构建和多层验证三个阶段进行处理。该框架的关键创新在于其统一的推理过程和高效的结构设计，使得在LogicCat和Spider2.0-Lite基准测试中，SteinerSQL分别达到了36.10%和40.04%的执行准确率，超越了主流基线。SteinerSQL不仅提升了准确性，还为文本到SQL生成提供了一种新的统一范式，具有广泛的应用潜力和深远的影响。",
        "en": "In the complex task of converting text to SQL queries, large language models face challenges in mathematical reasoning and navigating complex patterns. Existing methods often address these issues in isolation, leading to logical and structural flaws in the reasoning process. To address this, researchers proposed the SteinerSQL framework, which integrates these two major challenges into a graph-centered optimization problem, processed through three stages: mathematical decomposition, Steiner tree construction, and multi-layer verification. The key innovation of this framework lies in its unified reasoning process and efficient structural design, enabling SteinerSQL to achieve execution accuracies of 36.10% and 40.04% on the LogicCat and Spider2.0-Lite benchmark tests, respectively, surpassing mainstream baselines. SteinerSQL not only enhances accuracy but also provides a new unified paradigm for text-to-SQL generation, with broad application potential and far-reaching impact.",
        "es": "En la conversión de texto complejo a consultas SQL, los modelos de lenguaje grandes enfrentan desafíos en razonamiento matemático y navegación de patrones complejos. Los métodos existentes a menudo abordan estos problemas de manera aislada, lo que resulta en defectos lógicos y estructurales en el proceso de razonamiento. Para ello, los investigadores propusieron el marco SteinerSQL, que integra estos dos grandes desafíos en un problema de optimización centrado en gráficos, utilizando tres etapas de procesamiento: descomposición matemática, construcción de árboles de Steiner y validación en múltiples niveles. La innovación clave de este marco radica en su proceso de razonamiento unificado y su diseño estructural eficiente, lo que permitió que SteinerSQL alcanzara una precisión de ejecución del 36.10% y del 40.04% en las pruebas de referencia LogicCat y Spider2.0-Lite, superando las líneas base predominantes. SteinerSQL no solo mejora la precisión, sino que también ofrece un nuevo paradigma unificado para la generación de texto a SQL, con un amplio potencial de aplicación y un impacto profundo."
      },
      "host": "arxiv.org",
      "ts": "2025-09-25T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": [],
      "reusability_i18n": {
        "zh": [
          "采用数学分解优化推理过程",
          "构建Steiner树提升结构设计",
          "多层验证确保结果准确性"
        ],
        "en": [
          "Utilizes mathematical decomposition to optimize the reasoning process",
          "Constructs Steiner trees to enhance structural design",
          "Multi-layer validation ensures result accuracy"
        ],
        "es": [
          "Utiliza descomposición matemática para optimizar el proceso de razonamiento",
          "Construye árboles de Steiner para mejorar el diseño estructural",
          "La validación en múltiples capas asegura la precisión de los resultados"
        ]
      },
      "limitations_i18n": {
        "zh": [],
        "en": [],
        "es": []
      }
    }
  ],
  "refs": [
    {
      "title": "The Indispensable Role of User Simulation in the Pursuit of AGI",
      "url": "https://arxiv.org/abs/2509.19456"
    },
    {
      "title": "Evaluation-Aware Reinforcement Learning",
      "url": "https://arxiv.org/abs/2509.19464"
    },
    {
      "title": "Estimating the Self-Consistency of LLMs",
      "url": "https://arxiv.org/abs/2509.19489"
    },
    {
      "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
      "url": "https://arxiv.org/abs/2509.19517"
    },
    {
      "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2509.19524"
    },
    {
      "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics",
      "url": "https://arxiv.org/abs/2509.19566"
    },
    {
      "title": "What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities",
      "url": "https://arxiv.org/abs/2509.19590"
    },
    {
      "title": "SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation",
      "url": "https://arxiv.org/abs/2509.19623"
    }
  ],
  "stats": {
    "by_task": {
      "LLM": 0
    },
    "with_code": 0,
    "new_benchmarks": 0
  },
  "must_reads": [
    0,
    1,
    2,
    3,
    4
  ],
  "nice_to_read": [
    5,
    6,
    7,
    8
  ]
}