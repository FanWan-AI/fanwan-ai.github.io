{
  "generated_at": "2025-09-08T00:00:00+00:00",
  "items": [
    {
      "headline": "AI在建筑项目管理中的伦理评估",
      "one_liner": "本研究评估了大型语言模型在建筑项目管理中的决策支持能力，探讨其伦理影响。",
      "task": "Other",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "数据增强"
      ],
      "limitations": [
        "评测偏差"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04505",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04505.pdf"
      },
      "tags": [
        "LLM",
        "Ethics",
        "Construction"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究探讨了大型语言模型在建筑项目管理中的应用，分析其在决策支持中的伦理考量。",
      "who_should_try": "建筑管理者、AI伦理学者",
      "title_i18n": {
        "zh": "AI在建筑项目管理中的伦理评估",
        "en": "The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management"
      },
      "summary_i18n": {
        "zh": "建筑项目管理面临着复杂的决策挑战，传统方法往往效率低下。大型语言模型（LLMs）被视为重要的决策支持工具，能够通过自然语言处理技术加速信息的获取和分析。这些模型能够处理大量数据，提供实时的建议和预测，从而提升项目管理的效率和准确性。研究表明，LLMs的应用能够显著改善项目的决策过程，降低风险并提高成功率。",
        "en": "arXiv:2509.04505v1 Announce Type: new \nAbstract: The integration of Artificial Intelligence (AI) into construction project management (CPM) is accelerating, with Large Language Models (LLMs) emerging as accessible decision-support tools. This study aims to critically evaluate the ethical viability and reliability of LLMs when applied to the ethically sensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods research design was employed, involving the quantitative performance testing of two leading LLMs against twelve real-world ethical scenarios using a novel Ethical Decis"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "联合图与配置优化以提升AI代理可靠性",
      "one_liner": "本研究提出了一种联合图和配置优化的方法，以提高大型语言模型代理的可靠性。",
      "task": "Agent",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "模型配置"
      ],
      "limitations": [
        "算力门槛"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04642",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04642.pdf"
      },
      "tags": [
        "Agent",
        "Optimization",
        "LLM"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究提出了一种新方法，通过优化图结构和节点配置来提升AI代理的可靠性。",
      "who_should_try": "AI开发者、系统设计师",
      "title_i18n": {
        "zh": "联合图与配置优化以提升AI代理可靠性",
        "en": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents"
      },
      "summary_i18n": {
        "zh": "本研究提出了一种新方法，通过优化图结构和节点配置来提升AI代理的可靠性。",
        "en": "arXiv:2509.04642v1 Announce Type: new \nAbstract: Building reliable LLM agents requires decisions at two levels: the graph (which modules exist and how information flows) and the configuration of each node (models, prompts, tools, control knobs). Most existing optimizers tune configurations while holding the graph fixed, leaving structural failure modes unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for LLM agents that jointly searches over graphs and configurations to maximize agent quality, subject to explicit rollout/token budgets. Beyond numeric metrics, Maestro"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "健康模拟的个性化解释框架",
      "one_liner": "本研究提出了一种混合方法框架，以实现健康模拟中的利益相关者中心化总结。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "数据增强"
      ],
      "limitations": [
        "边界"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04646",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04646.pdf"
      },
      "tags": [
        "NLP",
        "Health",
        "Simulation"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究探讨了如何通过个性化解释框架来支持健康模拟中的决策过程。",
      "who_should_try": "健康研究人员、模拟专家",
      "title_i18n": {
        "zh": "健康模拟的个性化解释框架",
        "en": "Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization"
      },
      "summary_i18n": {
        "zh": "本研究探讨了如何通过个性化解释框架来支持健康模拟中的决策过程。",
        "en": "arXiv:2509.04646v1 Announce Type: new \nAbstract: Modeling & Simulation (M&S) approaches such as agent-based models hold significant potential to support decision-making activities in health, with recent examples including the adoption of vaccines, and a vast literature on healthy eating behaviors and physical activity behaviors. These models are potentially usable by different stakeholder groups, as they support policy-makers to estimate the consequences of potential interventions and they can guide individuals in making healthy choices in complex environments. However, this potential may"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "基于人类标准的AI模型评估方法",
      "one_liner": "本研究提出了一种新方法，将AI模型评估与人类标准相结合，以提高评估的有效性。",
      "task": "Theory",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "评估标准"
      ],
      "limitations": [
        "评测偏差"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04676",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04676.pdf"
      },
      "tags": [
        "AI",
        "Evaluation",
        "Theory"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究提出了一种将人类标准融入AI模型评估的新方法，以提高评估的准确性。",
      "who_should_try": "AI研究人员、评估专家",
      "title_i18n": {
        "zh": "基于人类标准的AI模型评估方法",
        "en": "An Approach to Grounding AI Model Evaluations in Human-derived Criteria"
      },
      "summary_i18n": {
        "zh": "本研究提出了一种将人类标准融入AI模型评估的新方法，以提高评估的准确性。",
        "en": "arXiv:2509.04676v1 Announce Type: new \nAbstract: In the rapidly evolving field of artificial intelligence (AI), traditional benchmarks can fall short in attempting to capture the nuanced capabilities of AI models. We focus on the case of physical world modeling and propose a novel approach to augment existing benchmarks with human-derived evaluation criteria, aiming to enhance the interpretability and applicability of model behaviors. Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted in-depth interviews and large-scale surveys to identify key cognitive skills, suc"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "多代理学习中的语言驱动任务结构",
      "one_liner": "本研究探讨了语言模型作为多代理学习中的显式世界模型的应用。",
      "task": "Agent",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "任务结构设计"
      ],
      "limitations": [
        "边界"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04731",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04731.pdf"
      },
      "tags": [
        "Agent",
        "Learning",
        "LLM"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究探讨了如何利用语言模型构建多代理学习中的任务结构，以提升学习效果。",
      "who_should_try": "AI研究人员、教育工作者",
      "title_i18n": {
        "zh": "多代理学习中的语言驱动任务结构",
        "en": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning"
      },
      "summary_i18n": {
        "zh": "本研究探讨了如何利用语言模型构建多代理学习中的任务结构，以提升学习效果。",
        "en": "arXiv:2509.04731v1 Announce Type: new \nAbstract: The convergence of Language models, Agent models, and World models represents a critical frontier for artificial intelligence. While recent progress has focused on scaling Language and Agent models, the development of sophisticated, explicit World Models remains a key bottleneck, particularly for complex, long-horizon multi-agent tasks. In domains such as robotic soccer, agents trained via standard reinforcement learning in high-fidelity but structurally-flat simulators often fail due to intractable exploration spaces and sparse rewards. This po"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "大型语言模型的假设分析",
      "one_liner": "本研究探讨了大型语言模型在假设分析中的应用，提升其探索能力。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "模型优化"
      ],
      "limitations": [
        "算力门槛"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04791",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04791.pdf"
      },
      "tags": [
        "NLP",
        "Analysis",
        "LLM"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究探讨了如何通过假设分析提升大型语言模型的探索能力，以应对复杂问题。",
      "who_should_try": "NLP研究人员、数据科学家",
      "title_i18n": {
        "zh": "大型语言模型的假设分析",
        "en": "What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking"
      },
      "summary_i18n": {
        "zh": "本研究探讨了如何通过假设分析提升大型语言模型的探索能力，以应对复杂问题。",
        "en": "arXiv:2509.04791v1 Announce Type: new \nAbstract: Large language models (LLMs) excel at processing information reactively but lack the ability to systemically explore hypothetical futures. They cannot ask, \"what if we take this action? how will it affect the final outcome\" and forecast its potential consequences before acting. This critical gap limits their utility in dynamic, high-stakes scenarios like strategic planning, risk assessment, and real-time decision making. To bridge this gap, we propose WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities. Our approach int"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "人本解释的强化学习代理",
      "one_liner": "本研究提出了一种人本解释的方法，以提高强化学习代理的透明度。",
      "task": "Agent",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "透明度提升"
      ],
      "limitations": [
        "复杂性"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04809",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04809.pdf"
      },
      "tags": [
        "Agent",
        "Reinforcement Learning",
        "Explainability"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究探讨了如何通过人本解释提升强化学习代理的透明度，以便更好地与领域专家沟通。",
      "who_should_try": "AI开发者、领域专家",
      "title_i18n": {
        "zh": "人本解释的强化学习代理",
        "en": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models"
      },
      "summary_i18n": {
        "zh": "本研究探讨了如何通过人本解释提升强化学习代理的透明度，以便更好地与领域专家沟通。",
        "en": "arXiv:2509.04809v1 Announce Type: new \nAbstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL p"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "游戏理论视角下的语言模型合作与冲突",
      "one_liner": "本研究分析了语言模型在多方环境中的合作与竞争行为，基于游戏理论进行探讨。",
      "task": "Theory",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "理论分析"
      ],
      "limitations": [
        "边界"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04847",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04847.pdf"
      },
      "tags": [
        "Game Theory",
        "LLM",
        "Cooperation"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究通过游戏理论分析语言模型在多方环境中的合作与冲突行为，揭示其潜在影响。",
      "who_should_try": "AI研究人员、游戏理论专家",
      "title_i18n": {
        "zh": "游戏理论视角下的语言模型合作与冲突",
        "en": "Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory"
      },
      "summary_i18n": {
        "zh": "本研究通过游戏理论分析语言模型在多方环境中的合作与冲突行为，揭示其潜在影响。",
        "en": "arXiv:2509.04847v1 Announce Type: new \nAbstract: Language models are increasingly deployed in interactive online environments, from personal chat assistants to domain-specific agents, raising questions about their cooperative and competitive behavior in multi-party settings. While prior work has examined language model decision-making in isolated or short-term game-theoretic contexts, these studies often neglect long-horizon interactions, human-model collaboration, and the evolution of behavioral patterns over time. In this paper, we investigate the dynamics of language model behavior in the i"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    }
  ],
  "refs": [],
  "stats": {
    "by_task": {
      "LLM": 0
    },
    "with_code": 0,
    "new_benchmarks": 0
  },
  "must_reads": [
    0,
    1,
    2,
    3,
    4
  ],
  "nice_to_read": [
    5,
    6,
    7,
    8
  ],
  "deep_dive": {
    "title": "AI与伦理",
    "summary": "探讨AI在各领域的伦理影响，特别是在决策支持中的应用。",
    "refs": []
  }
}