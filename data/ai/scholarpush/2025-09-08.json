{
  "generated_at": "2025-09-08T04:00:00+00:00",
  "items": [
    {
      "headline": "AI在建筑项目管理中的伦理评估",
      "one_liner": "本研究评估了大型语言模型在建筑项目管理中的决策支持能力，揭示了其伦理影响。",
      "task": "Other",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "数据增强"
      ],
      "limitations": [
        "评测偏差"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04505",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04505.pdf"
      },
      "tags": [
        "LLM",
        "Ethics",
        "Construction"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究探讨了大型语言模型在建筑项目管理中的应用，分析其在决策支持中的伦理考量。",
      "who_should_try": "建筑管理者、AI伦理研究者",
      "title_i18n": {
        "zh": "AI在建筑项目管理中的伦理评估",
        "en": "The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management"
      },
      "summary_i18n": {
        "zh": "随着人工智能技术的快速发展，大型语言模型（LLMs）在建筑项目管理中被广泛应用。这些模型能够提供决策支持，帮助项目经理在复杂的环境中做出更有效的决策。研究表明，LLMs的集成正在加速建筑管理的数字化转型，提升了项目的效率和准确性。",
        "en": "arXiv:2509.04505v1 Announce Type: new \nAbstract: The integration of Artificial Intelligence (AI) into construction project management (CPM) is accelerating, with Large Language Models (LLMs) emerging as accessible decision-support tools. This study aims to critically evaluate the"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "联合图与配置优化以提高AI代理可靠性",
      "one_liner": "本研究提出了一种联合图和配置优化的方法，以提高大型语言模型代理的可靠性。",
      "task": "Agent",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "模型配置"
      ],
      "limitations": [
        "算力门槛"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04642",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04642.pdf"
      },
      "tags": [
        "Agent",
        "Optimization",
        "AI"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究通过优化图结构和节点配置，提升了AI代理的决策能力和可靠性。",
      "who_should_try": "AI开发者、系统架构师",
      "title_i18n": {
        "zh": "联合图与配置优化以提高AI代理可靠性",
        "en": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents"
      },
      "summary_i18n": {
        "zh": "本研究通过优化图结构和节点配置，提升了AI代理的决策能力和可靠性。",
        "en": "arXiv:2509.04642v1 Announce Type: new \nAbstract: Building reliable LLM agents requires decisions at two levels: the graph (which modules exist and how information flows) and the configuration of each node (models, prompts, tools, control knobs). Most existing optimizers tune conf"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "健康模拟的个性化解释框架",
      "one_liner": "本研究提出了一种混合方法框架，以实现健康模拟中的利益相关者中心总结。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "数据增强"
      ],
      "limitations": [
        "评测偏差"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04646",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04646.pdf"
      },
      "tags": [
        "NLP",
        "Health",
        "Simulation"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究通过利益相关者中心的方法，提升了健康模拟的个性化解释能力。",
      "who_should_try": "健康研究人员、模拟专家",
      "title_i18n": {
        "zh": "健康模拟的个性化解释框架",
        "en": "Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization"
      },
      "summary_i18n": {
        "zh": "健康模拟领域正在探索如何为不同利益相关者提供个性化的解释。这种方法结合了定量和定性的方法，旨在提高模拟结果的可理解性和可用性。研究者们认为，这将有助于更好地支持公共健康决策和政策制定。",
        "en": "arXiv:2509.04646v1 Announce Type: new \nAbstract: Modeling & Simulation (M&S) approaches such as agent-based models hold significant potential to support decision-making activities in health, with recent examples including the adoption of vaccines, and a vast literature on hea"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "基于人类标准的AI模型评估方法",
      "one_liner": "本研究提出了一种新颖的方法，将AI模型评估与人类标准相结合，以提高评估的有效性。",
      "task": "Theory",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "评估标准"
      ],
      "limitations": [
        "数据泄漏"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04676",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04676.pdf"
      },
      "tags": [
        "AI",
        "Evaluation",
        "Theory"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究探讨了如何将人类标准融入AI模型的评估中，以更好地反映模型的实际能力。",
      "who_should_try": "AI研究者、评估专家",
      "title_i18n": {
        "zh": "基于人类标准的AI模型评估方法",
        "en": "An Approach to Grounding AI Model Evaluations in Human-derived Criteria"
      },
      "summary_i18n": {
        "zh": "传统的AI模型评估方法往往无法全面捕捉模型的复杂性。新的研究提出了一种基于人类标准的评估方法，旨在更准确地反映模型在实际应用中的表现。这一方法的提出为AI模型的评估提供了新的视角和工具。",
        "en": "arXiv:2509.04676v1 Announce Type: new \nAbstract: In the rapidly evolving field of artificial intelligence (AI), traditional benchmarks can fall short in attempting to capture the nuanced capabilities of AI models. We focus on the case of physical world modeling and propose a nove"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "多代理学习中的语言驱动任务结构",
      "one_liner": "本研究提出了语言驱动的层次任务结构，作为多代理学习的显式世界模型。",
      "task": "Agent",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "任务分解"
      ],
      "limitations": [
        "算力门槛"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04731",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04731.pdf"
      },
      "tags": [
        "Agent",
        "Learning",
        "Language"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究通过引入语言驱动的任务结构，提升了多代理学习的效率和效果。",
      "who_should_try": "AI研究者、代理开发者",
      "title_i18n": {
        "zh": "多代理学习中的语言驱动任务结构",
        "en": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning"
      },
      "summary_i18n": {
        "zh": "本研究通过引入语言驱动的任务结构，提升了多代理学习的效率和效果。",
        "en": "arXiv:2509.04731v1 Announce Type: new \nAbstract: The convergence of Language models, Agent models, and World models represents a critical frontier for artificial intelligence. While recent progress has focused on scaling Language and Agent models, the development of sophisticated"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "大型语言模型的假设分析",
      "one_liner": "本研究探讨了大型语言模型在系统性探索假设未来方面的局限性，并提出改进方法。",
      "task": "NLP",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "假设生成"
      ],
      "limitations": [
        "评测偏差"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04791",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04791.pdf"
      },
      "tags": [
        "NLP",
        "Analysis",
        "LLM"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究分析了大型语言模型在假设分析中的不足，并提出了改进建议。",
      "who_should_try": "NLP研究者、模型开发者",
      "title_i18n": {
        "zh": "大型语言模型的假设分析",
        "en": "What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking"
      },
      "summary_i18n": {
        "zh": "本研究分析了大型语言模型在假设分析中的不足，并提出了改进建议。",
        "en": "arXiv:2509.04791v1 Announce Type: new \nAbstract: Large language models (LLMs) excel at processing information reactively but lack the ability to systemically explore hypothetical futures. They cannot ask, \"what if we take this action? how will it affect the final outcome\" and for"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    },
    {
      "headline": "人本解释的强化学习代理",
      "one_liner": "本研究提出了TalkToAgent，一个以人类为中心的解释框架，提升了强化学习代理的透明度。",
      "task": "Agent",
      "type": "paper",
      "novelty": "method",
      "key_numbers": [
        {
          "dataset": "N/A",
          "metric": "N/A",
          "ours": "N/A",
          "baseline": "N/A",
          "impr_abs": "N/A",
          "impr_rel": "N/A"
        }
      ],
      "reusability": [
        "解释生成"
      ],
      "limitations": [
        "复杂性"
      ],
      "links": {
        "paper": "https://arxiv.org/abs/2509.04809",
        "code": "N/A",
        "project": "N/A",
        "pdf": "https://arxiv.org/pdf/2509.04809.pdf"
      },
      "tags": [
        "Agent",
        "Reinforcement Learning",
        "Explainability"
      ],
      "impact_score": 0,
      "reproducibility_score": 0,
      "quick_read": "本研究通过人本解释框架，提升了强化学习代理的透明度和可解释性。",
      "who_should_try": "强化学习研究者、AI伦理专家",
      "title_i18n": {
        "zh": "人本解释的强化学习代理",
        "en": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models"
      },
      "summary_i18n": {
        "zh": "本研究通过人本解释框架，提升了强化学习代理的透明度和可解释性。",
        "en": "arXiv:2509.04809v1 Announce Type: new \nAbstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to t"
      },
      "host": "arxiv.org",
      "ts": "2025-09-08T04:00:00+00:00",
      "has_code": false,
      "key_numbers_compact": []
    }
  ],
  "refs": [],
  "stats": {
    "by_task": {
      "LLM": 0
    },
    "with_code": 0,
    "new_benchmarks": 0
  },
  "must_reads": [
    0,
    1,
    2,
    3,
    4
  ],
  "nice_to_read": [
    5,
    6,
    7
  ],
  "deep_dive": {
    "title": "AI与伦理",
    "summary": "探讨AI在不同领域的应用及其伦理影响，强调透明度与可解释性的重要性。",
    "refs": []
  }
}